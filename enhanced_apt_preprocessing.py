import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import socket
import struct
import xgboost as xgb
import optuna
from sklearn.utils.class_weight import compute_class_weight
import warnings
import os
import time
import json
from sklearn.feature_selection import SelectFromModel, mutual_info_classif
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score, train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression, Lasso
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
import itertools
from collections import defaultdict, Counter

warnings.filterwarnings('ignore')

class EnhancedAPTPreprocessor:
    def __init__(self, input_path, output_path=None):
        self.input_path =input_path or r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
        self.df = None
        if output_path:
            self.output_path = output_path
            os.makedirs(output_path, exist_ok=True)
        else:
            self.output_path = os.path.dirname(input_path)
        
        self.df = None
        self.label_encoders = {}
        self.selected_features = []
        self.feature_importance = None
        self.cv_results = {}
        self.models_performance = {}
        
    def load_data(self):
        """ä»…åŠ è½½åŸå§‹ DSRL-APT-2023 æ•°æ®é›†ï¼Œä¸åšä»»ä½•åˆ—åˆ é™¤"""
        print("ğŸ”„ åŠ è½½åŸå§‹æ•°æ®é›†...")
        start_time = time.time()

        if not os.path.exists(self.input_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {self.input_path}")

        # ç›´æ¥è¯»å– CSV
        self.df = pd.read_csv(self.input_path)
        elapsed = time.time() - start_time
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {self.df.shape}")
        print(f"ğŸ“‹ æ•°æ®åˆ—: {self.df.columns.tolist()}\n")
        return self
    
    def clean_data(self):
        """æ•°æ®æ¸…æ´—"""
        print("\nğŸ§¹ æ•°æ®æ¸…æ´—...")

        original_shape = self.df.shape

        if "Flow ID" in self.df.columns:
            self.df.drop(columns=["Flow ID"], inplace=True)
            print("ğŸ—‘ å·²åˆ é™¤åˆ—: 'Flow ID'")
        else:
            print("â„¹ï¸ æœªå‘ç° 'Flow ID' åˆ—ï¼Œè·³è¿‡åˆ é™¤")

        # 2. å¤„ç†IPåœ°å€åˆ— - è½¬æ¢ä¸º32ä½æ•´æ•°
        ip_cols = ['Src IP', 'Dst IP']
        for ip_col in ip_cols:
            if ip_col in self.df.columns:
                print(f"è½¬æ¢IPåœ°å€åˆ— {ip_col} ä¸ºæ•´æ•°")
                self.df[f'{ip_col}_int'] = self.df[ip_col].apply(self._ip_to_int)
                # åˆ é™¤åŸå§‹IPåˆ—
                self.df.drop(columns=[ip_col], inplace=True)

        # 3. å¤„ç†æ—¶é—´æˆ³åˆ—
        if 'Timestamp' in self.df.columns:
            print("å¤„ç†æ—¶é—´æˆ³åˆ—ï¼Œæå–æ—¶é—´ç‰¹å¾")
            self._extract_time_features()

        # 4. åˆ é™¤é‡å¤è¡Œ
        dup_count = self.df.duplicated().sum()
        if dup_count > 0:
            print(f"åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
            self.df.drop_duplicates(inplace=True)

        # 5. å¤„ç†ç¼ºå¤±å€¼
        null_count = self.df.isnull().sum().sum()
        if null_count > 0:
            print(f"å¤„ç† {null_count} ä¸ªç¼ºå¤±å€¼")
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)
            for col in self.df.select_dtypes(exclude=['number']).columns:
                if not self.df[col].mode().empty:
                    self.df[col].fillna(self.df[col].mode()[0], inplace=True)

        # 6. å¤„ç†æ— ç©·å€¼
        inf_count = np.isinf(self.df.select_dtypes(include=['number'])).sum().sum()
        if inf_count > 0:
            print(f"å¤„ç† {inf_count} ä¸ªæ— ç©·å€¼")
            self.df.replace([np.inf, -np.inf], np.nan, inplace=True)
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)

        print(f"âœ… æ¸…æ´—å®Œæˆ: {original_shape} â†’ {self.df.shape}")
        return self

    def _ip_to_int(self, ip):
        """å°†IPåœ°å€è½¬æ¢ä¸º32ä½æ•´æ•°"""
        try:
            return struct.unpack("!I", socket.inet_aton(str(ip)))[0]
        except:
            return 0  # å¯¹äºæ— æ•ˆIPè¿”å›0

    def _extract_time_features(self):
        """ä»æ—¶é—´æˆ³æå–æ—¶é—´ç‰¹å¾"""
        try:
            # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
            self.df['datetime'] = pd.to_datetime(self.df['Timestamp'], errors='coerce')

            # æå–æ—¶é—´ç‰¹å¾
            self.df['hour'] = self.df['datetime'].dt.hour
            self.df['minute'] = self.df['datetime'].dt.minute
            self.df['second'] = self.df['datetime'].dt.second
            self.df['day_of_week'] = self.df['datetime'].dt.dayofweek  # 0=Monday

            print("âœ… æå–æ—¶é—´ç‰¹å¾: hour, minute, second, day_of_week")
        except:
            return 0



    def create_statistical_features(self):
        """åˆ›å»ºæµé‡ç»Ÿè®¡é‡ç‰¹å¾ï¼ˆåŸºäºæ¯æ¡æµï¼‰"""
        print("\nğŸ“Š åˆ›å»ºç»Ÿè®¡é‡ç‰¹å¾...")
        orig_cols = self.df.shape[1]

        # 1. æµé‡é€Ÿç‡ç»Ÿè®¡ï¼ˆBytes/sec & Packets/secï¼‰
        if 'Flow Duration' in self.df.columns:
            # é¿å…é™¤é›¶
            duration = self.df['Flow Duration'].replace(0, 1)

            # å‰å‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Fwd Packet' in self.df.columns:
                self.df['fwd_bytes_per_sec'] = (
                        self.df['Total Length of Fwd Packet'] / duration
                )
                self.df['fwd_bytes_per_sec_log'] = np.log1p(
                    self.df['fwd_bytes_per_sec']
                )

            # åå‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Bwd Packet' in self.df.columns:
                self.df['bwd_bytes_per_sec'] = (
                        self.df['Total Length of Bwd Packet'] / duration
                )
                self.df['bwd_bytes_per_sec_log'] = np.log1p(
                    self.df['bwd_bytes_per_sec']
                )

            # å‰å‘åŒ…é€Ÿç‡
            if 'Total Fwd Packet' in self.df.columns:
                self.df['fwd_packets_per_sec'] = (
                        self.df['Total Fwd Packet'] / duration
                )

            # åå‘åŒ…é€Ÿç‡
            if 'Total Bwd Packet' in self.df.columns:
                self.df['bwd_packets_per_sec'] = (
                        self.df['Total Bwd Packet'] / duration
                )

        # 2. åŒ…é•¿åº¦æ¯”ä¾‹
        if (
                'Fwd Packet Length Mean' in self.df.columns
                and 'Bwd Packet Length Mean' in self.df.columns
        ):
            bwd_mean = self.df['Bwd Packet Length Mean'].replace(0, 1)
            self.df['fwd_bwd_length_ratio'] = (
                    self.df['Fwd Packet Length Mean'] / bwd_mean
            )

        added = self.df.shape[1] - orig_cols
        print(f"âœ… ç»Ÿè®¡é‡ç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ–°å¢ {added} ä¸ªç‰¹å¾")
        return self

    def encode_and_normalize(self):
        """ç¼–ç å’Œå½’ä¸€åŒ–"""
        print("\nğŸ”¢ ç¼–ç å’Œå½’ä¸€åŒ–...")

        # 1. å¯¹æ‰€æœ‰åˆ†ç±»ç‰¹å¾è¿›è¡ŒLabel Encoding
        categorical_cols = []

        # æ£€æµ‹åˆ†ç±»åˆ—
        for col in self.df.columns:
            if self.df[col].dtype == 'object' or col in ['Stage', 'Activity', 'Protocol']:
                if col in self.df.columns:
                    categorical_cols.append(col)

        print(f"æ£€æµ‹åˆ°åˆ†ç±»åˆ—: {categorical_cols}")

        for col in categorical_cols:
            if col in self.df.columns:
                le = LabelEncoder()
                # å¤„ç†ç¼ºå¤±å€¼
                self.df[col] = self.df[col].astype(str).fillna('Unknown')
                self.df[f'{col}_encoded'] = le.fit_transform(self.df[col])
                self.label_encoders[col] = le
                print(f"âœ… Labelç¼–ç  {col} åˆ—: {len(le.classes_)} ä¸ªç±»åˆ«")

                # æ‰“å°ç¼–ç æ˜ å°„ï¼ˆå‰5ä¸ªï¼‰
                mapping = dict(zip(le.classes_[:5], le.transform(le.classes_[:5])))
                print(f"   æ˜ å°„ç¤ºä¾‹: {mapping}{'...' if len(le.classes_) > 5 else ''}")

        # 2. å¯¹æ‰€æœ‰æ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # æ’é™¤ç¼–ç åçš„æ ‡ç­¾åˆ—ï¼ˆä¿æŒåŸå§‹æ•´æ•°å€¼ï¼‰
        exclude_cols = [col for col in numeric_cols if col.endswith('_encoded')]
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        if feature_cols:
            print(f"å¯¹ {len(feature_cols)} ä¸ªæ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–...")
            scaler = MinMaxScaler()  # ä½¿ç”¨Min-Maxå½’ä¸€åŒ–
            self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])
            self.scaler = scaler  # ä¿å­˜scalerä»¥å¤‡åç”¨
            print("âœ… Min-Maxå½’ä¸€åŒ–å®Œæˆ: X_new = (X - min) / (max - min)")

        print(f"âœ… ç¼–ç å’Œå½’ä¸€åŒ–å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.df.shape}")
        return self

    def domain_knowledge_feature_classification(self):
        """åŸºäºåŸŸçŸ¥è¯†çš„ç‰¹å¾åˆ†ç±»"""
        print(f"\nğŸ·ï¸ åŸºäºåŸŸçŸ¥è¯†çš„ç‰¹å¾åˆ†ç±»")

        # è·å–æ‰€æœ‰æ•°å€¼ç‰¹å¾
        numeric_cols = self.df.select_dtypes(include=[int, float]).columns.tolist()

        # æ’é™¤ç›®æ ‡å˜é‡å’Œæ´»åŠ¨æ ‡ç­¾
        exclude_cols = {'Label', 'Label_encoded', 'Stage', 'Stage_encoded', 'Activity', 'Activity_encoded'}
        available_features = [col for col in numeric_cols if col not in exclude_cols]

        # å®šä¹‰ç‰¹å¾ç±»åˆ«
        self.feature_categories = {
            'temporal': [],      # æ—¶åºç‰¹å¾
            'flow_stats': [],    # æµé‡ç‰¹å¾
            'packet_length': [], # åŒ…é•¿åº¦ç»Ÿè®¡
            'tcp_flags': [],     # TCPæ ‡å¿—
            'network_meta': [],  # ç½‘ç»œå…ƒæ•°æ®
            'iat_features': [],  # æ—¶é—´é—´éš”ç‰¹å¾
            'other': []          # å…¶ä»–ç‰¹å¾
        }

        # åˆ†ç±»ç‰¹å¾
        for feature in available_features:
            feature_lower = feature.lower()

            # æ—¶åºç‰¹å¾
            if any(keyword in feature_lower for keyword in ['hour', 'minute', 'second', 'day_of_week', 'duration']):
                self.feature_categories['temporal'].append(feature)

            # æµé‡ç‰¹å¾
            elif any(keyword in feature_lower for keyword in ['bytes_s', 'packets_s', 'flow_bytes', 'flow_packets', 'total_fwd', 'total_bwd']):
                self.feature_categories['flow_stats'].append(feature)

            # åŒ…é•¿åº¦ç»Ÿè®¡
            elif any(keyword in feature_lower for keyword in ['packet_length', 'fwd_packet', 'bwd_packet', 'length_min', 'length_max', 'length_mean', 'length_std']):
                self.feature_categories['packet_length'].append(feature)

            # TCPæ ‡å¿—
            elif any(keyword in feature_lower for keyword in ['flag', 'fin', 'syn', 'rst', 'psh', 'ack', 'urg', 'cwr', 'ece']):
                self.feature_categories['tcp_flags'].append(feature)

            # æ—¶é—´é—´éš”ç‰¹å¾
            elif any(keyword in feature_lower for keyword in ['iat', 'inter_arrival']):
                self.feature_categories['iat_features'].append(feature)

            # ç½‘ç»œå…ƒæ•°æ®
            elif any(keyword in feature_lower for keyword in ['protocol', 'port', 'ip']):
                self.feature_categories['network_meta'].append(feature)

            # å…¶ä»–ç‰¹å¾
            else:
                self.feature_categories['other'].append(feature)

        # æ‰“å°åˆ†ç±»ç»“æœ
        print(f"ç‰¹å¾åˆ†ç±»ç»“æœ:")
        total_features = 0
        for category, features in self.feature_categories.items():
            print(f"  {category:15}: {len(features):3d} ä¸ªç‰¹å¾")
            if len(features) <= 5:
                print(f"    ç¤ºä¾‹: {features}")
            else:
                print(f"    ç¤ºä¾‹: {features[:5]} ...")
            total_features += len(features)

        print(f"  æ€»è®¡: {total_features} ä¸ªç‰¹å¾")

        # ç¡®ä¿æ¯ä¸ªå¤§ç±»è‡³å°‘ä¿ç•™1-2ä¸ªä»£è¡¨æ€§ç‰¹å¾
        self.representative_features = {}
        for category, features in self.feature_categories.items():
            if features:
                # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ–¹å·®ï¼Œé€‰æ‹©æ–¹å·®è¾ƒå¤§çš„ä½œä¸ºä»£è¡¨æ€§ç‰¹å¾
                feature_vars = {}
                for feat in features:
                    if feat in self.df.columns:
                        feature_vars[feat] = self.df[feat].var()

                # æŒ‰æ–¹å·®æ’åºï¼Œé€‰æ‹©å‰2ä¸ªä½œä¸ºä»£è¡¨æ€§ç‰¹å¾
                sorted_features = sorted(feature_vars.items(), key=lambda x: x[1], reverse=True)
                self.representative_features[category] = [feat for feat, _ in sorted_features[:2]]

        print(f"\nä»£è¡¨æ€§ç‰¹å¾é€‰æ‹©:")
        for category, features in self.representative_features.items():
            print(f"  {category:15}: {features}")

        return self

    def prepare_paper_aligned_features(self):
        # 1) è‡ªåŠ¨è¯†åˆ«â€œå”¯ä¸€â€ç›®æ ‡åˆ—
        for col in ('Label_encoded', 'Label', 'Stage_encoded', 'Stage'):
            if col in self.df.columns:
                target = col
                break
        else:
            raise ValueError("æ‰¾ä¸åˆ°ç›®æ ‡åˆ—ï¼šLabel/Label_encoded/Stage/Stage_encoded")

        # 2) å–æ‰€æœ‰æ•°å€¼å‹åˆ—
        numeric_cols = self.df.select_dtypes(include=[int, float]).columns.tolist()

        # 3) æ„é€ æ’é™¤æ¨¡å¼ï¼šæ ‡ç­¾ã€Activity + åç»­è¡ç”Ÿ
        exclude = {target, 'Activity', 'Activity_encoded'}
        # æ’é™¤æ‰€æœ‰åç§°ä¸­å«ä»¥ä¸‹å…³é”®å­—çš„åˆ—
        bad_kw = ('_encoded', '_log', 'per_sec', 'bulk', 'ratio')
        for c in numeric_cols:
            low = c.lower()
            if any(kw in low for kw in bad_kw):
                exclude.add(c)

        # 4) æœ€ç»ˆçš„å€™é€‰æ±  = numeric_cols - exclude
        feature_cols = [c for c in numeric_cols if c not in exclude]

        # 5) è¡¥å……æ—¶é—´æ‹†åˆ†ï¼ˆè‹¥å­˜åœ¨çš„è¯ï¼‰
        for tf in ('hour', 'minute', 'second', 'day_of_week'):
            if tf in self.df.columns and tf not in feature_cols:
                feature_cols.append(tf)

        # æ£€æŸ¥è®ºæ–‡ä¸­çš„å…³é”®ç‰¹å¾ï¼ˆåŒ…å«æ–°å¢çš„3ä¸ªé‡è¦ç‰¹å¾ï¼‰
        paper_critical_features = [
            'Protocol_encoded',  # åè®®ç‰¹å¾ï¼ˆTCP/UDPï¼‰
            'Flow Duration',     # æµæŒç»­æ—¶é—´
            'Fwd IAT Total',     # æ­£å‘IATæ€»æ—¶é—´
            'Fwd IAT Mean',      # æ­£å‘IATå¹³å‡æ—¶é—´
            'Bwd IAT Std',       # åå‘IATæ ‡å‡†å·®
            'Bwd IAT Max',       # åå‘IATæœ€å¤§å€¼
            'Bwd IAT Min',       # åå‘IATæœ€å°å€¼
            'FIN Flag Count',    # FINæ ‡å¿—ä½è®¡æ•°
            'SYN Flag Count',    # SYNæ ‡å¿—ä½è®¡æ•°
            'RST Flag Count',    # RSTæ ‡å¿—ä½è®¡æ•°
            'PSH Flag Count',    # PSHæ ‡å¿—ä½è®¡æ•°
            'ACK Flag Count',    # ACKæ ‡å¿—ä½è®¡æ•°
            'URG Flag Count',    # URGæ ‡å¿—ä½è®¡æ•°
            # æ–°å¢çš„3ä¸ªé‡è¦ç‰¹å¾
            'Total Length of Fwd Packet',  # å‰å‘åŒ…æ€»é•¿åº¦
            'Fwd Packet Length Min',       # å‰å‘åŒ…æœ€å°é•¿åº¦
            'Flow IAT Min'                 # æµé—´éš”æœ€å°å€¼
        ]

        # æ£€æŸ¥å…³é”®ç‰¹å¾çš„å­˜åœ¨æƒ…å†µ
        critical_found = []
        critical_missing = []

        for feat in paper_critical_features:
            if feat in self.df.columns:
                if feat not in feature_cols:
                    feature_cols.append(feat)  # ç¡®ä¿å…³é”®ç‰¹å¾è¢«åŒ…å«
                critical_found.append(feat)
            else:
                critical_missing.append(feat)

        # æ‰“å°ç¡®è®¤
        print(f"âœ… ç›®æ ‡åˆ—ï¼ˆå·²æ’é™¤ï¼‰: {target}")
        print(f"âœ… æ’é™¤äº† {len(exclude)} åˆ—ï¼Œå€™é€‰æ± å…±æœ‰ {len(feature_cols)} åˆ—")
        print("å€™é€‰ç‰¹å¾ç¤ºä¾‹:", feature_cols[:10], "...")

        # è¯¦ç»†åˆ†æè®ºæ–‡å…³é”®ç‰¹å¾
        print(f"\nğŸ“Š è®ºæ–‡å…³é”®ç‰¹å¾æ£€æŸ¥:")
        print(f"  è®ºæ–‡å…³é”®ç‰¹å¾æ‰¾åˆ°: {len(critical_found)}/13")

        if critical_found:
            print(f"\nâœ… æ‰¾åˆ°çš„è®ºæ–‡å…³é”®ç‰¹å¾:")
            for i, feat in enumerate(critical_found, 1):
                print(f"  {i:2d}. {feat}")

        if critical_missing:
            print(f"\nâŒ ç¼ºå¤±çš„è®ºæ–‡å…³é”®ç‰¹å¾:")
            for i, feat in enumerate(critical_missing, 1):
                print(f"  {i:2d}. {feat}")

        # æ£€æŸ¥ç‰¹å¾ç±»åˆ«åˆ†å¸ƒ
        protocol_features = [f for f in feature_cols if 'protocol' in f.lower()]
        duration_features = [f for f in feature_cols if 'duration' in f.lower()]
        iat_features = [f for f in feature_cols if 'iat' in f.lower()]
        flag_features = [f for f in feature_cols if 'flag' in f.lower()]
        time_features = ['hour', 'minute', 'day_of_week']
        time_in_candidates = [f for f in feature_cols if f in time_features]

        print(f"\nğŸ” ç‰¹å¾ç±»åˆ«åˆ†å¸ƒ:")
        print(f"  åè®®ç‰¹å¾: {len(protocol_features)} ({protocol_features})")
        print(f"  æµæŒç»­æ—¶é—´: {len(duration_features)} ({duration_features})")
        print(f"  IATæ—¶é—´é—´éš”: {len(iat_features)}")
        print(f"  TCPæ ‡å¿—ä½: {len(flag_features)}")
        print(f"  æ—¶é—´ç‰¹å¾: {len(time_in_candidates)} ({time_in_candidates})")

        # æ¸…ç†ç‰¹å¾åç§°ï¼Œé¿å…LightGBMçš„ç‰¹å¾åç§°é—®é¢˜
        cleaned_feature_cols = []
        for col in feature_cols:
            # æ›¿æ¢ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
            cleaned_col = col.replace(' ', '_').replace('/', '_').replace('-', '_')
            cleaned_feature_cols.append(cleaned_col)

        # é‡å‘½åDataFrameåˆ—
        rename_mapping = dict(zip(feature_cols, cleaned_feature_cols))
        self.df = self.df.rename(columns=rename_mapping)

        self.candidate_features = cleaned_feature_cols
        print(f"âœ… ç‰¹å¾åç§°å·²æ¸…ç†ï¼Œé¿å…LightGBMå…¼å®¹æ€§é—®é¢˜")
        return self

    def initial_feature_screening(self, correlation_threshold=0.9, variance_threshold=0.01):
        """åˆç­›ï¼šå»ç›¸å…³æ€§å’Œé«˜æ–¹å·®ç‰¹å¾ç­›é€‰"""
        print(f"\nğŸ” åˆç­›é˜¶æ®µï¼šå»ç›¸å…³æ€§(>{correlation_threshold})å’Œä½æ–¹å·®(<{variance_threshold})ç‰¹å¾")

        if not hasattr(self, 'candidate_features') or not self.candidate_features:
            raise ValueError("è¯·å…ˆæ‰§è¡Œç‰¹å¾å‡†å¤‡æ­¥éª¤")

        original_count = len(self.candidate_features)
        print(f"åˆå§‹ç‰¹å¾æ•°é‡: {original_count}")

        # 1. å»é™¤ä½æ–¹å·®ç‰¹å¾ï¼ˆè¿‘ä¹å¸¸æ•°çš„ç‰¹å¾ï¼‰
        print(f"\n1ï¸âƒ£ å»é™¤ä½æ–¹å·®ç‰¹å¾ï¼ˆæ–¹å·® < {variance_threshold}ï¼‰")
        low_variance_features = []
        remaining_features = []

        for feature in self.candidate_features:
            if feature in self.df.columns:
                feature_var = self.df[feature].var()
                if feature_var < variance_threshold:
                    low_variance_features.append(feature)
                else:
                    remaining_features.append(feature)

        print(f"  ç§»é™¤ä½æ–¹å·®ç‰¹å¾: {len(low_variance_features)} ä¸ª")
        if low_variance_features:
            print(f"  ç¤ºä¾‹: {low_variance_features[:5]}")

        # 2. è®¡ç®—ç›¸å…³æ€§çŸ©é˜µå¹¶å»é™¤é«˜ç›¸å…³ç‰¹å¾
        print(f"\n2ï¸âƒ£ å»é™¤é«˜ç›¸å…³ç‰¹å¾ï¼ˆç›¸å…³ç³»æ•° > {correlation_threshold}ï¼‰")

        if len(remaining_features) > 1:
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix = self.df[remaining_features].corr().abs()

            # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if corr_matrix.iloc[i, j] > correlation_threshold:
                        high_corr_pairs.append((
                            corr_matrix.columns[i],
                            corr_matrix.columns[j],
                            corr_matrix.iloc[i, j]
                        ))

            print(f"  å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³ç‰¹å¾")

            # é€‰æ‹©è¦ä¿ç•™çš„ç‰¹å¾ï¼ˆä¿ç•™æ–¹å·®æ›´å¤§çš„ï¼‰
            to_drop = set()
            for feat1, feat2, corr_val in high_corr_pairs:
                if feat1 not in to_drop and feat2 not in to_drop:
                    # ä¿ç•™æ–¹å·®æ›´å¤§çš„ç‰¹å¾
                    var1 = self.df[feat1].var()
                    var2 = self.df[feat2].var()

                    if var1 >= var2:
                        to_drop.add(feat2)
                        print(f"    ç§»é™¤ {feat2} (ç›¸å…³æ€§={corr_val:.3f}, ä¿ç•™æ–¹å·®æ›´å¤§çš„ {feat1})")
                    else:
                        to_drop.add(feat1)
                        print(f"    ç§»é™¤ {feat1} (ç›¸å…³æ€§={corr_val:.3f}, ä¿ç•™æ–¹å·®æ›´å¤§çš„ {feat2})")

            # æ›´æ–°ç‰¹å¾åˆ—è¡¨
            final_features = [f for f in remaining_features if f not in to_drop]
            print(f"  ç§»é™¤é«˜ç›¸å…³ç‰¹å¾: {len(to_drop)} ä¸ª")
        else:
            final_features = remaining_features
            print(f"  ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡ç›¸å…³æ€§åˆ†æ")

        # 3. ä¿å­˜ç­›é€‰ç»“æœ
        self.screened_features = final_features
        screened_count = len(final_features)

        print(f"\nâœ… åˆç­›å®Œæˆ:")
        print(f"  åŸå§‹ç‰¹å¾: {original_count}")
        print(f"  ç­›é€‰åç‰¹å¾: {screened_count}")
        print(f"  ç­›é€‰æ¯”ä¾‹: {(original_count - screened_count) / original_count * 100:.1f}%")
        print(f"  å‰©ä½™ç‰¹å¾é¢„ç®—: {screened_count} (ç›®æ ‡: 50-70)")

        # 4. æŒ‰ç±»åˆ«åˆ†æç­›é€‰ç»“æœ
        if hasattr(self, 'feature_categories'):
            print(f"\nğŸ“Š æŒ‰ç±»åˆ«åˆ†æç­›é€‰ç»“æœ:")
            category_stats = {}
            for category, original_features in self.feature_categories.items():
                remaining_in_category = [f for f in original_features if f in final_features]
                category_stats[category] = {
                    'original': len(original_features),
                    'remaining': len(remaining_in_category),
                    'features': remaining_in_category
                }
                print(f"  {category:15}: {len(original_features):2d} â†’ {len(remaining_in_category):2d}")

            self.category_screening_stats = category_stats

        return self

    def find_optimal_feature_count_with_shap(self, max_features=None, cv_folds=5):
        """ä½¿ç”¨SHAPå’Œäº¤å‰éªŒè¯æ‰¾åˆ°æœ€ä¼˜ç‰¹å¾æ•°é‡"""
        print(f"\nğŸ” ä½¿ç”¨SHAPåˆ†æå¯»æ‰¾æœ€ä¼˜ç‰¹å¾æ•°é‡")

        # ç¡®å®šç›®æ ‡å˜é‡
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        # å‡†å¤‡æ•°æ®
        X = self.df[self.candidate_features]
        y = self.df[target]

        if max_features is None:
            max_features = min(len(self.candidate_features), 60)  # æœ€å¤šæµ‹è¯•60ä¸ªç‰¹å¾

        # ä½¿ç”¨XGBoostè¿›è¡ŒSHAPåˆ†æ
        print(f"  è®­ç»ƒXGBoostæ¨¡å‹è¿›è¡ŒSHAPåˆ†æ...")
        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='mlogloss'
        )

        # è®­ç»ƒæ¨¡å‹
        xgb_model.fit(X, y)

        # ä½¿ç”¨XGBoostå†…ç½®çš„ç‰¹å¾é‡è¦æ€§ï¼ˆæ›´ç®€å•å¯é ï¼‰
        print(f"  ä½¿ç”¨XGBoostç‰¹å¾é‡è¦æ€§...")
        feature_importance = xgb_model.feature_importances_

        # æŒ‰é‡è¦æ€§æ’åºç‰¹å¾
        feature_importance_df = pd.DataFrame({
            'feature': X.columns.tolist(),
            'importance': feature_importance.tolist()
        }).sort_values('importance', ascending=False)

        print(f"  XGBoostç‰¹å¾é‡è¦æ€§Top 10:")
        for i, row in feature_importance_df.head(10).iterrows():
            print(f"    {row['feature']}: {row['importance']:.4f}")

        # æµ‹è¯•ä¸åŒç‰¹å¾æ•°é‡çš„æ€§èƒ½
        feature_counts = [10, 15, 20, 25, 30, 35, 40, 46, 50, min(max_features, len(self.candidate_features))]
        feature_counts = sorted(list(set(feature_counts)))  # å»é‡å¹¶æ’åº

        print(f"\n  æµ‹è¯•ä¸åŒç‰¹å¾æ•°é‡çš„æ€§èƒ½:")

        best_score = 0
        best_feature_count = 46
        results = []

        for n_feat in feature_counts:
            if n_feat > len(feature_importance_df):
                continue

            # é€‰æ‹©Top Nç‰¹å¾
            selected_features = feature_importance_df.head(n_feat)['feature'].tolist()
            X_selected = X[selected_features]

            # äº¤å‰éªŒè¯è¯„ä¼°
            scores = cross_val_score(
                xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),
                X_selected, y,
                cv=cv_folds,
                scoring='f1_macro',
                n_jobs=-1
            )

            mean_score = scores.mean()
            std_score = scores.std()

            results.append({
                'n_features': n_feat,
                'f1_score': mean_score,
                'f1_std': std_score
            })

            print(f"    {n_feat:2d}ä¸ªç‰¹å¾: F1={mean_score:.4f}Â±{std_score:.4f}")

            if mean_score > best_score:
                best_score = mean_score
                best_feature_count = n_feat

        # ä¿å­˜ç»“æœ
        self.feature_selection_results = {
            'xgboost_importance': feature_importance_df.to_dict('records'),
            'performance_by_count': results,
            'optimal_count': best_feature_count,
            'optimal_score': best_score
        }

        print(f"\nâœ… æœ€ä¼˜ç‰¹å¾æ•°é‡: {best_feature_count} (F1: {best_score:.4f})")

        # æ›´æ–°å€™é€‰ç‰¹å¾ä¸ºæœ€ä¼˜æ•°é‡çš„ç‰¹å¾
        self.optimal_features = feature_importance_df.head(best_feature_count)['feature'].tolist()

        return best_feature_count

    def automated_feature_scoring(self, use_screened_features=True):
        """è‡ªåŠ¨åŒ–æ‰“åˆ†ï¼šXGBoost+SHAPã€L1æ­£åˆ™åŒ–ã€mRMRä¸‰ç§æ–¹æ³•ç»¼åˆæ‰“åˆ†"""
        print(f"\nğŸ¯ è‡ªåŠ¨åŒ–ç‰¹å¾æ‰“åˆ†ï¼šXGBoost+SHAP + L1æ­£åˆ™åŒ– + mRMR")

        # é€‰æ‹©è¦è¯„åˆ†çš„ç‰¹å¾
        if use_screened_features and hasattr(self, 'screened_features'):
            features_to_score = self.screened_features
            print(f"ä½¿ç”¨ç­›é€‰åçš„ç‰¹å¾: {len(features_to_score)} ä¸ª")
        else:
            features_to_score = self.candidate_features
            print(f"ä½¿ç”¨å€™é€‰ç‰¹å¾: {len(features_to_score)} ä¸ª")

        if not features_to_score:
            raise ValueError("æ²¡æœ‰å¯ç”¨äºè¯„åˆ†çš„ç‰¹å¾")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[features_to_score]
        y = self.df[target]

        print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}, æ ·æœ¬æ•°é‡: {X.shape[0]}")

        # åˆå§‹åŒ–è¯„åˆ†ç»“æœ
        feature_scores = {feature: {'xgb_shap': 0, 'l1_reg': 0, 'mrmr': 0, 'combined': 0}
                         for feature in features_to_score}

        # 1. XGBoost + SHAP è¯„åˆ†
        print(f"\n1ï¸âƒ£ XGBoostç‰¹å¾é‡è¦æ€§è¯„åˆ†")
        try:
            xgb_model = xgb.XGBClassifier(
                n_estimators=200,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                eval_metric='mlogloss'
            )
            xgb_model.fit(X, y)

            # è·å–ç‰¹å¾é‡è¦æ€§
            xgb_importance = xgb_model.feature_importances_

            # å½’ä¸€åŒ–åˆ°0-1
            xgb_importance_norm = xgb_importance / np.max(xgb_importance)

            for i, feature in enumerate(features_to_score):
                feature_scores[feature]['xgb_shap'] = xgb_importance_norm[i]

            print(f"  âœ… XGBoostè¯„åˆ†å®Œæˆ")

        except Exception as e:
            print(f"  âŒ XGBoostè¯„åˆ†å¤±è´¥: {e}")

        # 2. L1æ­£åˆ™åŒ–è¯„åˆ†
        print(f"\n2ï¸âƒ£ L1æ­£åˆ™åŒ–ç‰¹å¾é€‰æ‹©è¯„åˆ†")
        try:
            # æ ‡å‡†åŒ–ç‰¹å¾
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)

            # L1æ­£åˆ™åŒ–Logisticå›å½’
            l1_model = LogisticRegression(
                penalty='l1',
                solver='liblinear',
                C=0.1,
                random_state=42,
                max_iter=1000
            )
            l1_model.fit(X_scaled, y)

            # è·å–ç³»æ•°çš„ç»å¯¹å€¼ä½œä¸ºé‡è¦æ€§
            l1_importance = np.abs(l1_model.coef_).mean(axis=0)

            # å½’ä¸€åŒ–åˆ°0-1
            if np.max(l1_importance) > 0:
                l1_importance_norm = l1_importance / np.max(l1_importance)
            else:
                l1_importance_norm = l1_importance

            for i, feature in enumerate(features_to_score):
                feature_scores[feature]['l1_reg'] = l1_importance_norm[i]

            print(f"  âœ… L1æ­£åˆ™åŒ–è¯„åˆ†å®Œæˆ")

        except Exception as e:
            print(f"  âŒ L1æ­£åˆ™åŒ–è¯„åˆ†å¤±è´¥: {e}")

        # 3. mRMR (æœ€å°å†—ä½™æœ€å¤§ç›¸å…³æ€§) è¯„åˆ†
        print(f"\n3ï¸âƒ£ äº’ä¿¡æ¯(MI)è¯„åˆ†")
        try:
            # è®¡ç®—äº’ä¿¡æ¯
            mi_scores = mutual_info_classif(X, y, random_state=42)

            # å½’ä¸€åŒ–åˆ°0-1
            if np.max(mi_scores) > 0:
                mi_scores_norm = mi_scores / np.max(mi_scores)
            else:
                mi_scores_norm = mi_scores

            for i, feature in enumerate(features_to_score):
                feature_scores[feature]['mrmr'] = mi_scores_norm[i]

            print(f"  âœ… äº’ä¿¡æ¯è¯„åˆ†å®Œæˆ")

        except Exception as e:
            print(f"  âŒ äº’ä¿¡æ¯è¯„åˆ†å¤±è´¥: {e}")

        # 4. ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        print(f"\n4ï¸âƒ£ ç»¼åˆè¯„åˆ†è®¡ç®—")
        weights = {'xgb_shap': 0.4, 'l1_reg': 0.3, 'mrmr': 0.3}  # å¯è°ƒæ•´æƒé‡

        for feature in features_to_score:
            combined_score = (
                weights['xgb_shap'] * feature_scores[feature]['xgb_shap'] +
                weights['l1_reg'] * feature_scores[feature]['l1_reg'] +
                weights['mrmr'] * feature_scores[feature]['mrmr']
            )
            feature_scores[feature]['combined'] = combined_score

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        sorted_features = sorted(feature_scores.items(),
                               key=lambda x: x[1]['combined'],
                               reverse=True)

        # ä¿å­˜è¯„åˆ†ç»“æœ
        self.feature_scores = feature_scores
        self.sorted_features_by_score = sorted_features

        # æ‰“å°Top 10ç‰¹å¾
        print(f"\nğŸ† Top 10 ç‰¹å¾ï¼ˆæŒ‰ç»¼åˆå¾—åˆ†ï¼‰:")
        print(f"{'ç‰¹å¾å':<30} {'XGB':>8} {'L1':>8} {'MI':>8} {'ç»¼åˆ':>8}")
        print("-" * 70)

        for i, (feature, scores) in enumerate(sorted_features[:10]):
            print(f"{feature:<30} {scores['xgb_shap']:>8.4f} {scores['l1_reg']:>8.4f} "
                  f"{scores['mrmr']:>8.4f} {scores['combined']:>8.4f}")

        print(f"\nâœ… è‡ªåŠ¨åŒ–ç‰¹å¾è¯„åˆ†å®Œæˆ")
        print(f"  è¯„åˆ†æƒé‡: XGBoost={weights['xgb_shap']}, L1={weights['l1_reg']}, MI={weights['mrmr']}")

        return self

    def filter_candidate_features(self, remove_overfitting_features=True, include_paper_features=True):
        """è¿‡æ»¤å€™é€‰ç‰¹å¾ï¼Œé¿å…è¿‡æ‹Ÿåˆå’Œä¿¡æ¯æ³„éœ²"""
        print(f"\nğŸ” è¿‡æ»¤å€™é€‰ç‰¹å¾ï¼Œé¿å…è¿‡æ‹Ÿåˆå’Œä¿¡æ¯æ³„éœ²")

        original_count = len(self.candidate_features)
        filtered_features = self.candidate_features.copy()

        # ç§»é™¤å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆæˆ–ä¿¡æ¯æ³„éœ²çš„ç‰¹å¾
        if remove_overfitting_features:
            overfitting_features = [
                'Dst_IP_int',      # ç›®æ ‡IPå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ
                'Src_IP_int',      # æºIPå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ
                'Timestamp_encoded', # æ—¶é—´æˆ³ç¼–ç å¯èƒ½æ³„éœ²ä¿¡æ¯
                'Activity_encoded'   # æ´»åŠ¨ç±»å‹å¯èƒ½æ³„éœ²æ ‡ç­¾ä¿¡æ¯
            ]

            removed_features = []
            for feature in overfitting_features:
                if feature in filtered_features:
                    filtered_features.remove(feature)
                    removed_features.append(feature)

            if removed_features:
                print(f"  ç§»é™¤å¯èƒ½è¿‡æ‹Ÿåˆçš„ç‰¹å¾: {removed_features}")

        # è®ºæ–‡é‡è¦ç‰¹å¾å¤„ç†
        paper_features = [
            'Protocol_encoded', 'Flow_Duration', 'Fwd_IAT_Total', 'Fwd_IAT_Mean',
            'Bwd_IAT_Std', 'Bwd_IAT_Max', 'Bwd_IAT_Min', 'FIN_Flag_Count',
            'SYN_Flag_Count', 'RST_Flag_Count', 'PSH_Flag_Count', 'ACK_Flag_Count',
            'URG_Flag_Count', 'Total_Length_of_Fwd_Packet', 'Fwd_Packet_Length_Min',
            'Flow_IAT_Min'
        ]

        if include_paper_features:
            # ç¡®ä¿è®ºæ–‡é‡è¦ç‰¹å¾è¢«åŒ…å«
            for feature in paper_features:
                if feature not in filtered_features and feature in self.df.columns:
                    filtered_features.append(feature)
            print(f"  ç¡®ä¿åŒ…å«è®ºæ–‡é‡è¦ç‰¹å¾: {len([f for f in paper_features if f in filtered_features])} ä¸ª")
        else:
            # ç§»é™¤è®ºæ–‡ç‰¹å¾ï¼Œæµ‹è¯•å…¶ä»–ç‰¹å¾çš„æ•ˆæœ
            removed_paper = []
            for feature in paper_features:
                if feature in filtered_features:
                    filtered_features.remove(feature)
                    removed_paper.append(feature)
            if removed_paper:
                print(f"  ç§»é™¤è®ºæ–‡ç‰¹å¾è¿›è¡Œæµ‹è¯•: {removed_paper}")

        # æ·»åŠ ç¨³å®šçš„ç½‘ç»œæµç‰¹å¾
        stable_features = [
            'Flow_Bytes_s', 'Flow_Packets_s', 'Fwd_Packets_s', 'Bwd_Packets_s',
            'Packet_Length_Mean', 'Packet_Length_Std', 'Packet_Length_Max',
            'Flow_IAT_Mean', 'Flow_IAT_Std', 'Active_Mean', 'Idle_Mean',
            'hour', 'minute', 'day_of_week'  # æ—¶é—´ç‰¹å¾é€šå¸¸æ¯”è¾ƒç¨³å®š
        ]

        for feature in stable_features:
            if feature not in filtered_features and feature in self.df.columns:
                filtered_features.append(feature)

        self.candidate_features = filtered_features
        filtered_count = len(filtered_features)

        print(f"âœ… ç‰¹å¾è¿‡æ»¤å®Œæˆ")
        print(f"  åŸå§‹ç‰¹å¾æ•°: {original_count}")
        print(f"  è¿‡æ»¤åç‰¹å¾æ•°: {filtered_count}")
        print(f"  è¿‡æ»¤æ¯”ä¾‹: {(original_count - filtered_count) / original_count * 100:.1f}%")

        return self

    def stability_selection(self, n_runs=50, subsample_ratio=0.7, cv_folds=5, stability_threshold=0.6):
        """ç¨³å®šæ€§æ£€éªŒï¼ˆStability Selectionï¼‰"""
        print(f"\nğŸ”„ ç¨³å®šæ€§æ£€éªŒï¼š{n_runs}æ¬¡å­é‡‡æ · + {cv_folds}æŠ˜äº¤å‰éªŒè¯")
        print(f"å­é‡‡æ ·æ¯”ä¾‹: {subsample_ratio}, ç¨³å®šæ€§é˜ˆå€¼: {stability_threshold}")

        # ä½¿ç”¨è¯„åˆ†åçš„ç‰¹å¾
        if hasattr(self, 'sorted_features_by_score'):
            features_to_test = [feat for feat, _ in self.sorted_features_by_score]
            print(f"ä½¿ç”¨è¯„åˆ†åçš„ç‰¹å¾: {len(features_to_test)} ä¸ª")
        elif hasattr(self, 'screened_features'):
            features_to_test = self.screened_features
            print(f"ä½¿ç”¨ç­›é€‰åçš„ç‰¹å¾: {len(features_to_test)} ä¸ª")
        else:
            features_to_test = self.candidate_features
            print(f"ä½¿ç”¨å€™é€‰ç‰¹å¾: {len(features_to_test)} ä¸ª")

        if not features_to_test:
            raise ValueError("æ²¡æœ‰å¯ç”¨äºç¨³å®šæ€§æ£€éªŒçš„ç‰¹å¾")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[features_to_test]
        y = self.df[target]

        # è®°å½•æ¯ä¸ªç‰¹å¾åœ¨æ¯æ¬¡è¿è¡Œä¸­çš„é€‰æ‹©æƒ…å†µ
        feature_selection_counts = defaultdict(int)
        total_selections = 0

        print(f"\nå¼€å§‹ {n_runs} æ¬¡ç¨³å®šæ€§æ£€éªŒ...")

        import random
        random.seed(42)
        np.random.seed(42)

        for run_idx in range(n_runs):
            if (run_idx + 1) % 10 == 0:
                print(f"  å®Œæˆ {run_idx + 1}/{n_runs} æ¬¡è¿è¡Œ")

            # å­é‡‡æ ·
            n_samples = int(len(X) * subsample_ratio)
            sample_indices = np.random.choice(len(X), n_samples, replace=False)
            X_sub = X.iloc[sample_indices]
            y_sub = y.iloc[sample_indices]

            # äº¤å‰éªŒè¯ç‰¹å¾é€‰æ‹©
            skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=run_idx)

            for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_sub, y_sub)):
                X_train = X_sub.iloc[train_idx]
                y_train = y_sub.iloc[train_idx]

                try:
                    # ä½¿ç”¨XGBoostè¿›è¡Œç‰¹å¾é€‰æ‹©
                    xgb_selector = xgb.XGBClassifier(
                        n_estimators=100,
                        max_depth=4,
                        learning_rate=0.1,
                        random_state=42,
                        eval_metric='mlogloss'
                    )
                    xgb_selector.fit(X_train, y_train)

                    # è·å–ç‰¹å¾é‡è¦æ€§
                    importance = xgb_selector.feature_importances_

                    # é€‰æ‹©é‡è¦æ€§å¤§äºå¹³å‡å€¼çš„ç‰¹å¾
                    mean_importance = np.mean(importance)
                    selected_features = [
                        features_to_test[i] for i, imp in enumerate(importance)
                        if imp > mean_importance
                    ]

                    # è®°å½•é€‰æ‹©çš„ç‰¹å¾
                    for feature in selected_features:
                        feature_selection_counts[feature] += 1

                    total_selections += 1

                except Exception as e:
                    print(f"    è¿è¡Œ {run_idx+1} æŠ˜ {fold_idx+1} å¤±è´¥: {e}")
                    continue

        # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°
        print(f"\nğŸ“Š ç¨³å®šæ€§åˆ†æç»“æœ:")
        print(f"æ€»é€‰æ‹©æ¬¡æ•°: {total_selections}")

        stable_features = []
        feature_stability_scores = {}

        for feature in features_to_test:
            selection_frequency = feature_selection_counts[feature] / total_selections
            feature_stability_scores[feature] = selection_frequency

            if selection_frequency >= stability_threshold:
                stable_features.append(feature)

        # æŒ‰ç¨³å®šæ€§åˆ†æ•°æ’åº
        sorted_by_stability = sorted(feature_stability_scores.items(),
                                   key=lambda x: x[1],
                                   reverse=True)

        # ä¿å­˜ç»“æœ
        self.stability_scores = feature_stability_scores
        self.stable_features = stable_features
        self.sorted_features_by_stability = sorted_by_stability

        print(f"\nğŸ† ç¨³å®šæ€§æ£€éªŒç»“æœ:")
        print(f"  ç¨³å®šç‰¹å¾æ•°é‡: {len(stable_features)} (é˜ˆå€¼: {stability_threshold})")
        print(f"  åŸå§‹ç‰¹å¾æ•°é‡: {len(features_to_test)}")
        print(f"  ç¨³å®šæ€§æ¯”ä¾‹: {len(stable_features) / len(features_to_test) * 100:.1f}%")

        # æ‰“å°Top 15ç¨³å®šç‰¹å¾
        print(f"\nğŸ“‹ Top 15 æœ€ç¨³å®šç‰¹å¾:")
        print(f"{'ç‰¹å¾å':<35} {'ç¨³å®šæ€§åˆ†æ•°':>12} {'çŠ¶æ€':>8}")
        print("-" * 60)

        for i, (feature, score) in enumerate(sorted_by_stability[:15]):
            status = "âœ…ç¨³å®š" if score >= stability_threshold else "âŒä¸ç¨³å®š"
            print(f"{feature:<35} {score:>12.3f} {status:>8}")

        # æŒ‰ç±»åˆ«åˆ†æç¨³å®šæ€§
        if hasattr(self, 'feature_categories'):
            print(f"\nğŸ“Š æŒ‰ç±»åˆ«åˆ†æç¨³å®šæ€§:")
            for category, original_features in self.feature_categories.items():
                stable_in_category = [f for f in original_features if f in stable_features]
                total_in_category = len([f for f in original_features if f in features_to_test])
                if total_in_category > 0:
                    stability_ratio = len(stable_in_category) / total_in_category
                    print(f"  {category:15}: {len(stable_in_category):2d}/{total_in_category:2d} "
                          f"({stability_ratio*100:5.1f}%) ç¨³å®š")

        print(f"\nâœ… ç¨³å®šæ€§æ£€éªŒå®Œæˆ")

        return self

    def optimize_xgboost_hyperparameters(self, n_trials=100, cv_folds=5):
        """ä½¿ç”¨Optunaä¼˜åŒ–XGBoostè¶…å‚æ•°"""
        print(f"\nğŸ¯ ä½¿ç”¨Optunaä¼˜åŒ–XGBoostè¶…å‚æ•° (è¯•éªŒæ¬¡æ•°: {n_trials})")

        # ç¡®å®šç›®æ ‡å˜é‡
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.candidate_features]
        y = self.df[target]

        # è®¡ç®—ç±»åˆ«æƒé‡
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))
        print(f"  ç±»åˆ«æƒé‡: {class_weight_dict}")

        def objective(trial):
            # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'gamma': trial.suggest_float('gamma', 0, 5),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                'random_state': 42,
                'eval_metric': 'mlogloss',
                'early_stopping_rounds': 50,
                'n_jobs': -1
            }

            # äº¤å‰éªŒè¯è¯„ä¼°
            skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            scores = []

            for train_idx, val_idx in skf.split(X, y):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                # åˆ›å»ºXGBoostæ¨¡å‹
                model = xgb.XGBClassifier(**params)

                # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ—©åœï¼‰
                model.fit(
                    X_train, y_train,
                    eval_set=[(X_val, y_val)],
                    verbose=False
                )

                # é¢„æµ‹å’Œè¯„ä¼°
                y_pred = model.predict(X_val)
                f1 = f1_score(y_val, y_pred, average='macro')
                scores.append(f1)

            return np.mean(scores)

        # åˆ›å»ºOptunaç ”ç©¶
        study = optuna.create_study(direction='maximize',
                                   sampler=optuna.samplers.TPESampler(seed=42))

        # ä¼˜åŒ–è¶…å‚æ•°
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        # ä¿å­˜æœ€ä½³å‚æ•°
        self.best_params = study.best_params
        self.best_score = study.best_value

        print(f"âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ")
        print(f"  æœ€ä½³F1åˆ†æ•°: {self.best_score:.4f}")
        print(f"  æœ€ä½³å‚æ•°: {self.best_params}")

        return self.best_params

    def stage_aware_feature_refinement(self, target_features=35):
        """ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰ï¼šæŒ‰æ”»å‡»é˜¶æ®µåˆ†å±‚é€‰æ‹©ç‰¹å¾"""
        print(f"\nğŸ¯ ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰ï¼šæŒ‰æ”»å‡»é˜¶æ®µåˆ†å±‚é€‰æ‹©ç‰¹å¾ï¼ˆç›®æ ‡: {target_features}ä¸ªï¼‰")

        # ä½¿ç”¨ç¨³å®šç‰¹å¾ä½œä¸ºåŸºç¡€
        if hasattr(self, 'stable_features') and self.stable_features:
            base_features = self.stable_features
            print(f"ä½¿ç”¨ç¨³å®šç‰¹å¾ä½œä¸ºåŸºç¡€: {len(base_features)} ä¸ª")
        elif hasattr(self, 'sorted_features_by_score'):
            # ä½¿ç”¨è¯„åˆ†å‰50%çš„ç‰¹å¾
            top_half = len(self.sorted_features_by_score) // 2
            base_features = [feat for feat, _ in self.sorted_features_by_score[:top_half]]
            print(f"ä½¿ç”¨è¯„åˆ†å‰50%ç‰¹å¾ä½œä¸ºåŸºç¡€: {len(base_features)} ä¸ª")
        else:
            raise ValueError("è¯·å…ˆæ‰§è¡Œç‰¹å¾è¯„åˆ†æˆ–ç¨³å®šæ€§æ£€éªŒ")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[base_features]
        y = self.df[target]

        # æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®
        stage_data = {}
        stage_names = {0: 'Normal', 1: 'Reconnaissance', 2: 'Establish_Foothold',
                      3: 'Lateral_Movement', 4: 'Data_Exfiltration'}

        for stage_id in y.unique():
            stage_mask = (y == stage_id)
            stage_data[stage_id] = {
                'name': stage_names.get(stage_id, f'Stage_{stage_id}'),
                'X': X[stage_mask],
                'y': y[stage_mask],
                'count': stage_mask.sum()
            }

        print(f"\nğŸ“Š æ”»å‡»é˜¶æ®µæ•°æ®åˆ†å¸ƒ:")
        for stage_id, data in stage_data.items():
            print(f"  {data['name']:<20}: {data['count']:>6} æ ·æœ¬")

        # 1. é˜¶æ®µé€šç”¨ç‰¹å¾é€‰æ‹©
        print(f"\n1ï¸âƒ£ é€‰æ‹©é˜¶æ®µé€šç”¨ç‰¹å¾")
        universal_features = []

        # å¯¹æ¯ä¸ªé˜¶æ®µè¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æ
        stage_feature_importance = {}

        for stage_id, data in stage_data.items():
            if data['count'] < 50:  # æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
                print(f"  è·³è¿‡ {data['name']} (æ ·æœ¬æ•°ä¸è¶³)")
                continue

            try:
                # åˆ›å»ºäºŒåˆ†ç±»é—®é¢˜ï¼šå½“å‰é˜¶æ®µ vs å…¶ä»–é˜¶æ®µ
                y_binary = (y == stage_id).astype(int)

                # ä½¿ç”¨XGBoostè¯„ä¼°ç‰¹å¾é‡è¦æ€§
                xgb_model = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=4,
                    learning_rate=0.1,
                    random_state=42,
                    eval_metric='logloss'
                )
                xgb_model.fit(X, y_binary)

                # è·å–ç‰¹å¾é‡è¦æ€§
                importance = xgb_model.feature_importances_
                stage_feature_importance[stage_id] = dict(zip(base_features, importance))

                print(f"  {data['name']:<20}: ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆ")

            except Exception as e:
                print(f"  {data['name']:<20}: åˆ†æå¤±è´¥ - {e}")
                continue

        # è®¡ç®—æ¯ä¸ªç‰¹å¾åœ¨å„é˜¶æ®µçš„å¹³å‡é‡è¦æ€§
        feature_universal_scores = {}
        for feature in base_features:
            scores = []
            for stage_id, importance_dict in stage_feature_importance.items():
                if feature in importance_dict:
                    scores.append(importance_dict[feature])

            if scores:
                # ä½¿ç”¨å¹³å‡å€¼å’Œæœ€å°å€¼çš„ç»„åˆï¼ˆç¡®ä¿åœ¨æ‰€æœ‰é˜¶æ®µéƒ½æœ‰ä¸€å®šé‡è¦æ€§ï¼‰
                avg_score = np.mean(scores)
                min_score = np.min(scores)
                universal_score = 0.7 * avg_score + 0.3 * min_score
                feature_universal_scores[feature] = universal_score

        # é€‰æ‹©é€šç”¨ç‰¹å¾ï¼ˆæŒ‰é€šç”¨åˆ†æ•°æ’åºï¼‰
        sorted_universal = sorted(feature_universal_scores.items(),
                                key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰60%ä½œä¸ºé€šç”¨ç‰¹å¾
        n_universal = max(int(target_features * 0.6), 15)  # è‡³å°‘15ä¸ªé€šç”¨ç‰¹å¾
        universal_features = [feat for feat, _ in sorted_universal[:n_universal]]

        print(f"  é€‰æ‹©é€šç”¨ç‰¹å¾: {len(universal_features)} ä¸ª")

        # 2. é˜¶æ®µä¸“ç”¨ç‰¹å¾é€‰æ‹©
        print(f"\n2ï¸âƒ£ é€‰æ‹©é˜¶æ®µä¸“ç”¨ç‰¹å¾")
        stage_specific_features = []

        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é˜¶æ®µç‰¹å¼‚æ€§
        feature_specificity_scores = {}
        for feature in base_features:
            if feature in universal_features:
                continue  # è·³è¿‡å·²é€‰æ‹©çš„é€šç”¨ç‰¹å¾

            scores = []
            for stage_id, importance_dict in stage_feature_importance.items():
                if feature in importance_dict:
                    scores.append(importance_dict[feature])

            if scores and len(scores) > 1:
                # è®¡ç®—ç‰¹å¼‚æ€§ï¼šæœ€å¤§å€¼ä¸å¹³å‡å€¼çš„æ¯”å€¼
                max_score = np.max(scores)
                avg_score = np.mean(scores)
                specificity = max_score / (avg_score + 1e-8)  # é¿å…é™¤é›¶
                feature_specificity_scores[feature] = specificity

        # é€‰æ‹©ç‰¹å¼‚æ€§é«˜çš„ç‰¹å¾
        sorted_specific = sorted(feature_specificity_scores.items(),
                               key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰©ä½™çš„ç‰¹å¾ä½œä¸ºä¸“ç”¨ç‰¹å¾
        n_specific = target_features - len(universal_features)
        stage_specific_features = [feat for feat, _ in sorted_specific[:n_specific]]

        print(f"  é€‰æ‹©ä¸“ç”¨ç‰¹å¾: {len(stage_specific_features)} ä¸ª")

        # 3. åˆå¹¶æœ€ç»ˆç‰¹å¾é›†
        final_features = universal_features + stage_specific_features

        # ä¿å­˜ç»“æœ
        self.refined_features = final_features
        self.universal_features = universal_features
        self.stage_specific_features = stage_specific_features
        self.stage_feature_importance = stage_feature_importance

        print(f"\nâœ… ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰å®Œæˆ:")
        print(f"  é€šç”¨ç‰¹å¾: {len(universal_features)} ä¸ª")
        print(f"  ä¸“ç”¨ç‰¹å¾: {len(stage_specific_features)} ä¸ª")
        print(f"  æœ€ç»ˆç‰¹å¾: {len(final_features)} ä¸ª")

        # æ‰“å°æœ€ç»ˆç‰¹å¾åˆ—è¡¨
        print(f"\nğŸ“‹ æœ€ç»ˆç‰¹å¾åˆ—è¡¨:")
        print(f"é€šç”¨ç‰¹å¾ ({len(universal_features)} ä¸ª):")
        for i, feat in enumerate(universal_features, 1):
            print(f"  {i:2d}. {feat}")

        if stage_specific_features:
            print(f"\nä¸“ç”¨ç‰¹å¾ ({len(stage_specific_features)} ä¸ª):")
            for i, feat in enumerate(stage_specific_features, 1):
                print(f"  {i:2d}. {feat}")

        # æŒ‰ç±»åˆ«åˆ†ææœ€ç»ˆç‰¹å¾
        if hasattr(self, 'feature_categories'):
            print(f"\nğŸ“Š æœ€ç»ˆç‰¹å¾ç±»åˆ«åˆ†å¸ƒ:")
            for category, original_features in self.feature_categories.items():
                final_in_category = [f for f in original_features if f in final_features]
                if final_in_category:
                    print(f"  {category:15}: {len(final_in_category):2d} ä¸ª - {final_in_category}")

        return self

    def evaluate_with_fold_internal_pipeline(self, n_features=None, cv_folds=10,
                                           optimize_hyperparams=True, n_trials=50):
        """æŒ‰è®ºæ–‡æ–¹æ³•ï¼šæ¯æŠ˜å†…éƒ¨åšç‰¹å¾é€‰æ‹©+åˆ†ç±»çš„å®Œæ•´pipelineï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        print(f"\nğŸ¯ XGBoostè¯„ä¼°ï¼šç‰¹å¾è¿‡æ»¤â†’è¶…å‚æ•°ä¼˜åŒ–â†’ç‰¹å¾é€‰æ‹©â†’åˆ†ç±»")

        # 1. è¿‡æ»¤å€™é€‰ç‰¹å¾ï¼Œé¿å…è¿‡æ‹Ÿåˆ
        self.filter_candidate_features(remove_overfitting_features=True, include_paper_features=True)

        # 2. è¶…å‚æ•°ä¼˜åŒ–
        if optimize_hyperparams:
            best_params = self.optimize_xgboost_hyperparameters(n_trials=n_trials, cv_folds=5)
        else:
            # ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å‚æ•°
            best_params = {
                'n_estimators': 500,
                'max_depth': 6,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'min_child_weight': 1,
                'gamma': 0,
                'reg_alpha': 0,
                'reg_lambda': 1,
                'random_state': 42,
                'eval_metric': 'mlogloss',
                'early_stopping_rounds': 50,
                'n_jobs': -1
            }

        # 3. å¦‚æœæ²¡æœ‰æŒ‡å®šç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨XGBoostå¯»æ‰¾æœ€ä¼˜æ•°é‡
        if n_features is None:
            print(f"  æœªæŒ‡å®šç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨XGBoostå¯»æ‰¾æœ€ä¼˜ç‰¹å¾æ•°é‡...")
            n_features = self.find_optimal_feature_count_with_shap()
            # ä½¿ç”¨XGBoosté€‰æ‹©çš„æœ€ä¼˜ç‰¹å¾
            if hasattr(self, 'optimal_features'):
                self.candidate_features = self.optimal_features
                print(f"  ä½¿ç”¨XGBoosté€‰æ‹©çš„{len(self.optimal_features)}ä¸ªæœ€ä¼˜ç‰¹å¾")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.candidate_features]
        y = self.df[target]

        print(f"å€™é€‰ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"ä½¿ç”¨ç‰¹å¾æ•°é‡: {n_features}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")

        # æ˜¾ç¤ºæ­£ç¡®çš„ç±»åˆ«åˆ†å¸ƒ
        class_counts = dict(y.value_counts().sort_index())
        print(f"ç±»åˆ«åˆ†å¸ƒ: {class_counts}")
        print(f"  0(æ­£å¸¸): {class_counts.get(0, 0)} æ ·æœ¬")
        print(f"  1(æ•°æ®æ³„éœ²): {class_counts.get(1, 0)} æ ·æœ¬")
        print(f"  2(å»ºç«‹ç«‹è¶³ç‚¹): {class_counts.get(2, 0)} æ ·æœ¬")
        print(f"  3(æ¨ªå‘ç§»åŠ¨): {class_counts.get(3, 0)} æ ·æœ¬")
        print(f"  4(ä¾¦å¯Ÿ): {class_counts.get(4, 0)} æ ·æœ¬")

        # è®¡ç®—ç±»åˆ«æƒé‡
        classes = np.unique(y)
        class_weights = compute_class_weight('balanced', classes=classes, y=y)
        class_weight_dict = dict(zip(classes, class_weights))
        print(f"ç±»åˆ«æƒé‡: {class_weight_dict}")

        # ä½¿ç”¨ä¼˜åŒ–åçš„XGBoostæ¨¡å‹
        xgb_model = xgb.XGBClassifier(**best_params)

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

        print(f"\nğŸš€ è¯„ä¼°ä¼˜åŒ–åçš„XGBoostæ¨¡å‹")

        # æ„å»ºæŠ˜å†…Pipelineï¼šç‰¹å¾é€‰æ‹© â†’ XGBooståˆ†ç±»å™¨
        feature_selector = SelectFromModel(
            xgb.XGBClassifier(
                n_estimators=100,
                random_state=42,
                eval_metric='mlogloss',
                early_stopping_rounds=10
            ),
            threshold=-np.inf,
            max_features=n_features,
            importance_getter='feature_importances_'
        )

        pipeline = Pipeline([
            ('feat_sel', feature_selector),
            ('clf', xgb_model)
        ])

        # æ‰‹åŠ¨äº¤å‰éªŒè¯ä»¥æ”¯æŒæ—©åœ
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring_results = {
            'accuracy': [], 'precision_macro': [], 'recall_macro': [], 'f1_macro': []
        }
        selected_features_per_fold = []

        print(f"å¼€å§‹{cv_folds}æŠ˜äº¤å‰éªŒè¯...")

        for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # ç‰¹å¾é€‰æ‹©
            feature_selector.fit(X_train, y_train)
            X_train_selected = feature_selector.transform(X_train)
            X_test_selected = feature_selector.transform(X_test)

            # è·å–é€‰æ‹©çš„ç‰¹å¾å
            selected_mask = feature_selector.get_support()
            selected_feats = X.columns[selected_mask].tolist()
            selected_features_per_fold.append(selected_feats)

            # åˆ†å‰²è®­ç»ƒé›†ä¸ºè®­ç»ƒå’ŒéªŒè¯é›†ï¼ˆç”¨äºæ—©åœï¼‰
            X_tr, X_val, y_tr, y_val = train_test_split(
                X_train_selected, y_train, test_size=0.2,
                random_state=42, stratify=y_train
            )

            # è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆå¸¦æ—©åœï¼‰
            model = xgb.XGBClassifier(**best_params)
            model.fit(
                X_tr, y_tr,
                eval_set=[(X_val, y_val)],
                verbose=False
            )

            # é¢„æµ‹
            y_pred = model.predict(X_test_selected)

            # è®¡ç®—æŒ‡æ ‡
            scoring_results['accuracy'].append(accuracy_score(y_test, y_pred))
            scoring_results['precision_macro'].append(precision_score(y_test, y_pred, average='macro'))
            scoring_results['recall_macro'].append(recall_score(y_test, y_pred, average='macro'))
            scoring_results['f1_macro'].append(f1_score(y_test, y_pred, average='macro'))

            print(f"Fold {fold_idx} é€‰å‡ºçš„ {len(selected_feats)} ä¸ªç‰¹å¾ï¼š")
            print(selected_feats)

        # è®¡ç®—ç»Ÿè®¡é‡
        results = {}
        for metric, scores in scoring_results.items():
            scores_array = np.array(scores)
            results[metric] = {
                'mean': scores_array.mean(),
                'std': scores_array.std(),
                'scores': scores
            }
            print(f"  {metric:<15}: {scores_array.mean():.4f} Â± {scores_array.std():.4f}")

        self.cv_results = {
            'XGBoost': results,
            'selected_features_per_fold': selected_features_per_fold,
            'best_params': best_params,
            'class_weights': class_weight_dict
        }

        print(f"\nğŸ† ä¼˜åŒ–åXGBoostæ¨¡å‹ F1: {results['f1_macro']['mean']:.4f}")
        print(f"ğŸ“Š ä½¿ç”¨çš„æœ€ä½³å‚æ•°: {best_params}")
        print(f"âš–ï¸ ç±»åˆ«æƒé‡: {class_weight_dict}")

        return self

    def evaluate_multiple_models(self, cv_folds=10):
        """æŒ‰è®ºæ–‡æ–¹æ³•ï¼šç”¨å›ºå®šçš„46ä¸ªç‰¹å¾ï¼Œåœ¨10æŠ˜CVä¸­è¯„ä¼°å¤šæ¨¡å‹"""
        print(f"\nğŸ¯ ä½¿ç”¨å›ºå®šçš„{len(self.selected_features)}ä¸ªç‰¹å¾è¿›è¡Œ{cv_folds}æŠ˜äº¤å‰éªŒè¯")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")

        # æŒ‰è®ºæ–‡é…ç½®æ¨¡å‹è¶…å‚æ•°
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=0.01,
                random_state=42
            ),
            'SVM': SVC(
                kernel='rbf',
                C=1.0,
                random_state=42
            )
        }

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

        self.cv_results = {}

        for name, clf in models.items():
            print(f"\nğŸš€ è¯„ä¼°æ¨¡å‹: {name}")

            # å¯¹MLPå’ŒSVMä½¿ç”¨Pipelineè¿›è¡ŒæŠ˜å†…æ ‡å‡†åŒ–
            if name in ('MLP', 'SVM'):
                estimator = Pipeline([
                    ('scaler', StandardScaler()),
                    ('clf', clf)
                ])
            else:
                estimator = clf

            # ä½¿ç”¨cross_validateè¿›è¡Œè¯„ä¼°
            from sklearn.model_selection import cross_validate
            cv_results = cross_validate(
                estimator, X, y,
                cv=skf,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False
            )

            # è®¡ç®—ç»Ÿè®¡é‡
            results = {}
            for metric in scoring:
                scores = cv_results[f'test_{metric}']
                results[metric] = {
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'scores': scores.tolist()
                }
                print(f"  {metric:<15}: {scores.mean():.4f} Â± {scores.std():.4f}")

            self.cv_results[name] = results

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        best_f1 = self.cv_results[best_model]['f1_macro']['mean']
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (F1: {best_f1:.4f})")

        return self

    def detailed_model_analysis(self, cv_folds=10):
        """è¯¦ç»†åˆ†ææœ€ä½³æ¨¡å‹ï¼šèšåˆ10æŠ˜CVçš„é¢„æµ‹ç»“æœ"""
        print(f"\nğŸ” è¯¦ç»†æ¨¡å‹åˆ†æ (èšåˆ{cv_folds}æŠ˜ç»“æœ)")

        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        print(f"ğŸ† åˆ†ææœ€ä½³æ¨¡å‹: {best_model}")

        # é‡å»ºæœ€ä½³æ¨¡å‹
        if best_model == 'RandomForest':
            clf = RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            )
            estimator = clf
        elif best_model == 'MLP':
            clf = MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=1e-3,
                random_state=42
            )
            estimator = Pipeline([('scaler', StandardScaler()), ('clf', clf)])
        else:  # SVM
            clf = SVC(kernel='rbf', C=1.0, random_state=42)
            estimator = Pipeline([('scaler', StandardScaler()), ('clf', clf)])

        # 10æŠ˜äº¤å‰éªŒè¯ï¼Œæ”¶é›†æ‰€æœ‰é¢„æµ‹
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        y_true_all, y_pred_all = [], []

        print("ğŸ”„ æ‰§è¡Œ10æŠ˜äº¤å‰éªŒè¯...")
        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # è®­ç»ƒå’Œé¢„æµ‹
            estimator.fit(X_train, y_train)
            y_pred = estimator.predict(X_test)

            y_true_all.extend(y_test.values)
            y_pred_all.extend(y_pred)

            print(f"  æŠ˜ {fold:2d} å®Œæˆ")

        # èšåˆç»“æœ
        y_true = np.array(y_true_all)
        y_pred = np.array(y_pred_all)

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='macro')
        recall = recall_score(y_true, y_pred, average='macro')
        f1 = f1_score(y_true, y_pred, average='macro')

        print(f"\nğŸ“Š èšåˆç»“æœ ({best_model}):")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"  å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"  F1åˆ†æ•° (F1-Score): {f1:.4f}")

        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred))

        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print(confusion_matrix(y_true, y_pred))

        # æ·»åŠ æ¯ä¸ªæ”»å‡»é˜¶æ®µçš„è¯¦ç»†åˆ†æ
        self._analyze_per_stage_performance(y_true, y_pred)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.models_performance = {
            'best_model': best_model,
            'aggregated_metrics': {
                'accuracy': accuracy,
                'precision_macro': precision,
                'recall_macro': recall,
                'f1_macro': f1
            },
            'classification_report': classification_report(y_true, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
        }

        return self

    def _analyze_per_stage_performance(self, y_true, y_pred):
        """åˆ†ææ¯ä¸ªAPTæ”»å‡»é˜¶æ®µçš„æ£€æµ‹æ€§èƒ½"""
        print(f"\nğŸ¯ æ¯ä¸ªAPTæ”»å‡»é˜¶æ®µçš„æ£€æµ‹æ€§èƒ½åˆ†æ:")
        print("="*60)

        # å®šä¹‰é˜¶æ®µæ˜ å°„
        stage_names = {
            0: 'Benign (æ­£å¸¸æµé‡)',
            4: 'Data Exfiltration (æ•°æ®æ¸—é€)',
            2: 'Establish Foothold (å»ºç«‹ç«‹è¶³ç‚¹)',
            3: 'Lateral Movement (æ¨ªå‘ç§»åŠ¨)',
            1: 'Reconnaissance (ä¾¦å¯Ÿ)'
        }

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)

        print(f"{'é˜¶æ®µ':<25} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'æ ·æœ¬æ•°':<8} {'æ£€æµ‹ç‡':<10}")
        print("-" * 80)

        total_correct = 0
        total_samples = 0

        for i, (stage_id, stage_name) in enumerate(stage_names.items()):
            if i < len(precision):
                # è®¡ç®—æ£€æµ‹ç‡ (è¯¥é˜¶æ®µè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹)
                detection_rate = cm[i, i] / support[i] if support[i] > 0 else 0

                print(f"{stage_name:<25} {precision[i]:<10.4f} {recall[i]:<10.4f} "
                      f"{f1[i]:<10.4f} {support[i]:<8d} {detection_rate:<10.4f}")

                total_correct += cm[i, i]
                total_samples += support[i]

        print("-" * 80)
        overall_accuracy = total_correct / total_samples if total_samples > 0 else 0
        print(f"{'æ€»ä½“å‡†ç¡®ç‡':<25} {'':<10} {'':<10} {'':<10} {total_samples:<8d} {overall_accuracy:<10.4f}")

        # åˆ†ææ”»å‡»é˜¶æ®µé—´çš„æ··æ·†æƒ…å†µ
        print(f"\nğŸ” æ”»å‡»é˜¶æ®µé—´æ··æ·†åˆ†æ:")
        print("="*50)

        for i, (true_stage_id, true_stage_name) in enumerate(stage_names.items()):
            if i < len(cm):
                print(f"\nçœŸå®é˜¶æ®µ: {true_stage_name}")
                for j, (pred_stage_id, pred_stage_name) in enumerate(stage_names.items()):
                    if j < len(cm[i]) and cm[i, j] > 0:
                        confusion_rate = cm[i, j] / support[i] if support[i] > 0 else 0
                        if i != j:  # åªæ˜¾ç¤ºé”™è¯¯åˆ†ç±»
                            print(f"  â†’ è¯¯åˆ†ç±»ä¸º {pred_stage_name}: {cm[i, j]} æ ·æœ¬ ({confusion_rate:.3f})")

        # è®¡ç®—å®å¹³å‡æŒ‡æ ‡
        macro_precision = np.mean(precision)
        macro_recall = np.mean(recall)
        macro_f1 = np.mean(f1)

        print(f"\nğŸ“Š å®å¹³å‡æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å®å¹³å‡ç²¾ç¡®ç‡: {macro_precision:.4f}")
        print(f"  å®å¹³å‡å¬å›ç‡: {macro_recall:.4f}")
        print(f"  å®å¹³å‡F1åˆ†æ•°: {macro_f1:.4f}")

        # è¯†åˆ«è¡¨ç°æœ€å¥½å’Œæœ€å·®çš„é˜¶æ®µ
        best_stage_idx = np.argmax(f1)
        worst_stage_idx = np.argmin(f1)

        print(f"\nğŸ† æ€§èƒ½åˆ†æ:")
        print(f"  æœ€ä½³æ£€æµ‹é˜¶æ®µ: {list(stage_names.values())[best_stage_idx]} (F1: {f1[best_stage_idx]:.4f})")
        print(f"  æœ€å·®æ£€æµ‹é˜¶æ®µ: {list(stage_names.values())[worst_stage_idx]} (F1: {f1[worst_stage_idx]:.4f})")

        # ä¿å­˜æ¯é˜¶æ®µæ€§èƒ½
        self.per_stage_performance = {
            'stage_names': stage_names,
            'precision': precision.tolist(),
            'recall': recall.tolist(),
            'f1_score': f1.tolist(),
            'support': support.tolist(),
            'confusion_matrix': cm.tolist(),
            'macro_avg': {
                'precision': macro_precision,
                'recall': macro_recall,
                'f1_score': macro_f1
            }
        }

    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {self.output_path}")

        # ä¿å­˜å¢å¼ºåçš„æ•°æ®
        enhanced_data_path = os.path.join(self.output_path, 'enhanced_apt_data.csv')
        self.df.to_csv(enhanced_data_path, index=False)
        print(f"âœ… å¢å¼ºæ•°æ®ä¿å­˜è‡³: {enhanced_data_path}")

        # ä¿å­˜é€‰æ‹©çš„ç‰¹å¾æ•°æ®
        if self.selected_features:
            target_col = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
            selected_cols = self.selected_features + [target_col, 'Stage']
            selected_data_path = os.path.join(self.output_path, 'selected_features_data.csv')
            self.df[selected_cols].to_csv(selected_data_path, index=False)
            print(f"âœ… é€‰æ‹©ç‰¹å¾æ•°æ®ä¿å­˜è‡³: {selected_data_path}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            importance_path = os.path.join(self.output_path, 'feature_importance.csv')
            self.feature_importance.to_csv(importance_path, index=False)
            print(f"âœ… ç‰¹å¾é‡è¦æ€§ä¿å­˜è‡³: {importance_path}")

        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        if self.cv_results:
            cv_path = os.path.join(self.output_path, 'cross_validation_results.json')
            with open(cv_path, 'w') as f:
                json.dump(self.cv_results, f, indent=4)
            print(f"âœ… äº¤å‰éªŒè¯ç»“æœä¿å­˜è‡³: {cv_path}")

        # ä¿å­˜è¯¦ç»†æ¨¡å‹æ€§èƒ½
        if self.models_performance:
            models_path = os.path.join(self.output_path, 'models_performance.json')
            with open(models_path, 'w') as f:
                json.dump(self.models_performance, f, indent=4)
            print(f"âœ… æ¨¡å‹æ€§èƒ½ç»“æœä¿å­˜è‡³: {models_path}")

        # ä¿å­˜æ¯é˜¶æ®µæ€§èƒ½åˆ†æ
        if hasattr(self, 'per_stage_performance'):
            stage_path = os.path.join(self.output_path, 'per_stage_performance.json')
            with open(stage_path, 'w') as f:
                json.dump(self.per_stage_performance, f, indent=4)
            print(f"âœ… æ¯é˜¶æ®µæ€§èƒ½åˆ†æä¿å­˜è‡³: {stage_path}")

        return self

    def run_domain_knowledge_feature_selection(self, target_features=35,
                                              correlation_threshold=0.9,
                                              variance_threshold=0.01,
                                              stability_runs=50,
                                              stability_threshold=0.6):
        """è¿è¡ŒåŸºäºåŸŸçŸ¥è¯†çš„å®Œæ•´ç‰¹å¾é€‰æ‹©æµç¨‹"""
        print("ğŸš€ å¼€å§‹åŸºäºåŸŸçŸ¥è¯†çš„ç‰¹å¾é€‰æ‹©æµç¨‹")
        print("="*80)
        print("æµç¨‹: åŸŸçŸ¥è¯†åˆ†ç±» â†’ åˆç­› â†’ è‡ªåŠ¨åŒ–æ‰“åˆ† â†’ ç¨³å®šæ€§æ£€éªŒ â†’ ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰")
        print("="*80)

        start_time = time.time()

        try:
            # 1. åŸŸçŸ¥è¯†ç‰¹å¾åˆ†ç±»
            print(f"\n{'='*20} æ­¥éª¤ 1: åŸŸçŸ¥è¯†ç‰¹å¾åˆ†ç±» {'='*20}")
            self.domain_knowledge_feature_classification()

            # 2. åˆç­›ï¼šå»ç›¸å…³æ€§å’Œä½æ–¹å·®ç‰¹å¾
            print(f"\n{'='*20} æ­¥éª¤ 2: åˆç­›é˜¶æ®µ {'='*20}")
            self.initial_feature_screening(
                correlation_threshold=correlation_threshold,
                variance_threshold=variance_threshold
            )

            # 3. è‡ªåŠ¨åŒ–æ‰“åˆ†
            print(f"\n{'='*20} æ­¥éª¤ 3: è‡ªåŠ¨åŒ–æ‰“åˆ† {'='*20}")
            self.automated_feature_scoring(use_screened_features=True)

            # 4. ç¨³å®šæ€§æ£€éªŒ
            print(f"\n{'='*20} æ­¥éª¤ 4: ç¨³å®šæ€§æ£€éªŒ {'='*20}")
            self.stability_selection(
                n_runs=stability_runs,
                stability_threshold=stability_threshold
            )

            # 5. ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰
            print(f"\n{'='*20} æ­¥éª¤ 5: ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰ {'='*20}")
            self.stage_aware_feature_refinement(target_features=target_features)

            # 6. æ›´æ–°å€™é€‰ç‰¹å¾ä¸ºæœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾
            if hasattr(self, 'refined_features'):
                self.candidate_features = self.refined_features
                self.selected_features = self.refined_features
                print(f"\nâœ… æ›´æ–°å€™é€‰ç‰¹å¾ä¸ºæœ€ç»ˆé€‰æ‹©çš„ {len(self.refined_features)} ä¸ªç‰¹å¾")

            total_time = time.time() - start_time

            # 7. ç”Ÿæˆç‰¹å¾é€‰æ‹©æŠ¥å‘Š
            self._generate_feature_selection_report(total_time)

        except Exception as e:
            print(f"âŒ ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        return self

    def _generate_feature_selection_report(self, total_time):
        """ç”Ÿæˆç‰¹å¾é€‰æ‹©æŠ¥å‘Š"""
        print(f"\n{'='*20} ç‰¹å¾é€‰æ‹©å®ŒæˆæŠ¥å‘Š {'='*20}")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")

        # ç»Ÿè®¡å„é˜¶æ®µçš„ç‰¹å¾æ•°é‡å˜åŒ–
        stages_stats = []

        if hasattr(self, 'feature_categories'):
            original_count = sum(len(features) for features in self.feature_categories.values())
            stages_stats.append(("åŸå§‹ç‰¹å¾", original_count))

        if hasattr(self, 'screened_features'):
            stages_stats.append(("åˆç­›å", len(self.screened_features)))

        if hasattr(self, 'stable_features'):
            stages_stats.append(("ç¨³å®šæ€§æ£€éªŒå", len(self.stable_features)))

        if hasattr(self, 'refined_features'):
            stages_stats.append(("æœ€ç»ˆé€‰æ‹©", len(self.refined_features)))

        print(f"\nğŸ“Š ç‰¹å¾æ•°é‡å˜åŒ–:")
        for stage_name, count in stages_stats:
            print(f"  {stage_name:<15}: {count:>4} ä¸ªç‰¹å¾")

        if len(stages_stats) >= 2:
            reduction_ratio = (stages_stats[0][1] - stages_stats[-1][1]) / stages_stats[0][1]
            print(f"  ç‰¹å¾å‡å°‘æ¯”ä¾‹: {reduction_ratio*100:.1f}%")

        # æŒ‰ç±»åˆ«åˆ†ææœ€ç»ˆç‰¹å¾åˆ†å¸ƒ
        if hasattr(self, 'feature_categories') and hasattr(self, 'refined_features'):
            print(f"\nğŸ“‹ æœ€ç»ˆç‰¹å¾ç±»åˆ«åˆ†å¸ƒ:")
            for category, original_features in self.feature_categories.items():
                final_in_category = [f for f in original_features if f in self.refined_features]
                if final_in_category:
                    print(f"  {category:15}: {len(final_in_category):2d}/{len(original_features):2d} "
                          f"({len(final_in_category)/len(original_features)*100:5.1f}%)")

        # ç‰¹å¾è´¨é‡è¯„ä¼°
        if hasattr(self, 'stability_scores') and hasattr(self, 'refined_features'):
            avg_stability = np.mean([self.stability_scores.get(f, 0) for f in self.refined_features])
            print(f"\nğŸ¯ ç‰¹å¾è´¨é‡è¯„ä¼°:")
            print(f"  å¹³å‡ç¨³å®šæ€§åˆ†æ•°: {avg_stability:.3f}")

            if hasattr(self, 'feature_scores'):
                avg_combined_score = np.mean([
                    self.feature_scores.get(f, {}).get('combined', 0)
                    for f in self.refined_features
                ])
                print(f"  å¹³å‡ç»¼åˆè¯„åˆ†: {avg_combined_score:.3f}")

        # æ¨èçš„ä¸‹ä¸€æ­¥
        print(f"\nğŸ¯ æ¨èçš„ä¸‹ä¸€æ­¥:")
        print(f"  1. ä½¿ç”¨é€‰æ‹©çš„ {len(self.refined_features)} ä¸ªç‰¹å¾è¿›è¡Œæ¨¡å‹è®­ç»ƒ")
        print(f"  2. è¿›è¡Œäº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½")
        print(f"  3. æ„å»ºæ”»å‡»åºåˆ—ç”¨äºåºåˆ—ç”Ÿæˆæ¨¡å‹")

        # ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœæ‘˜è¦
        self.feature_selection_summary = {
            'total_time': total_time,
            'stages_stats': stages_stats,
            'final_feature_count': len(self.refined_features) if hasattr(self, 'refined_features') else 0,
            'average_stability': avg_stability if 'avg_stability' in locals() else 0,
            'category_distribution': {}
        }

        if hasattr(self, 'feature_categories') and hasattr(self, 'refined_features'):
            for category, original_features in self.feature_categories.items():
                final_in_category = [f for f in original_features if f in self.refined_features]
                self.feature_selection_summary['category_distribution'][category] = {
                    'original': len(original_features),
                    'final': len(final_in_category),
                    'features': final_in_category
                }

    def run_complete_pipeline(self, n_features=46, cv_folds=10):
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹")
        print("="*80)

        start_time = time.time()

        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹ï¼ˆæŒ‰è®ºæ–‡æ–¹æ³•ä¿®æ­£ï¼‰
            (self
             .load_data()
             .clean_data()
             .create_statistical_features()
             .encode_and_normalize()
             .prepare_paper_aligned_features()
             .evaluate_with_fold_internal_pipeline(n_features=n_features, cv_folds=cv_folds)
             .save_results())

            total_time = time.time() - start_time

            print(f"\nğŸ‰ é¢„å¤„ç†å’Œè¯„ä¼°å®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ¯ é€‰æ‹©ç‰¹å¾æ•°é‡: {len(self.selected_features)}")

            # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
            if self.cv_results:
                print(f"\nğŸ“ˆ è®ºæ–‡æ–¹æ³•æ¨¡å‹æ€§èƒ½æ‘˜è¦ ({cv_folds}æŠ˜äº¤å‰éªŒè¯):")
                for model_name, metrics in self.cv_results.items():
                    f1_mean = metrics['f1_macro']['mean']
                    f1_std = metrics['f1_macro']['std']
                    print(f"  {model_name}: F1={f1_mean:.4f}Â±{f1_std:.3f}")

            # æ‰“å°ç‰¹å¾é€‰æ‹©æ‘˜è¦
            if hasattr(self, 'candidate_features'):
                print(f"\nğŸ¯ è®ºæ–‡å¯¹é½ç‰¹å¾æ‘˜è¦:")
                print(f"  å€™é€‰ç‰¹å¾æ•°é‡: {len(self.candidate_features)}")

                # æ£€æŸ¥æ—¶é—´ç‰¹å¾
                time_features = ['hour', 'minute', 'day_of_week']
                time_in_candidates = [f for f in self.candidate_features if f in time_features]
                print(f"  åŒ…å«æ—¶é—´ç‰¹å¾: {time_in_candidates}")

                # æ£€æŸ¥æ˜¯å¦æ’é™¤äº†Activity
                activity_excluded = not any('activity' in f.lower() for f in self.candidate_features)
                print(f"  å·²æ’é™¤Activityæ ‡ç­¾: {'âœ…' if activity_excluded else 'âŒ'}")

                # æ˜¾ç¤ºç‰¹å¾ç±»å‹åˆ†å¸ƒ
                network_features = len([f for f in self.candidate_features if f not in time_features + ['Protocol_encoded']])
                print(f"  ç½‘ç»œæµç‰¹å¾: {network_features}")
                print(f"  æ—¶é—´ç‰¹å¾: {len(time_in_candidates)}")
                print(f"  åè®®ç‰¹å¾: {1 if 'Protocol_encoded' in self.candidate_features else 0}")

            # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
            if self.cv_results:
                best_f1 = max(metrics['f1_macro']['mean'] for metrics in self.cv_results.values())
                print(f"\nğŸ¯ ä¸è®ºæ–‡å¯¹æ¯”:")
                print(f"  æˆ‘ä»¬çš„æœ€ä½³F1: {best_f1:.4f}")
                print(f"  è®ºæ–‡æŠ¥å‘ŠF1: ~0.9800")
                print(f"  å·®è·: {0.98 - best_f1:.4f}")
                if best_f1 >= 0.97:
                    print(f"  âœ… æ¥è¿‘è®ºæ–‡æ°´å¹³ï¼")
                elif best_f1 >= 0.95:
                    print(f"  ğŸ”¶ è‰¯å¥½æ°´å¹³ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    print(f"  âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")

        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        return self

    def run_domain_knowledge_pipeline(self, target_features=35, cv_folds=10,
                                     correlation_threshold=0.9, variance_threshold=0.01,
                                     stability_runs=30, stability_threshold=0.6):
        """è¿è¡ŒåŸºäºåŸŸçŸ¥è¯†çš„å®Œæ•´é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹åŸºäºåŸŸçŸ¥è¯†çš„APTæ•°æ®é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹")
        print("="*80)
        print("ç‰¹è‰²: åŸŸçŸ¥è¯†åˆ†ç±» â†’ åˆç­› â†’ è‡ªåŠ¨åŒ–æ‰“åˆ† â†’ ç¨³å®šæ€§æ£€éªŒ â†’ ç²¾ç‚¼åˆ†å±‚æŒ‘é€‰")
        print("="*80)

        start_time = time.time()

        try:
            # 1. åŸºç¡€æ•°æ®å¤„ç†
            print(f"\n{'='*20} é˜¶æ®µ 1: åŸºç¡€æ•°æ®å¤„ç† {'='*20}")
            (self
             .load_data()
             .clean_data()
             .create_statistical_features()
             .encode_and_normalize())

            # 2. åŸŸçŸ¥è¯†ç‰¹å¾é€‰æ‹©
            print(f"\n{'='*20} é˜¶æ®µ 2: åŸŸçŸ¥è¯†ç‰¹å¾é€‰æ‹© {'='*20}")
            self.run_domain_knowledge_feature_selection(
                target_features=target_features,
                correlation_threshold=correlation_threshold,
                variance_threshold=variance_threshold,
                stability_runs=stability_runs,
                stability_threshold=stability_threshold
            )

            # 3. æ¨¡å‹è¯„ä¼°
            print(f"\n{'='*20} é˜¶æ®µ 3: æ¨¡å‹è¯„ä¼° {'='*20}")
            self.evaluate_with_fold_internal_pipeline(
                n_features=len(self.refined_features) if hasattr(self, 'refined_features') else target_features,
                cv_folds=cv_folds,
                optimize_hyperparams=True,
                n_trials=30
            )

            # 4. ä¿å­˜ç»“æœ
            print(f"\n{'='*20} é˜¶æ®µ 4: ä¿å­˜ç»“æœ {'='*20}")
            self.save_results()

            total_time = time.time() - start_time

            # 5. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self._generate_final_pipeline_report(total_time, target_features, cv_folds)

        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        return self

    def _generate_final_pipeline_report(self, total_time, target_features, cv_folds):
        """ç”Ÿæˆæœ€ç»ˆæµç¨‹æŠ¥å‘Š"""
        print(f"\n{'='*20} ğŸ‰ æµç¨‹å®ŒæˆæŠ¥å‘Š {'='*20}")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {self.df.shape}")

        if hasattr(self, 'refined_features'):
            print(f"ğŸ¯ åŸŸçŸ¥è¯†é€‰æ‹©ç‰¹å¾æ•°é‡: {len(self.refined_features)}")

        # æ‰“å°æ¨¡å‹æ€§èƒ½æ‘˜è¦
        if hasattr(self, 'cv_results') and self.cv_results:
            print(f"\nğŸ“ˆ åŸŸçŸ¥è¯†æ–¹æ³•æ¨¡å‹æ€§èƒ½æ‘˜è¦ ({cv_folds}æŠ˜äº¤å‰éªŒè¯):")
            for model_name, metrics in self.cv_results.items():
                if isinstance(metrics, dict) and 'f1_macro' in metrics:
                    f1_mean = metrics['f1_macro']['mean']
                    f1_std = metrics['f1_macro']['std']
                    print(f"  {model_name}: F1={f1_mean:.4f}Â±{f1_std:.3f}")

        # ç‰¹å¾é€‰æ‹©æ•ˆæœåˆ†æ
        if hasattr(self, 'feature_selection_summary'):
            summary = self.feature_selection_summary
            print(f"\nğŸ¯ åŸŸçŸ¥è¯†ç‰¹å¾é€‰æ‹©æ•ˆæœ:")
            print(f"  ç›®æ ‡ç‰¹å¾æ•°: {target_features}")
            print(f"  å®é™…é€‰æ‹©æ•°: {summary.get('final_feature_count', 0)}")
            print(f"  å¹³å‡ç¨³å®šæ€§: {summary.get('average_stability', 0):.3f}")

            if 'stages_stats' in summary:
                print(f"  ç‰¹å¾ç­›é€‰è¿‡ç¨‹:")
                for stage_name, count in summary['stages_stats']:
                    print(f"    {stage_name}: {count} ä¸ª")

        # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
        if hasattr(self, 'cv_results') and self.cv_results:
            best_f1 = 0
            for model_name, metrics in self.cv_results.items():
                if isinstance(metrics, dict) and 'f1_macro' in metrics:
                    f1_mean = metrics['f1_macro']['mean']
                    if f1_mean > best_f1:
                        best_f1 = f1_mean

            print(f"\nğŸ¯ ä¸è®ºæ–‡å¯¹æ¯”:")
            print(f"  åŸŸçŸ¥è¯†æ–¹æ³•æœ€ä½³F1: {best_f1:.4f}")
            print(f"  è®ºæ–‡æŠ¥å‘ŠF1: ~0.9800")
            print(f"  å·®è·: {0.98 - best_f1:.4f}")

            if best_f1 >= 0.97:
                print(f"  âœ… æ¥è¿‘è®ºæ–‡æ°´å¹³ï¼åŸŸçŸ¥è¯†æ–¹æ³•æ•ˆæœä¼˜ç§€")
            elif best_f1 >= 0.95:
                print(f"  ğŸ”¶ è‰¯å¥½æ°´å¹³ï¼ŒåŸŸçŸ¥è¯†æ–¹æ³•æœ‰æ•ˆ")
            else:
                print(f"  âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜åŸŸçŸ¥è¯†æ–¹æ³•")

        # æ¨èåç»­æ­¥éª¤
        print(f"\nğŸš€ æ¨èåç»­æ­¥éª¤:")
        print(f"  1. ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾æ„å»ºæ”»å‡»åºåˆ—")
        print(f"  2. è®­ç»ƒSeqGANç”Ÿæˆæ¨¡å‹")
        print(f"  3. è¿›è¡Œæ•°æ®å¢å¼ºå’Œæ¨¡å‹ä¼˜åŒ–")

        # ä¿å­˜åŸŸçŸ¥è¯†æ–¹æ³•çš„é…ç½®
        if hasattr(self, 'refined_features'):
            domain_config = {
                'method': 'domain_knowledge_feature_selection',
                'target_features': target_features,
                'final_features': len(self.refined_features),
                'selected_features': self.refined_features,
                'performance': best_f1 if 'best_f1' in locals() else 0,
                'processing_time': total_time
            }

            # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
            config_path = os.path.join(self.output_path, 'domain_knowledge_config.json')
            with open(config_path, 'w') as f:
                json.dump(domain_config, f, indent=4)
            print(f"âœ… åŸŸçŸ¥è¯†æ–¹æ³•é…ç½®ä¿å­˜è‡³: {config_path}")

    def build_attack_sequences(self, num_apt_sequences=1000, min_normal_insert=1, max_normal_insert=5):
        """æ„å»ºæ”»å‡»åºåˆ—ï¼Œå‚è€ƒdapt_preprocessing.pyçš„æ–¹æ³•"""
        print(f"\nğŸ”— æ„å»ºæ”»å‡»åºåˆ— (ç”Ÿæˆ{num_apt_sequences}ä¸ªAPTåºåˆ—)")

        # ç¡®å®šç›®æ ‡å˜é‡
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        # æ„å»ºattack2idæ˜ å°„
        self._build_attack2id_mapping()

        # æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®
        self._partition_data_by_stage(target)

        # æ„å»ºAPTåºåˆ—æ ‡ç­¾
        self._build_apt_sequence_labels(num_apt_sequences, min_normal_insert, max_normal_insert)

        # æ„å»ºæ­£å¸¸åºåˆ—æ ‡ç­¾
        self._build_normal_sequence_labels()

        # ä¸ºåºåˆ—é€‰æ‹©å®é™…æ•°æ®æ ·æœ¬
        self._select_samples_for_sequences()

        # è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—
        self._convert_labels_to_ids()

        # åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾
        self._assign_final_sequence_labels()

        # ä¿å­˜åºåˆ—æ•°æ®
        self._save_sequence_results()

        print(f"âœ… æ”»å‡»åºåˆ—æ„å»ºå®Œæˆ")
        print(f"  APTåºåˆ—æ•°é‡: {len(self.apt_sequences_data)}")
        print(f"  æ­£å¸¸åºåˆ—æ•°é‡: {len(self.normal_sequences_data)}")
        print(f"  Attack2IDæ˜ å°„: {self.attack2id}")

        return self

    def build_session_based_attack_sequences(self, num_apt_sequences=1000, session_timeout=300):
        """åŸºäºä¼šè¯çš„æ”»å‡»åºåˆ—æ„å»ºæ–¹æ³•"""
        print(f"\nğŸ”— æ„å»ºåŸºäºä¼šè¯çš„æ”»å‡»åºåˆ— (ç”Ÿæˆ{num_apt_sequences}ä¸ªAPTåºåˆ—)")
        print("æ–¹æ³•: é€šè¿‡æºIPã€ç›®æ ‡IPã€åè®®å’Œæ—¶é—´ä¿¡æ¯æ„å»ºä¼šè¯")

        # ç¡®å®šç›®æ ‡å˜é‡
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        # æ„å»ºattack2idæ˜ å°„
        self._build_attack2id_mapping()

        # åŸºäºä¼šè¯æ„å»ºæ”»å‡»åºåˆ—
        self._build_sessions_from_network_flows(session_timeout)

        # ä»ä¼šè¯ä¸­æå–æ”»å‡»åºåˆ—
        self._extract_attack_sequences_from_sessions(num_apt_sequences)

        # æ„å»ºæ­£å¸¸åºåˆ—
        self._build_normal_sequences_from_sessions(num_apt_sequences)

        # è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—
        self._convert_session_labels_to_ids()

        # åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾
        self._assign_session_sequence_labels()

        # ä¿å­˜åºåˆ—æ•°æ®
        self._save_session_sequence_results()

        print(f"âœ… åŸºäºä¼šè¯çš„æ”»å‡»åºåˆ—æ„å»ºå®Œæˆ")
        print(f"  APTåºåˆ—æ•°é‡: {len(self.session_apt_sequences_data)}")
        print(f"  æ­£å¸¸åºåˆ—æ•°é‡: {len(self.session_normal_sequences_data)}")
        print(f"  Attack2IDæ˜ å°„: {self.attack2id}")

        return self

    def _build_sessions_from_network_flows(self, session_timeout):
        """åŸºäºç½‘ç»œæµæ„å»ºä¼šè¯"""
        print("åŸºäºç½‘ç»œæµæ„å»ºä¼šè¯...")

        # ç¡®ä¿æœ‰æ—¶é—´æˆ³åˆ—
        if 'Timestamp' not in self.df.columns:
            print("  è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°Timestampåˆ—ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºæ—¶é—´é¡ºåº")
            self.df['Timestamp'] = pd.to_datetime(self.df.index, unit='s')
        else:
            self.df['Timestamp'] = pd.to_datetime(self.df['Timestamp'])

        # æŒ‰æ—¶é—´æ’åº
        self.df = self.df.sort_values('Timestamp')

        # å®šä¹‰ä¼šè¯é”®ï¼ˆæºIPã€ç›®æ ‡IPã€åè®®ï¼‰
        session_keys = ['Src_IP_int', 'Dst_IP_int', 'Protocol_encoded']

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        missing_cols = [col for col in session_keys if col not in self.df.columns]
        if missing_cols:
            print(f"  è­¦å‘Š: ç¼ºå°‘ä¼šè¯é”®åˆ—: {missing_cols}")
            # ä½¿ç”¨å¯ç”¨çš„åˆ—
            session_keys = [col for col in session_keys if col in self.df.columns]
            if not session_keys:
                print("  é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„ä¼šè¯é”®åˆ—")
                return

        print(f"  ä½¿ç”¨ä¼šè¯é”®: {session_keys}")

        # æ„å»ºä¼šè¯
        self.sessions = {}
        session_id = 0

        # æŒ‰ä¼šè¯é”®åˆ†ç»„
        for session_key, group in self.df.groupby(session_keys):
            # æŒ‰æ—¶é—´æ’åº
            group = group.sort_values('Timestamp')

            # æ ¹æ®æ—¶é—´é—´éš”åˆ†å‰²ä¼šè¯
            current_session = []
            last_time = None

            for idx, row in group.iterrows():
                current_time = row['Timestamp']

                # å¦‚æœæ—¶é—´é—´éš”è¶…è¿‡é˜ˆå€¼ï¼Œå¼€å§‹æ–°ä¼šè¯
                if last_time is not None and (current_time - last_time).total_seconds() > session_timeout:
                    if len(current_session) > 1:  # åªä¿ç•™æœ‰å¤šä¸ªæµçš„ä¼šè¯
                        self.sessions[session_id] = current_session
                        session_id += 1
                    current_session = []

                current_session.append(row.to_dict())
                last_time = current_time

            # æ·»åŠ æœ€åä¸€ä¸ªä¼šè¯
            if len(current_session) > 1:
                self.sessions[session_id] = current_session
                session_id += 1

        print(f"  æ„å»ºäº† {len(self.sessions)} ä¸ªä¼šè¯")

        # åˆ†æä¼šè¯ä¸­çš„æ”»å‡»é˜¶æ®µåˆ†å¸ƒ
        self._analyze_session_attack_stages()

    def _analyze_session_attack_stages(self):
        """åˆ†æä¼šè¯ä¸­çš„æ”»å‡»é˜¶æ®µåˆ†å¸ƒ"""
        print("åˆ†æä¼šè¯ä¸­çš„æ”»å‡»é˜¶æ®µåˆ†å¸ƒ...")

        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        session_stage_stats = {
            'normal_only': 0,      # åªæœ‰æ­£å¸¸æµé‡
            'single_attack': 0,    # å•ä¸€æ”»å‡»é˜¶æ®µ
            'multi_attack': 0,     # å¤šä¸ªæ”»å‡»é˜¶æ®µ
            'complete_apt': 0      # å®Œæ•´APTæ”»å‡»é“¾
        }

        self.attack_sessions = []  # åŒ…å«æ”»å‡»çš„ä¼šè¯
        self.normal_sessions = []  # åªæœ‰æ­£å¸¸æµé‡çš„ä¼šè¯

        for session_id, flows in self.sessions.items():
            # æå–ä¼šè¯ä¸­çš„æ”»å‡»é˜¶æ®µ
            stages = [flow[target] for flow in flows]
            unique_stages = set(stages)

            # åˆ†ç±»ä¼šè¯
            if unique_stages == {0}:  # åªæœ‰æ­£å¸¸æµé‡
                session_stage_stats['normal_only'] += 1
                self.normal_sessions.append((session_id, flows))
            else:
                attack_stages = unique_stages - {0}  # å»é™¤æ­£å¸¸æµé‡
                if len(attack_stages) == 1:
                    session_stage_stats['single_attack'] += 1
                else:
                    session_stage_stats['multi_attack'] += 1
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„APTæ”»å‡»é“¾
                    if {1, 2, 3, 4}.issubset(unique_stages):
                        session_stage_stats['complete_apt'] += 1

                self.attack_sessions.append((session_id, flows, list(attack_stages)))

        print(f"  ä¼šè¯ç»Ÿè®¡:")
        for stat_name, count in session_stage_stats.items():
            print(f"    {stat_name}: {count}")

        print(f"  æ”»å‡»ä¼šè¯: {len(self.attack_sessions)}")
        print(f"  æ­£å¸¸ä¼šè¯: {len(self.normal_sessions)}")

    def _extract_attack_sequences_from_sessions(self, num_apt_sequences):
        """ä»ä¼šè¯ä¸­æå–æ”»å‡»åºåˆ—"""
        print("ä»ä¼šè¯ä¸­æå–æ”»å‡»åºåˆ—...")

        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        self.session_apt_sequences_data = []
        self.session_apt_sequences_labels = []

        import random
        random.seed(42)

        # å¦‚æœæ”»å‡»ä¼šè¯ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨
        available_sessions = self.attack_sessions.copy()

        for i in range(num_apt_sequences):
            if not available_sessions:
                available_sessions = self.attack_sessions.copy()
                random.shuffle(available_sessions)

            session_id, flows, attack_stages = available_sessions.pop()

            # æŒ‰æ—¶é—´é¡ºåºæå–æ”»å‡»åºåˆ—
            sequence_data = []
            sequence_labels = []

            for flow in flows:
                stage = flow[target]
                stage_label = self.stage_to_internal[stage]

                # åªä¿ç•™é€‰æ‹©çš„ç‰¹å¾
                if hasattr(self, 'candidate_features'):
                    flow_data = {k: v for k, v in flow.items() if k in self.candidate_features}
                else:
                    flow_data = flow

                sequence_data.append(flow_data)
                sequence_labels.append(stage_label)

            self.session_apt_sequences_data.append(sequence_data)
            self.session_apt_sequences_labels.append(sequence_labels)

        print(f"  æå–äº† {len(self.session_apt_sequences_data)} ä¸ªAPTåºåˆ—")

    def _build_normal_sequences_from_sessions(self, num_sequences):
        """ä»ä¼šè¯ä¸­æ„å»ºæ­£å¸¸åºåˆ—"""
        print("ä»ä¼šè¯ä¸­æ„å»ºæ­£å¸¸åºåˆ—...")

        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        self.session_normal_sequences_data = []
        self.session_normal_sequences_labels = []

        import random
        random.seed(42)

        # å¦‚æœæ­£å¸¸ä¼šè¯ä¸å¤Ÿï¼Œé‡å¤ä½¿ç”¨
        available_sessions = self.normal_sessions.copy()

        for i in range(num_sequences):
            if not available_sessions:
                available_sessions = self.normal_sessions.copy()
                random.shuffle(available_sessions)

            session_id, flows = available_sessions.pop()

            # æå–æ­£å¸¸åºåˆ—
            sequence_data = []
            sequence_labels = []

            for flow in flows:
                stage = flow[target]
                stage_label = self.stage_to_internal[stage]

                # åªä¿ç•™é€‰æ‹©çš„ç‰¹å¾
                if hasattr(self, 'candidate_features'):
                    flow_data = {k: v for k, v in flow.items() if k in self.candidate_features}
                else:
                    flow_data = flow

                sequence_data.append(flow_data)
                sequence_labels.append(stage_label)

            self.session_normal_sequences_data.append(sequence_data)
            self.session_normal_sequences_labels.append(sequence_labels)

        print(f"  æ„å»ºäº† {len(self.session_normal_sequences_data)} ä¸ªæ­£å¸¸åºåˆ—")

    def _convert_session_labels_to_ids(self):
        """è½¬æ¢ä¼šè¯æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—"""
        print("è½¬æ¢ä¼šè¯æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—...")

        try:
            self.session_apt_sequences_ids = [[self.attack2id[label] for label in seq] for seq in self.session_apt_sequences_labels]
            self.session_normal_sequences_ids = [[self.attack2id[label] for label in seq] for seq in self.session_normal_sequences_labels]
            print(f"  APTåºåˆ—ID: {len(self.session_apt_sequences_ids)}")
            print(f"  æ­£å¸¸åºåˆ—ID: {len(self.session_normal_sequences_ids)}")
        except KeyError as e:
            print(f"é”™è¯¯: æ ‡ç­¾ {e} ä¸åœ¨attack2idæ˜ å°„ä¸­")
            raise

    def _assign_session_sequence_labels(self):
        """åˆ†é…ä¼šè¯åºåˆ—æ ‡ç­¾"""
        print("åˆ†é…ä¼šè¯åºåˆ—æ ‡ç­¾...")

        # APTåºåˆ—æ ‡ç­¾åŸºäºæœ€é«˜æ”»å‡»é˜¶æ®µ
        stage_to_final_label = {'S1': 1, 'S2': 2, 'S3': 3, 'S4': 4, 'SN': 0}

        self.session_apt_labels = []
        for seq_labels in self.session_apt_sequences_labels:
            max_stage_num = 0
            for label in seq_labels:
                stage_num = stage_to_final_label.get(label, 0)
                max_stage_num = max(max_stage_num, stage_num)

            # æ ¹æ®æœ€é«˜æ”»å‡»é˜¶æ®µç¡®å®šAPTç±»å‹
            if max_stage_num == 1:
                apt_type = 1  # APT1: ä»…ä¾¦å¯Ÿ
            elif max_stage_num == 2:
                apt_type = 2  # APT2: ä¾¦å¯Ÿ+ç«‹è¶³ç‚¹
            elif max_stage_num == 3:
                apt_type = 3  # APT3: å‰ä¸‰é˜¶æ®µ
            elif max_stage_num == 4:
                apt_type = 4  # APT4: å®Œæ•´æ”»å‡»é“¾
            else:
                apt_type = 1  # é»˜è®¤ä¸ºAPT1

            self.session_apt_labels.append(apt_type)

        # æ­£å¸¸åºåˆ—æ ‡ç­¾éƒ½æ˜¯0 (NAPT)
        self.session_normal_labels = [0] * len(self.session_normal_sequences_ids)

        print(f"  APTæ ‡ç­¾: {len(self.session_apt_labels)}")
        print(f"  æ­£å¸¸æ ‡ç­¾: {len(self.session_normal_labels)}")

    def _save_session_sequence_results(self):
        """ä¿å­˜ä¼šè¯åºåˆ—ç»“æœ"""
        print("ä¿å­˜ä¼šè¯åºåˆ—ç»“æœ...")

        # åˆ›å»ºä¼šè¯ä¸“ç”¨è¾“å‡ºç›®å½•
        session_output_path = os.path.join(self.output_path, 'session_based')
        os.makedirs(session_output_path, exist_ok=True)

        # ä¿å­˜attack2idæ˜ å°„
        attack2id_path = os.path.join(session_output_path, 'attack2id.json')
        with open(attack2id_path, 'w') as f:
            json.dump(self.attack2id, f, indent=4)
        print(f"  Attack2IDæ˜ å°„ä¿å­˜è‡³: {attack2id_path}")

        # ä¿å­˜APTåºåˆ—æ•°æ®
        apt_data_path = os.path.join(session_output_path, 'apt_sequences_data.json')
        with open(apt_data_path, 'w') as f:
            serializable_apt_data = [
                [{k: (v.item() if hasattr(v, 'item') else v) for k, v in step.items()} for step in seq]
                for seq in self.session_apt_sequences_data
            ]
            json.dump(serializable_apt_data, f, indent=2)
        print(f"  APTåºåˆ—æ•°æ®ä¿å­˜è‡³: {apt_data_path}")

        # ä¿å­˜æ­£å¸¸åºåˆ—æ•°æ®
        normal_data_path = os.path.join(session_output_path, 'normal_sequences_data.json')
        with open(normal_data_path, 'w') as f:
            serializable_normal_data = [
                [{k: (v.item() if hasattr(v, 'item') else v) for k, v in step.items()} for step in seq]
                for seq in self.session_normal_sequences_data
            ]
            json.dump(serializable_normal_data, f, indent=2)
        print(f"  æ­£å¸¸åºåˆ—æ•°æ®ä¿å­˜è‡³: {normal_data_path}")

        # ä¿å­˜åºåˆ—æ ‡ç­¾
        apt_labels_path = os.path.join(session_output_path, 'apt_labels.npy')
        np.save(apt_labels_path, np.array(self.session_apt_labels))
        print(f"  APTæ ‡ç­¾ä¿å­˜è‡³: {apt_labels_path}")

        normal_labels_path = os.path.join(session_output_path, 'normal_labels.npy')
        np.save(normal_labels_path, np.array(self.session_normal_labels))
        print(f"  æ­£å¸¸æ ‡ç­¾ä¿å­˜è‡³: {normal_labels_path}")

        # ä¿å­˜åºåˆ—ID
        apt_ids_path = os.path.join(session_output_path, 'apt_sequences_ids.npy')
        np.save(apt_ids_path, np.array(self.session_apt_sequences_ids, dtype=object), allow_pickle=True)
        print(f"  APTåºåˆ—IDä¿å­˜è‡³: {apt_ids_path}")

        normal_ids_path = os.path.join(session_output_path, 'normal_sequences_ids.npy')
        np.save(normal_ids_path, np.array(self.session_normal_sequences_ids, dtype=object), allow_pickle=True)
        print(f"  æ­£å¸¸åºåˆ—IDä¿å­˜è‡³: {normal_ids_path}")

    def _build_attack2id_mapping(self):
        """æ„å»ºattack2idæ˜ å°„"""
        print("æ„å»ºattack2idæ˜ å°„...")

        # åŸºäºStage_encodedçš„å€¼æ„å»ºæ˜ å°„
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        unique_stages = sorted(self.df[target].unique())

        # åˆ›å»ºå†…éƒ¨æ ‡ç­¾æ˜ å°„ï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼‰
        # æ ¹æ®æ•°æ®é›†å®é™…åˆ†å¸ƒï¼š{0: 10000, 1: 5002, 2: 22968, 3: 12664, 4: 14366}
        # å…¶ä¸­1å¯¹åº”çš„æ˜¯æ•°æ®æ³„éœ²é˜¶æ®µï¼ˆæ•°é‡5002ï¼‰ï¼Œéœ€è¦é‡æ–°æ˜ å°„
        self.stage_to_internal = {
            0: 'SN',  # æ­£å¸¸æµé‡ (Normal) - 10000æ ·æœ¬
            4: 'S1',  # ä¾¦å¯Ÿé˜¶æ®µ (Reconnaissance) - 14366æ ·æœ¬
            2: 'S2',  # å»ºç«‹ç«‹è¶³ç‚¹ (Establish Foothold) - 22968æ ·æœ¬
            3: 'S3',  # æ¨ªå‘ç§»åŠ¨ (Lateral Movement) - 12664æ ·æœ¬
            1: 'S4'   # æ•°æ®æ³„éœ² (Data Exfiltration) - 5002æ ·æœ¬
        }

        # æ„å»ºattack2idæ˜ å°„
        self.attack2id = {
            'SN': 0,  # æ­£å¸¸æµé‡
            'S1': 1,  # ä¾¦å¯Ÿé˜¶æ®µ
            'S2': 2,  # å»ºç«‹ç«‹è¶³ç‚¹
            'S3': 3,  # æ¨ªå‘ç§»åŠ¨
            'S4': 4   # æ•°æ®æ³„éœ²
        }

        print(f"  Attack2IDæ˜ å°„: {self.attack2id}")

    def _partition_data_by_stage(self, target):
        """æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®"""
        print("æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®...")

        self.stage_dataframes = {}

        for stage_encoded, internal_label in self.stage_to_internal.items():
            stage_data = self.df[self.df[target] == stage_encoded]
            if not stage_data.empty:
                # åªä¿ç•™é€‰æ‹©çš„ç‰¹å¾
                if hasattr(self, 'candidate_features'):
                    stage_data = stage_data[self.candidate_features]
                self.stage_dataframes[internal_label] = stage_data
                print(f"  {internal_label}: {len(stage_data)} æ ·æœ¬")
            else:
                print(f"  {internal_label}: 0 æ ·æœ¬ (è·³è¿‡)")

    def _build_apt_sequence_labels(self, num_apt_sequences, min_normal_insert, max_normal_insert):
        """æ„å»ºAPTåºåˆ—æ ‡ç­¾"""
        print("æ„å»ºAPTåºåˆ—æ ‡ç­¾...")

        import random
        random.seed(42)

        # å¯ç”¨çš„æ”»å‡»é˜¶æ®µï¼ˆæŒ‰é¡ºåºï¼‰
        available_attack_stages = ['S1', 'S2', 'S3', 'S4']
        normal_stage = 'SN'

        # åºåˆ—ç±»å‹æ•°é‡ï¼ˆ1-4ä¸ªæ”»å‡»é˜¶æ®µï¼‰
        num_sequence_types = len(available_attack_stages)
        num_per_type = num_apt_sequences // num_sequence_types
        remainder = num_apt_sequences % num_sequence_types

        self.apt_sequences_labels = []

        for i in range(num_sequence_types):
            # å®šä¹‰å½“å‰åºåˆ—ç±»å‹çš„åŸºç¡€æ”»å‡»é˜¶æ®µ
            current_base_stages = available_attack_stages[:i+1]
            print(f"  ç”Ÿæˆç±»å‹{i+1} (æœ€å¤§é˜¶æ®µ: {current_base_stages[-1]}): {current_base_stages}")

            num_for_this_type = num_per_type + (1 if i < remainder else 0)

            for _ in range(num_for_this_type):
                # åŸºç¡€åºåˆ—
                current_seq = list(current_base_stages)

                # æ’å…¥éšæœºæ•°é‡çš„æ­£å¸¸æµé‡
                num_sn_to_insert = random.randint(min_normal_insert, max_normal_insert)
                for _ in range(num_sn_to_insert):
                    insert_pos = random.randint(0, len(current_seq))
                    current_seq.insert(insert_pos, normal_stage)

                self.apt_sequences_labels.append(current_seq)

        print(f"  æ„å»ºäº†{len(self.apt_sequences_labels)}ä¸ªAPTåºåˆ—æ ‡ç­¾")

    def _build_normal_sequence_labels(self):
        """æ„å»ºæ­£å¸¸åºåˆ—æ ‡ç­¾"""
        print("æ„å»ºæ­£å¸¸åºåˆ—æ ‡ç­¾...")

        self.normal_sequences_labels = []

        # ä¸ºæ¯ä¸ªAPTåºåˆ—ç”Ÿæˆå¯¹åº”é•¿åº¦çš„æ­£å¸¸åºåˆ—
        for apt_seq in self.apt_sequences_labels:
            normal_len = len(apt_seq)
            self.normal_sequences_labels.append(['SN'] * normal_len)

        print(f"  æ„å»ºäº†{len(self.normal_sequences_labels)}ä¸ªæ­£å¸¸åºåˆ—æ ‡ç­¾")

    def _select_samples_for_sequences(self):
        """ä¸ºåºåˆ—é€‰æ‹©å®é™…æ•°æ®æ ·æœ¬"""
        print("ä¸ºåºåˆ—é€‰æ‹©å®é™…æ•°æ®æ ·æœ¬...")

        import random
        random.seed(42)

        self.apt_sequences_data = []
        self.normal_sequences_data = []

        # ä¸ºAPTåºåˆ—é€‰æ‹©æ ·æœ¬
        for seq_labels in self.apt_sequences_labels:
            sequence_data = []
            for stage_label in seq_labels:
                if stage_label in self.stage_dataframes:
                    stage_df = self.stage_dataframes[stage_label]
                    if not stage_df.empty:
                        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
                        sample = stage_df.sample(1).iloc[0].to_dict()
                        sequence_data.append(sample)
                    else:
                        print(f"    è­¦å‘Š: {stage_label} é˜¶æ®µæ•°æ®ä¸ºç©º")
                else:
                    print(f"    è­¦å‘Š: æ‰¾ä¸åˆ° {stage_label} é˜¶æ®µæ•°æ®")

            if sequence_data:  # åªæ·»åŠ éç©ºåºåˆ—
                self.apt_sequences_data.append(sequence_data)

        # ä¸ºæ­£å¸¸åºåˆ—é€‰æ‹©æ ·æœ¬
        for seq_labels in self.normal_sequences_labels:
            sequence_data = []
            for stage_label in seq_labels:  # éƒ½æ˜¯'SN'
                if stage_label in self.stage_dataframes:
                    stage_df = self.stage_dataframes[stage_label]
                    if not stage_df.empty:
                        sample = stage_df.sample(1).iloc[0].to_dict()
                        sequence_data.append(sample)

            if sequence_data:
                self.normal_sequences_data.append(sequence_data)

        print(f"  APTåºåˆ—æ•°æ®: {len(self.apt_sequences_data)}")
        print(f"  æ­£å¸¸åºåˆ—æ•°æ®: {len(self.normal_sequences_data)}")

    def _convert_labels_to_ids(self):
        """è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—"""
        print("è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—...")

        try:
            self.apt_sequences_ids = [[self.attack2id[label] for label in seq] for seq in self.apt_sequences_labels]
            self.normal_sequences_ids = [[self.attack2id[label] for label in seq] for seq in self.normal_sequences_labels]
            print(f"  APTåºåˆ—ID: {len(self.apt_sequences_ids)}")
            print(f"  æ­£å¸¸åºåˆ—ID: {len(self.normal_sequences_ids)}")
        except KeyError as e:
            print(f"é”™è¯¯: æ ‡ç­¾ {e} ä¸åœ¨attack2idæ˜ å°„ä¸­")
            raise

    def _assign_final_sequence_labels(self):
        """åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾"""
        print("åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾...")

        # APTåºåˆ—æ ‡ç­¾åŸºäºæœ€é«˜æ”»å‡»é˜¶æ®µ
        stage_to_final_label = {'S1': 1, 'S2': 2, 'S3': 3, 'S4': 4, 'SN': 0}

        self.apt_labels = []
        for seq_labels in self.apt_sequences_labels:
            max_stage_num = 0
            for label in seq_labels:
                stage_num = stage_to_final_label.get(label, 0)
                max_stage_num = max(max_stage_num, stage_num)
            self.apt_labels.append(max_stage_num if max_stage_num > 0 else 1)

        # æ­£å¸¸åºåˆ—æ ‡ç­¾éƒ½æ˜¯0
        self.normal_labels = [0] * len(self.normal_sequences_ids)

        print(f"  APTæ ‡ç­¾: {len(self.apt_labels)}")
        print(f"  æ­£å¸¸æ ‡ç­¾: {len(self.normal_labels)}")

    def _save_sequence_results(self):
        """ä¿å­˜åºåˆ—ç»“æœ"""
        print("ä¿å­˜åºåˆ—ç»“æœ...")

        # ä¿å­˜attack2idæ˜ å°„
        attack2id_path = os.path.join(self.output_path, 'attack2id.json')
        with open(attack2id_path, 'w') as f:
            json.dump(self.attack2id, f, indent=4)
        print(f"  Attack2IDæ˜ å°„ä¿å­˜è‡³: {attack2id_path}")

        # ä¿å­˜APTåºåˆ—æ•°æ®
        apt_data_path = os.path.join(self.output_path, 'apt_sequences_data.json')
        with open(apt_data_path, 'w') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹
            serializable_apt_data = [
                [{k: (v.item() if hasattr(v, 'item') else v) for k, v in step.items()} for step in seq]
                for seq in self.apt_sequences_data
            ]
            json.dump(serializable_apt_data, f, indent=2)
        print(f"  APTåºåˆ—æ•°æ®ä¿å­˜è‡³: {apt_data_path}")

        # ä¿å­˜æ­£å¸¸åºåˆ—æ•°æ®
        normal_data_path = os.path.join(self.output_path, 'normal_sequences_data.json')
        with open(normal_data_path, 'w') as f:
            serializable_normal_data = [
                [{k: (v.item() if hasattr(v, 'item') else v) for k, v in step.items()} for step in seq]
                for seq in self.normal_sequences_data
            ]
            json.dump(serializable_normal_data, f, indent=2)
        print(f"  æ­£å¸¸åºåˆ—æ•°æ®ä¿å­˜è‡³: {normal_data_path}")

        # ä¿å­˜åºåˆ—æ ‡ç­¾
        apt_labels_path = os.path.join(self.output_path, 'apt_labels.npy')
        np.save(apt_labels_path, np.array(self.apt_labels))
        print(f"  APTæ ‡ç­¾ä¿å­˜è‡³: {apt_labels_path}")

        normal_labels_path = os.path.join(self.output_path, 'normal_labels.npy')
        np.save(normal_labels_path, np.array(self.normal_labels))
        print(f"  æ­£å¸¸æ ‡ç­¾ä¿å­˜è‡³: {normal_labels_path}")

        # ä¿å­˜åºåˆ—ID
        apt_ids_path = os.path.join(self.output_path, 'apt_sequences_ids.npy')
        np.save(apt_ids_path, np.array(self.apt_sequences_ids, dtype=object), allow_pickle=True)
        print(f"  APTåºåˆ—IDä¿å­˜è‡³: {apt_ids_path}")

        normal_ids_path = os.path.join(self.output_path, 'normal_sequences_ids.npy')
        np.save(normal_ids_path, np.array(self.normal_sequences_ids, dtype=object), allow_pickle=True)
        print(f"  æ­£å¸¸åºåˆ—IDä¿å­˜è‡³: {normal_ids_path}")




def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨")
    print("ç‰¹ç‚¹: XGBoostç‰¹å¾é€‰æ‹© + åŠ¨æ€ç‰¹å¾æ•°é‡ä¼˜åŒ– + ä¸¤ç§æ”»å‡»åºåˆ—æ„å»ºæ–¹æ³•")
    print("="*80)

    # è®¾ç½®è·¯å¾„
    input_path = r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
    output_path = "enhanced_apt_output"

    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = EnhancedAPTPreprocessor(input_path, output_path)

    # è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹
    processor.load_data().clean_data().create_statistical_features().encode_and_normalize().prepare_paper_aligned_features()
    available_features = len(processor.candidate_features)

    print(f"\nğŸ“Š ç‰¹å¾æ•°é‡åˆ†æ:")
    print(f"  å¯ç”¨ç‰¹å¾æ•°é‡: {available_features}")
    print(f"  å°†ä½¿ç”¨XGBooståˆ†æå¯»æ‰¾æœ€ä¼˜ç‰¹å¾æ•°é‡")

    # æµ‹è¯•ä¸åŒçš„ç‰¹å¾é€‰æ‹©ç­–ç•¥
    print(f"\nğŸ”¬ æµ‹è¯•ä¸åŒç‰¹å¾é€‰æ‹©ç­–ç•¥")
    print("="*60)

    # ç­–ç•¥1: åŒ…å«è®ºæ–‡ç‰¹å¾ + ç§»é™¤è¿‡æ‹Ÿåˆç‰¹å¾
    print(f"\nğŸ“‹ ç­–ç•¥1: åŒ…å«è®ºæ–‡ç‰¹å¾ + ç§»é™¤è¿‡æ‹Ÿåˆç‰¹å¾")
    processor1 = EnhancedAPTPreprocessor(input_path, output_path + "_strategy1")
    processor1.load_data().clean_data().create_statistical_features().encode_and_normalize().prepare_paper_aligned_features()
    processor1.evaluate_with_fold_internal_pipeline(n_features=None, cv_folds=5, optimize_hyperparams=True, n_trials=10)
    f1_strategy1 = processor1.cv_results['XGBoost']['f1_macro']['mean']
    print(f"ç­–ç•¥1 F1åˆ†æ•°: {f1_strategy1:.4f}")

    # ç­–ç•¥2: ä¸åŒ…å«è®ºæ–‡ç‰¹å¾ï¼Œæµ‹è¯•å…¶ä»–ç‰¹å¾
    print(f"\nğŸ“‹ ç­–ç•¥2: ä¸åŒ…å«è®ºæ–‡ç‰¹å¾ï¼Œæµ‹è¯•å…¶ä»–ç‰¹å¾")
    processor2 = EnhancedAPTPreprocessor(input_path, output_path + "_strategy2")
    processor2.load_data().clean_data().create_statistical_features().encode_and_normalize().prepare_paper_aligned_features()
    # ä¿®æ”¹è¿‡æ»¤ç­–ç•¥
    processor2.filter_candidate_features(remove_overfitting_features=True, include_paper_features=False)
    processor2.evaluate_with_fold_internal_pipeline(n_features=None, cv_folds=5, optimize_hyperparams=True, n_trials=10)
    f1_strategy2 = processor2.cv_results['XGBoost']['f1_macro']['mean']
    print(f"ç­–ç•¥2 F1åˆ†æ•°: {f1_strategy2:.4f}")

    # ç­–ç•¥3: åŒ…å«æ‰€æœ‰ç‰¹å¾ï¼ˆåŒ…æ‹¬å¯èƒ½è¿‡æ‹Ÿåˆçš„ï¼‰
    print(f"\nğŸ“‹ ç­–ç•¥3: åŒ…å«æ‰€æœ‰ç‰¹å¾ï¼ˆåŒ…æ‹¬å¯èƒ½è¿‡æ‹Ÿåˆçš„ï¼‰")
    processor3 = EnhancedAPTPreprocessor(input_path, output_path + "_strategy3")
    processor3.load_data().clean_data().create_statistical_features().encode_and_normalize().prepare_paper_aligned_features()
    # ä¸è¿‡æ»¤ä»»ä½•ç‰¹å¾
    processor3.filter_candidate_features(remove_overfitting_features=False, include_paper_features=True)
    processor3.evaluate_with_fold_internal_pipeline(n_features=None, cv_folds=5, optimize_hyperparams=True, n_trials=10)
    f1_strategy3 = processor3.cv_results['XGBoost']['f1_macro']['mean']
    print(f"ç­–ç•¥3 F1åˆ†æ•°: {f1_strategy3:.4f}")

    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š ç‰¹å¾é€‰æ‹©ç­–ç•¥å¯¹æ¯”:")
    print("="*60)
    strategies = [
        ("ç­–ç•¥1 (è®ºæ–‡ç‰¹å¾+è¿‡æ»¤)", f1_strategy1, processor1),
        ("ç­–ç•¥2 (æ— è®ºæ–‡ç‰¹å¾)", f1_strategy2, processor2),
        ("ç­–ç•¥3 (æ‰€æœ‰ç‰¹å¾)", f1_strategy3, processor3)
    ]

    best_strategy = max(strategies, key=lambda x: x[1])

    for name, f1, processor in strategies:
        status = "âœ… æœ€ä½³" if (name, f1, processor) == best_strategy else ""
        print(f"  {name}: {f1:.4f} {status}")

    print(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy[0]} (F1: {best_strategy[1]:.4f})")

    # ä¿å­˜æœ€ä½³ç»“æœ
    best_processor = best_strategy[2]
    best_processor.save_results()

    # æ¯”è¾ƒä¸¤ç§æ”»å‡»åºåˆ—æ„å»ºæ–¹æ³•
    print(f"\nğŸ”¬ æ¯”è¾ƒä¸¤ç§æ”»å‡»åºåˆ—æ„å»ºæ–¹æ³•")
    print("="*60)

    # æ–¹æ³•1: åŸºäºAttackIDçš„éšæœºæ„å»º
    print(f"\nğŸ“‹ æ–¹æ³•1: åŸºäºAttackIDçš„éšæœºæ„å»º")
    processor_method1 = EnhancedAPTPreprocessor(input_path, output_path + "_method1_attackid")
    processor_method1.load_data().clean_data().create_statistical_features().encode_and_normalize().prepare_paper_aligned_features()
    processor_method1.build_attack_sequences(num_apt_sequences=1000, min_normal_insert=1, max_normal_insert=5)

    # æ–¹æ³•2: åŸºäºä¼šè¯çš„æ„å»º
    print(f"\nğŸ“‹ æ–¹æ³•2: åŸºäºä¼šè¯çš„æ„å»º")
    processor_method2 = EnhancedAPTPreprocessor(input_path, output_path + "_method2_session")
    processor_method2.load_data().clean_data().create_statistical_features().encode_and_normalize().prepare_paper_aligned_features()
    processor_method2.build_session_based_attack_sequences(num_apt_sequences=1000, session_timeout=300)

    # åˆ†æä¸¤ç§æ–¹æ³•çš„å·®å¼‚
    print(f"\nğŸ“Š ä¸¤ç§æ–¹æ³•å¯¹æ¯”åˆ†æ:")
    print("="*60)

    # æ–¹æ³•1ç»Ÿè®¡
    print(f"\næ–¹æ³•1 (AttackIDéšæœºæ„å»º):")
    print(f"  APTåºåˆ—æ•°é‡: {len(processor_method1.apt_sequences_data)}")
    print(f"  æ­£å¸¸åºåˆ—æ•°é‡: {len(processor_method1.normal_sequences_data)}")

    # åˆ†ææ–¹æ³•1çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ
    method1_lengths = [len(seq) for seq in processor_method1.apt_sequences_data]
    print(f"  APTåºåˆ—é•¿åº¦: å¹³å‡={np.mean(method1_lengths):.1f}, æœ€å°={min(method1_lengths)}, æœ€å¤§={max(method1_lengths)}")

    # åˆ†ææ–¹æ³•1çš„APTç±»å‹åˆ†å¸ƒ
    from collections import Counter
    method1_apt_types = Counter(processor_method1.apt_labels)
    print(f"  APTç±»å‹åˆ†å¸ƒ: {dict(method1_apt_types)}")

    # æ–¹æ³•2ç»Ÿè®¡
    print(f"\næ–¹æ³•2 (ä¼šè¯æ„å»º):")
    print(f"  APTåºåˆ—æ•°é‡: {len(processor_method2.session_apt_sequences_data)}")
    print(f"  æ­£å¸¸åºåˆ—æ•°é‡: {len(processor_method2.session_normal_sequences_data)}")

    # åˆ†ææ–¹æ³•2çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ
    method2_lengths = [len(seq) for seq in processor_method2.session_apt_sequences_data]
    print(f"  APTåºåˆ—é•¿åº¦: å¹³å‡={np.mean(method2_lengths):.1f}, æœ€å°={min(method2_lengths)}, æœ€å¤§={max(method2_lengths)}")

    # åˆ†ææ–¹æ³•2çš„APTç±»å‹åˆ†å¸ƒ
    method2_apt_types = Counter(processor_method2.session_apt_labels)
    print(f"  APTç±»å‹åˆ†å¸ƒ: {dict(method2_apt_types)}")

    # æ¯”è¾ƒåˆ†æ
    print(f"\nğŸ” æ–¹æ³•å¯¹æ¯”æ€»ç»“:")
    print(f"  åºåˆ—é•¿åº¦å·®å¼‚:")
    print(f"    æ–¹æ³•1å¹³å‡é•¿åº¦: {np.mean(method1_lengths):.1f}")
    print(f"    æ–¹æ³•2å¹³å‡é•¿åº¦: {np.mean(method2_lengths):.1f}")
    print(f"    é•¿åº¦å·®å¼‚: {np.mean(method2_lengths) - np.mean(method1_lengths):+.1f}")

    print(f"\n  çœŸå®æ€§åˆ†æ:")
    print(f"    æ–¹æ³•1: éšæœºæ„å»ºï¼Œå¯èƒ½ä¸ç¬¦åˆçœŸå®æ”»å‡»æ—¶åº")
    print(f"    æ–¹æ³•2: åŸºäºçœŸå®ç½‘ç»œä¼šè¯ï¼Œä¿æŒæ—¶é—´å’Œç½‘ç»œå…³ç³»çš„è¿ç»­æ€§")

    print(f"\n  æ¨èä½¿ç”¨:")
    if np.mean(method2_lengths) > np.mean(method1_lengths):
        print(f"    âœ… æ¨èæ–¹æ³•2 (ä¼šè¯æ„å»º): åºåˆ—æ›´é•¿ï¼Œæ›´ç¬¦åˆçœŸå®æ”»å‡»åœºæ™¯")
    else:
        print(f"    âš ï¸ ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜åŠ¿ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")

    print(f"\nâœ… æ”»å‡»åºåˆ—æ„å»ºæ–¹æ³•æ¯”è¾ƒå®Œæˆ")
    print(f"  æ–¹æ³•1ç»“æœä¿å­˜åœ¨: {output_path}_method1_attackid")
    print(f"  æ–¹æ³•2ç»“æœä¿å­˜åœ¨: {output_path}_method2_session")


if __name__ == "__main__":
    main()
