"""
å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨
- ä¸°å¯Œçš„ç»Ÿè®¡é‡ç‰¹å¾
- è¯¦ç»†çš„æ—¶é—´å±æ€§
- éšæœºæ£®æ—ç‰¹å¾é€‰æ‹©
- å¤šæ¨¡å‹äº¤å‰éªŒè¯ (RF, MLP, SVM)
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import socket
import struct
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import warnings
import os
import time
import json
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import classification_report, confusion_matrix

warnings.filterwarnings('ignore')

class EnhancedAPTPreprocessor:
    def __init__(self, input_path, output_path=None):
        self.input_path =input_path or r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
        self.df = None
        if output_path:
            self.output_path = output_path
            os.makedirs(output_path, exist_ok=True)
        else:
            self.output_path = os.path.dirname(input_path)
        
        self.df = None
        self.label_encoders = {}
        self.selected_features = []
        self.feature_importance = None
        self.cv_results = {}
        self.models_performance = {}
        
    def load_data(self):
        """ä»…åŠ è½½åŸå§‹ DSRL-APT-2023 æ•°æ®é›†ï¼Œä¸åšä»»ä½•åˆ—åˆ é™¤"""
        print("ğŸ”„ åŠ è½½åŸå§‹æ•°æ®é›†...")
        start_time = time.time()

        if not os.path.exists(self.input_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {self.input_path}")

        # ç›´æ¥è¯»å– CSV
        self.df = pd.read_csv(self.input_path)
        elapsed = time.time() - start_time
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {self.df.shape}")
        print(f"ğŸ“‹ æ•°æ®åˆ—: {self.df.columns.tolist()}\n")
        return self
    
    def clean_data(self):
        """æ•°æ®æ¸…æ´—"""
        print("\nğŸ§¹ æ•°æ®æ¸…æ´—...")

        original_shape = self.df.shape

        if "Flow ID" in self.df.columns:
            self.df.drop(columns=["Flow ID"], inplace=True)
            print("ğŸ—‘ å·²åˆ é™¤åˆ—: 'Flow ID'")
        else:
            print("â„¹ï¸ æœªå‘ç° 'Flow ID' åˆ—ï¼Œè·³è¿‡åˆ é™¤")

        # 2. å¤„ç†IPåœ°å€åˆ— - è½¬æ¢ä¸º32ä½æ•´æ•°
        ip_cols = ['Src IP', 'Dst IP']
        for ip_col in ip_cols:
            if ip_col in self.df.columns:
                print(f"è½¬æ¢IPåœ°å€åˆ— {ip_col} ä¸ºæ•´æ•°")
                self.df[f'{ip_col}_int'] = self.df[ip_col].apply(self._ip_to_int)
                # åˆ é™¤åŸå§‹IPåˆ—
                self.df.drop(columns=[ip_col], inplace=True)

        # 3. å¤„ç†æ—¶é—´æˆ³åˆ—
        if 'Timestamp' in self.df.columns:
            print("å¤„ç†æ—¶é—´æˆ³åˆ—ï¼Œæå–æ—¶é—´ç‰¹å¾")
            self._extract_time_features()

        # 4. åˆ é™¤é‡å¤è¡Œ
        dup_count = self.df.duplicated().sum()
        if dup_count > 0:
            print(f"åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
            self.df.drop_duplicates(inplace=True)

        # 5. å¤„ç†ç¼ºå¤±å€¼
        null_count = self.df.isnull().sum().sum()
        if null_count > 0:
            print(f"å¤„ç† {null_count} ä¸ªç¼ºå¤±å€¼")
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)
            for col in self.df.select_dtypes(exclude=['number']).columns:
                if not self.df[col].mode().empty:
                    self.df[col].fillna(self.df[col].mode()[0], inplace=True)

        # 6. å¤„ç†æ— ç©·å€¼
        inf_count = np.isinf(self.df.select_dtypes(include=['number'])).sum().sum()
        if inf_count > 0:
            print(f"å¤„ç† {inf_count} ä¸ªæ— ç©·å€¼")
            self.df.replace([np.inf, -np.inf], np.nan, inplace=True)
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)

        print(f"âœ… æ¸…æ´—å®Œæˆ: {original_shape} â†’ {self.df.shape}")
        return self

    def _ip_to_int(self, ip):
        """å°†IPåœ°å€è½¬æ¢ä¸º32ä½æ•´æ•°"""
        try:
            return struct.unpack("!I", socket.inet_aton(str(ip)))[0]
        except:
            return 0  # å¯¹äºæ— æ•ˆIPè¿”å›0

    def _extract_time_features(self):
        """ä»æ—¶é—´æˆ³æå–æ—¶é—´ç‰¹å¾"""
        try:
            # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
            self.df['datetime'] = pd.to_datetime(self.df['Timestamp'], errors='coerce')

            # æå–æ—¶é—´ç‰¹å¾
            self.df['hour'] = self.df['datetime'].dt.hour
            self.df['minute'] = self.df['datetime'].dt.minute
            self.df['second'] = self.df['datetime'].dt.second
            self.df['day_of_week'] = self.df['datetime'].dt.dayofweek  # 0=Monday

            print("âœ… æå–æ—¶é—´ç‰¹å¾: hour, minute, second, day_of_week")

            # åˆ é™¤åŸå§‹æ—¶é—´æˆ³å’Œä¸´æ—¶datetimeåˆ—
            self.df.drop(columns=['Timestamp', 'datetime'], inplace=True, errors='ignore')

        except Exception as e:
            print(f"âš ï¸ æ—¶é—´æˆ³å¤„ç†å¤±è´¥: {e}")
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œåˆ é™¤æ—¶é—´æˆ³åˆ—
            self.df.drop(columns=['Timestamp'], inplace=True, errors='ignore')

    def create_statistical_features(self):
        """åˆ›å»ºæµé‡ç»Ÿè®¡é‡ç‰¹å¾ï¼ˆåŸºäºæ¯æ¡æµï¼‰"""
        print("\nğŸ“Š åˆ›å»ºç»Ÿè®¡é‡ç‰¹å¾...")
        orig_cols = self.df.shape[1]

        # 1. æµé‡é€Ÿç‡ç»Ÿè®¡ï¼ˆBytes/sec & Packets/secï¼‰
        if 'Flow Duration' in self.df.columns:
            # é¿å…é™¤é›¶
            duration = self.df['Flow Duration'].replace(0, 1)

            # å‰å‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Fwd Packet' in self.df.columns:
                self.df['fwd_bytes_per_sec'] = (
                        self.df['Total Length of Fwd Packet'] / duration
                )
                self.df['fwd_bytes_per_sec_log'] = np.log1p(
                    self.df['fwd_bytes_per_sec']
                )

            # åå‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Bwd Packet' in self.df.columns:
                self.df['bwd_bytes_per_sec'] = (
                        self.df['Total Length of Bwd Packet'] / duration
                )
                self.df['bwd_bytes_per_sec_log'] = np.log1p(
                    self.df['bwd_bytes_per_sec']
                )

            # å‰å‘åŒ…é€Ÿç‡
            if 'Total Fwd Packet' in self.df.columns:
                self.df['fwd_packets_per_sec'] = (
                        self.df['Total Fwd Packet'] / duration
                )

            # åå‘åŒ…é€Ÿç‡
            if 'Total Bwd Packet' in self.df.columns:
                self.df['bwd_packets_per_sec'] = (
                        self.df['Total Bwd Packet'] / duration
                )

        # 2. åŒ…é•¿åº¦æ¯”ä¾‹
        if (
                'Fwd Packet Length Mean' in self.df.columns
                and 'Bwd Packet Length Mean' in self.df.columns
        ):
            bwd_mean = self.df['Bwd Packet Length Mean'].replace(0, 1)
            self.df['fwd_bwd_length_ratio'] = (
                    self.df['Fwd Packet Length Mean'] / bwd_mean
            )

        added = self.df.shape[1] - orig_cols
        print(f"âœ… ç»Ÿè®¡é‡ç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ–°å¢ {added} ä¸ªç‰¹å¾")
        return self

    def encode_and_normalize(self):
        """ç¼–ç å’Œå½’ä¸€åŒ–"""
        print("\nğŸ”¢ ç¼–ç å’Œå½’ä¸€åŒ–...")

        # 1. å¯¹æ‰€æœ‰åˆ†ç±»ç‰¹å¾è¿›è¡ŒLabel Encoding
        categorical_cols = []

        # æ£€æµ‹åˆ†ç±»åˆ—
        for col in self.df.columns:
            if self.df[col].dtype == 'object' or col in ['Stage', 'Activity', 'Protocol']:
                if col in self.df.columns:
                    categorical_cols.append(col)

        print(f"æ£€æµ‹åˆ°åˆ†ç±»åˆ—: {categorical_cols}")

        for col in categorical_cols:
            if col in self.df.columns:
                le = LabelEncoder()
                # å¤„ç†ç¼ºå¤±å€¼
                self.df[col] = self.df[col].astype(str).fillna('Unknown')
                self.df[f'{col}_encoded'] = le.fit_transform(self.df[col])
                self.label_encoders[col] = le
                print(f"âœ… Labelç¼–ç  {col} åˆ—: {len(le.classes_)} ä¸ªç±»åˆ«")

                # æ‰“å°ç¼–ç æ˜ å°„ï¼ˆå‰5ä¸ªï¼‰
                mapping = dict(zip(le.classes_[:5], le.transform(le.classes_[:5])))
                print(f"   æ˜ å°„ç¤ºä¾‹: {mapping}{'...' if len(le.classes_) > 5 else ''}")

        # 2. å¯¹æ‰€æœ‰æ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # æ’é™¤ç¼–ç åçš„æ ‡ç­¾åˆ—ï¼ˆä¿æŒåŸå§‹æ•´æ•°å€¼ï¼‰
        exclude_cols = [col for col in numeric_cols if col.endswith('_encoded')]
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        if feature_cols:
            print(f"å¯¹ {len(feature_cols)} ä¸ªæ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–...")
            scaler = MinMaxScaler()  # ä½¿ç”¨Min-Maxå½’ä¸€åŒ–
            self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])
            self.scaler = scaler  # ä¿å­˜scalerä»¥å¤‡åç”¨
            print("âœ… Min-Maxå½’ä¸€åŒ–å®Œæˆ: X_new = (X - min) / (max - min)")

        print(f"âœ… ç¼–ç å’Œå½’ä¸€åŒ–å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.df.shape}")
        return self

    def select_features_with_rf_cv(self, n_features=46, n_estimators=100, cv_folds=10):
        """åŸºäº10æŠ˜CVçš„é²æ£’éšæœºæ£®æ—ç‰¹å¾é€‰æ‹©ï¼Œåªç”¨æ•°å€¼ç‰¹å¾"""
        print(f"\nğŸŒ² ç”¨{cv_folds}æŠ˜CVåšé²æ£’ç‰¹å¾é€‰æ‹©ï¼Œç›®æ ‡Top {n_features}")

        # ç¡®å®šç›®æ ‡å˜é‡
        if 'Stage_encoded' in self.df.columns:
            target = 'Stage_encoded'
        elif 'Label' in self.df.columns:
            target = 'Label'
        else:
            raise ValueError("æ‰¾ä¸åˆ°ç›®æ ‡å˜é‡åˆ—")

        # åªä¿ç•™æ•°å€¼åˆ—ï¼Œå¹¶æ’é™¤ç›®æ ‡
        num_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()
        feature_cols = [c for c in num_cols if c != target]

        X = self.df[feature_cols]
        y = self.df[target]

        print(f"æ•°å€¼ç‰¹å¾æ€»æ•°: {len(feature_cols)}, æ ·æœ¬æ•°: {len(X)}")

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        freq = pd.Series(0, index=feature_cols)

        print("ğŸ”„ æŠ˜å†…é€‰ç‰¹å¾ä¸­â€¦")
        for fold, (tr, _) in enumerate(skf.split(X, y), 1):
            X_tr, y_tr = X.iloc[tr], y.iloc[tr]
            selector = SelectFromModel(
                RandomForestClassifier(n_estimators=n_estimators,
                                       random_state=42,
                                       n_jobs=-1),
                max_features=n_features
            ).fit(X_tr, y_tr)
            chosen = X_tr.columns[selector.get_support()]
            freq[chosen] += 1
            print(f"  æŠ˜ {fold:2d}: é€‰ä¸­ {len(chosen)} ä¸ªç‰¹å¾")

        # æ±‡æ€»é€‰é¢‘ï¼Œå–Top n_features
        self.selected_features = freq.sort_values(ascending=False).head(n_features).index.tolist()
        print("âœ… æœ€ç»ˆTopç‰¹å¾ï¼š")
        for i, f in enumerate(self.selected_features, 1):
            print(f"  {i:2d}. {f}")

        return self

    def evaluate_multiple_models(self, cv_folds=10):
        """åœ¨å·²é€‰ç‰¹å¾ä¸Šï¼Œç”¨10æŠ˜CVå¯¹å¤šæ¨¡å‹åšMacroæŒ‡æ ‡è¯„ä¼°"""
        print(f"\nğŸ¯ ç”¨{cv_folds}æŠ˜CVè¯„ä¼°å¤šæ¨¡å‹â€¦")
        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                random_state=42),
            'SVM': SVC(kernel='rbf', C=1.0, random_state=42)
        }

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

        self.cv_results = {}
        for name, clf in models.items():
            print(f"\nğŸš€ è¯„ä¼°æ¨¡å‹: {name}")
            # MLP/SVM å‰åšæ ‡å‡†åŒ–
            if name in ('MLP', 'SVM'):
                pipe = Pipeline([
                    ('scaler', StandardScaler()),
                    ('clf', clf)
                ])
            else:
                pipe = clf

            res = cross_validate(
                pipe, X, y,
                cv=skf,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False
            )
            # æ±‡æ€»
            stats = {m: (res[f'test_{m}'].mean(), res[f'test_{m}'].std())
                     for m in scoring}
            for m, (mu, sd) in stats.items():
                print(f"  {m:<15}: {mu:.4f} Â± {sd:.4f}")
            self.cv_results[name] = stats

        return self

    def detailed_model_analysis(self, cv_folds=10):
        """åœ¨10æŠ˜CVä¸­ï¼Œæ”¶é›†æ‰€æœ‰æŠ˜çš„é¢„æµ‹ï¼Œè¾“å‡ºæ•´ä½“Classification Reportå’ŒConfusion Matrix"""
        print(f"\nğŸ” è¯¦ç»†æ¨¡å‹åˆ†æ (èšåˆ10æŠ˜ç»“æœ)â€¦")
        target = 'Stage_encoded' if 'Stage_encoded' in self.df else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        # åªé€‰è¡¨ç°æœ€å¥½çš„æ¨¡å‹
        best = max(self.cv_results,
                   key=lambda nm: self.cv_results[nm]['f1_macro'][0])
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best}")

        # é‡å»ºå¥½ç®¡é“
        if best in ('MLP', 'SVM'):
            pipe = Pipeline([
                ('scaler', StandardScaler()),
                ('clf', {
                    'MLP': self.models_performance['MLP'],
                    'SVM': self.models_performance['SVM']
                }[best])
            ])
        else:
            pipe = RandomForestClassifier(
                n_estimators=200, class_weight='balanced',
                random_state=42, n_jobs=-1)

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        y_true_all, y_pred_all = [], []

        for tr, te in skf.split(X, y):
            pipe.fit(X.iloc[tr], y.iloc[tr])
            y_pred = pipe.predict(X.iloc[te])
            y_true_all.append(y.iloc[te].values)
            y_pred_all.append(y_pred)

        y_true = np.concatenate(y_true_all)
        y_pred = np.concatenate(y_pred_all)

        print("\n--- Aggregated Classification Report ---")
        print(classification_report(y_true, y_pred))
        print("--- Aggregated Confusion Matrix ---")
        print(confusion_matrix(y_true, y_pred))
        return self

    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {self.output_path}")

        # ä¿å­˜å¢å¼ºåçš„æ•°æ®
        enhanced_data_path = os.path.join(self.output_path, 'enhanced_apt_data.csv')
        self.df.to_csv(enhanced_data_path, index=False)
        print(f"âœ… å¢å¼ºæ•°æ®ä¿å­˜è‡³: {enhanced_data_path}")

        # ä¿å­˜é€‰æ‹©çš„ç‰¹å¾æ•°æ®
        if self.selected_features:
            target_col = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
            selected_cols = self.selected_features + [target_col, 'Stage']
            selected_data_path = os.path.join(self.output_path, 'selected_features_data.csv')
            self.df[selected_cols].to_csv(selected_data_path, index=False)
            print(f"âœ… é€‰æ‹©ç‰¹å¾æ•°æ®ä¿å­˜è‡³: {selected_data_path}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            importance_path = os.path.join(self.output_path, 'feature_importance.csv')
            self.feature_importance.to_csv(importance_path, index=False)
            print(f"âœ… ç‰¹å¾é‡è¦æ€§ä¿å­˜è‡³: {importance_path}")

        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        if self.cv_results:
            cv_path = os.path.join(self.output_path, 'cross_validation_results.json')
            with open(cv_path, 'w') as f:
                json.dump(self.cv_results, f, indent=4)
            print(f"âœ… äº¤å‰éªŒè¯ç»“æœä¿å­˜è‡³: {cv_path}")

        # ä¿å­˜è¯¦ç»†æ¨¡å‹æ€§èƒ½
        if self.models_performance:
            models_path = os.path.join(self.output_path, 'models_performance.json')
            with open(models_path, 'w') as f:
                json.dump(self.models_performance, f, indent=4)
            print(f"âœ… æ¨¡å‹æ€§èƒ½ç»“æœä¿å­˜è‡³: {models_path}")

        return self

    def run_complete_pipeline(self, n_features=46, cv_folds=10):
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹")
        print("="*80)

        start_time = time.time()

        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            (self
             .load_data()
             .clean_data()
             .create_statistical_features()
             .encode_and_normalize()
             .select_features_with_rf_cv(n_features=n_features, cv_folds=cv_folds)
             .evaluate_multiple_models(cv_folds=cv_folds)
             .detailed_model_analysis()
             .save_results())

            total_time = time.time() - start_time

            print(f"\nğŸ‰ é¢„å¤„ç†å’Œè¯„ä¼°å®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ¯ é€‰æ‹©ç‰¹å¾æ•°é‡: {len(self.selected_features)}")

            # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
            if self.cv_results:
                print(f"\nğŸ“ˆ æœ€ç»ˆæ¨¡å‹æ€§èƒ½æ‘˜è¦ ({cv_folds}æŠ˜äº¤å‰éªŒè¯):")
                for model_name, metrics in self.cv_results.items():
                    print(f"  {model_name}: F1={metrics['f1_mean']:.4f}Â±{metrics['f1_std']:.3f}")

            # æ‰“å°ç‰¹å¾é€‰æ‹©æ‘˜è¦
            if hasattr(self, 'feature_importance'):
                print(f"\nğŸ¯ ç‰¹å¾é€‰æ‹©æ‘˜è¦:")
                high_freq = (self.feature_importance['selection_frequency'] >= 0.8).sum()
                print(f"  é«˜ç¨³å®šæ€§ç‰¹å¾ (â‰¥80%): {high_freq}/{n_features}")
                print(f"  å¹³å‡é‡è¦æ€§: {self.feature_importance.head(n_features)['importance_mean'].mean():.6f}")

        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        return self


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨")
    print("ç‰¹ç‚¹: ä¸°å¯Œç»Ÿè®¡é‡ + æ—¶é—´ç‰¹å¾ + éšæœºæ£®æ—ç‰¹å¾é€‰æ‹© + å¤šæ¨¡å‹è¯„ä¼°")
    print("="*80)

    # è®¾ç½®è·¯å¾„
    input_path = r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
    output_path = "enhanced_apt_output"

    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = EnhancedAPTPreprocessor(input_path, output_path)
    processor.run_complete_pipeline(n_features=46, cv_folds=10)


if __name__ == "__main__":
    main()
