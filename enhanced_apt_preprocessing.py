"""
å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨
- ä¸°å¯Œçš„ç»Ÿè®¡é‡ç‰¹å¾
- è¯¦ç»†çš„æ—¶é—´å±æ€§
- éšæœºæ£®æ—ç‰¹å¾é€‰æ‹©
- å¤šæ¨¡å‹äº¤å‰éªŒè¯ (RF, MLP, SVM)
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import socket
import struct
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import warnings
import os
import time
import json
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import classification_report, confusion_matrix

warnings.filterwarnings('ignore')

class EnhancedAPTPreprocessor:
    def __init__(self, input_path, output_path=None):
        self.input_path =input_path or r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
        self.df = None
        if output_path:
            self.output_path = output_path
            os.makedirs(output_path, exist_ok=True)
        else:
            self.output_path = os.path.dirname(input_path)
        
        self.df = None
        self.label_encoders = {}
        self.selected_features = []
        self.feature_importance = None
        self.cv_results = {}
        self.models_performance = {}
        
    def load_data(self):
        """ä»…åŠ è½½åŸå§‹ DSRL-APT-2023 æ•°æ®é›†ï¼Œä¸åšä»»ä½•åˆ—åˆ é™¤"""
        print("ğŸ”„ åŠ è½½åŸå§‹æ•°æ®é›†...")
        start_time = time.time()

        if not os.path.exists(self.input_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {self.input_path}")

        # ç›´æ¥è¯»å– CSV
        self.df = pd.read_csv(self.input_path)
        elapsed = time.time() - start_time
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {self.df.shape}")
        print(f"ğŸ“‹ æ•°æ®åˆ—: {self.df.columns.tolist()}\n")
        return self
    
    def clean_data(self):
        """æ•°æ®æ¸…æ´—"""
        print("\nğŸ§¹ æ•°æ®æ¸…æ´—...")

        original_shape = self.df.shape

        if "Flow ID" in self.df.columns:
            self.df.drop(columns=["Flow ID"], inplace=True)
            print("ğŸ—‘ å·²åˆ é™¤åˆ—: 'Flow ID'")
        else:
            print("â„¹ï¸ æœªå‘ç° 'Flow ID' åˆ—ï¼Œè·³è¿‡åˆ é™¤")

        # 2. å¤„ç†IPåœ°å€åˆ— - è½¬æ¢ä¸º32ä½æ•´æ•°
        ip_cols = ['Src IP', 'Dst IP']
        for ip_col in ip_cols:
            if ip_col in self.df.columns:
                print(f"è½¬æ¢IPåœ°å€åˆ— {ip_col} ä¸ºæ•´æ•°")
                self.df[f'{ip_col}_int'] = self.df[ip_col].apply(self._ip_to_int)
                # åˆ é™¤åŸå§‹IPåˆ—
                self.df.drop(columns=[ip_col], inplace=True)

        # 3. å¤„ç†æ—¶é—´æˆ³åˆ—
        if 'Timestamp' in self.df.columns:
            print("å¤„ç†æ—¶é—´æˆ³åˆ—ï¼Œæå–æ—¶é—´ç‰¹å¾")
            self._extract_time_features()

        # 4. åˆ é™¤é‡å¤è¡Œ
        dup_count = self.df.duplicated().sum()
        if dup_count > 0:
            print(f"åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
            self.df.drop_duplicates(inplace=True)

        # 5. å¤„ç†ç¼ºå¤±å€¼
        null_count = self.df.isnull().sum().sum()
        if null_count > 0:
            print(f"å¤„ç† {null_count} ä¸ªç¼ºå¤±å€¼")
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)
            for col in self.df.select_dtypes(exclude=['number']).columns:
                if not self.df[col].mode().empty:
                    self.df[col].fillna(self.df[col].mode()[0], inplace=True)

        # 6. å¤„ç†æ— ç©·å€¼
        inf_count = np.isinf(self.df.select_dtypes(include=['number'])).sum().sum()
        if inf_count > 0:
            print(f"å¤„ç† {inf_count} ä¸ªæ— ç©·å€¼")
            self.df.replace([np.inf, -np.inf], np.nan, inplace=True)
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)

        print(f"âœ… æ¸…æ´—å®Œæˆ: {original_shape} â†’ {self.df.shape}")
        return self

    def _ip_to_int(self, ip):
        """å°†IPåœ°å€è½¬æ¢ä¸º32ä½æ•´æ•°"""
        try:
            return struct.unpack("!I", socket.inet_aton(str(ip)))[0]
        except:
            return 0  # å¯¹äºæ— æ•ˆIPè¿”å›0

    def _extract_time_features(self):
        """ä»æ—¶é—´æˆ³æå–æ—¶é—´ç‰¹å¾"""
        try:
            # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
            self.df['datetime'] = pd.to_datetime(self.df['Timestamp'], errors='coerce')

            # æå–æ—¶é—´ç‰¹å¾
            self.df['hour'] = self.df['datetime'].dt.hour
            self.df['minute'] = self.df['datetime'].dt.minute
            self.df['second'] = self.df['datetime'].dt.second
            self.df['day_of_week'] = self.df['datetime'].dt.dayofweek  # 0=Monday

            print("âœ… æå–æ—¶é—´ç‰¹å¾: hour, minute, second, day_of_week")

            # åˆ é™¤åŸå§‹æ—¶é—´æˆ³å’Œä¸´æ—¶datetimeåˆ—
            self.df.drop(columns=['Timestamp', 'datetime'], inplace=True, errors='ignore')

        except Exception as e:
            print(f"âš ï¸ æ—¶é—´æˆ³å¤„ç†å¤±è´¥: {e}")
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œåˆ é™¤æ—¶é—´æˆ³åˆ—
            self.df.drop(columns=['Timestamp'], inplace=True, errors='ignore')

    def create_statistical_features(self):
        """åˆ›å»ºæµé‡ç»Ÿè®¡é‡ç‰¹å¾ï¼ˆåŸºäºæ¯æ¡æµï¼‰"""
        print("\nğŸ“Š åˆ›å»ºç»Ÿè®¡é‡ç‰¹å¾...")
        orig_cols = self.df.shape[1]

        # 1. æµé‡é€Ÿç‡ç»Ÿè®¡ï¼ˆBytes/sec & Packets/secï¼‰
        if 'Flow Duration' in self.df.columns:
            # é¿å…é™¤é›¶
            duration = self.df['Flow Duration'].replace(0, 1)

            # å‰å‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Fwd Packet' in self.df.columns:
                self.df['fwd_bytes_per_sec'] = (
                        self.df['Total Length of Fwd Packet'] / duration
                )
                self.df['fwd_bytes_per_sec_log'] = np.log1p(
                    self.df['fwd_bytes_per_sec']
                )

            # åå‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Bwd Packet' in self.df.columns:
                self.df['bwd_bytes_per_sec'] = (
                        self.df['Total Length of Bwd Packet'] / duration
                )
                self.df['bwd_bytes_per_sec_log'] = np.log1p(
                    self.df['bwd_bytes_per_sec']
                )

            # å‰å‘åŒ…é€Ÿç‡
            if 'Total Fwd Packet' in self.df.columns:
                self.df['fwd_packets_per_sec'] = (
                        self.df['Total Fwd Packet'] / duration
                )

            # åå‘åŒ…é€Ÿç‡
            if 'Total Bwd Packet' in self.df.columns:
                self.df['bwd_packets_per_sec'] = (
                        self.df['Total Bwd Packet'] / duration
                )

        # 2. åŒ…é•¿åº¦æ¯”ä¾‹
        if (
                'Fwd Packet Length Mean' in self.df.columns
                and 'Bwd Packet Length Mean' in self.df.columns
        ):
            bwd_mean = self.df['Bwd Packet Length Mean'].replace(0, 1)
            self.df['fwd_bwd_length_ratio'] = (
                    self.df['Fwd Packet Length Mean'] / bwd_mean
            )

        added = self.df.shape[1] - orig_cols
        print(f"âœ… ç»Ÿè®¡é‡ç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ–°å¢ {added} ä¸ªç‰¹å¾")
        return self

    def encode_and_normalize(self):
        """ç¼–ç å’Œå½’ä¸€åŒ–"""
        print("\nğŸ”¢ ç¼–ç å’Œå½’ä¸€åŒ–...")

        # 1. å¯¹æ‰€æœ‰åˆ†ç±»ç‰¹å¾è¿›è¡ŒLabel Encoding
        categorical_cols = []

        # æ£€æµ‹åˆ†ç±»åˆ—
        for col in self.df.columns:
            if self.df[col].dtype == 'object' or col in ['Stage', 'Activity', 'Protocol']:
                if col in self.df.columns:
                    categorical_cols.append(col)

        print(f"æ£€æµ‹åˆ°åˆ†ç±»åˆ—: {categorical_cols}")

        for col in categorical_cols:
            if col in self.df.columns:
                le = LabelEncoder()
                # å¤„ç†ç¼ºå¤±å€¼
                self.df[col] = self.df[col].astype(str).fillna('Unknown')
                self.df[f'{col}_encoded'] = le.fit_transform(self.df[col])
                self.label_encoders[col] = le
                print(f"âœ… Labelç¼–ç  {col} åˆ—: {len(le.classes_)} ä¸ªç±»åˆ«")

                # æ‰“å°ç¼–ç æ˜ å°„ï¼ˆå‰5ä¸ªï¼‰
                mapping = dict(zip(le.classes_[:5], le.transform(le.classes_[:5])))
                print(f"   æ˜ å°„ç¤ºä¾‹: {mapping}{'...' if len(le.classes_) > 5 else ''}")

        # 2. å¯¹æ‰€æœ‰æ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # æ’é™¤ç¼–ç åçš„æ ‡ç­¾åˆ—ï¼ˆä¿æŒåŸå§‹æ•´æ•°å€¼ï¼‰
        exclude_cols = [col for col in numeric_cols if col.endswith('_encoded')]
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        if feature_cols:
            print(f"å¯¹ {len(feature_cols)} ä¸ªæ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–...")
            scaler = MinMaxScaler()  # ä½¿ç”¨Min-Maxå½’ä¸€åŒ–
            self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])
            self.scaler = scaler  # ä¿å­˜scalerä»¥å¤‡åç”¨
            print("âœ… Min-Maxå½’ä¸€åŒ–å®Œæˆ: X_new = (X - min) / (max - min)")

        print(f"âœ… ç¼–ç å’Œå½’ä¸€åŒ–å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.df.shape}")
        return self

    def prepare_paper_aligned_features(self):
        """æŒ‰è®ºæ–‡æ–¹æ³•å‡†å¤‡ç‰¹å¾ï¼šåªç”¨ç½‘ç»œæµç‰¹å¾+æ—¶é—´ç‰¹å¾ï¼Œæ’é™¤Activityç­‰æ ‡ç­¾ä¿¡æ¯"""
        print(f"\nğŸ¯ æŒ‰è®ºæ–‡æ–¹æ³•å‡†å¤‡ç‰¹å¾ï¼ˆæ’é™¤Activityç­‰æ ‡ç­¾ä¿¡æ¯ï¼‰")

        # ç¡®å®šç›®æ ‡å˜é‡
        if 'Stage_encoded' in self.df.columns:
            target = 'Stage_encoded'
        elif 'Label' in self.df.columns:
            target = 'Label'
        else:
            raise ValueError("æ‰¾ä¸åˆ°ç›®æ ‡å˜é‡åˆ—")

        # è·å–æ‰€æœ‰æ•°å€¼ç‰¹å¾
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # æ’é™¤ç›®æ ‡å˜é‡å’ŒActivityç›¸å…³çš„ç¼–ç åˆ—ï¼ˆé¿å…æ ‡ç­¾æ³„éœ²ï¼‰
        exclude_cols = [target, 'Activity_encoded', 'Stage_encoded']
        if target == 'Stage_encoded':
            exclude_cols.remove('Stage_encoded')  # å¦‚æœStage_encodedæ˜¯ç›®æ ‡ï¼Œå°±ä¸æ’é™¤å®ƒ

        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        # ç¡®ä¿æ—¶é—´ç‰¹å¾è¢«åŒ…å«ï¼ˆè®ºæ–‡Table 3ä¸­çš„å…³é”®ç‰¹å¾ï¼‰
        time_features = ['hour', 'minute', 'day_of_week']  # ç§»é™¤secondï¼ŒæŒ‰è®ºæ–‡åªç”¨è¿™3ä¸ª
        for tf in time_features:
            if tf in self.df.columns and tf not in feature_cols:
                feature_cols.append(tf)

        # ç¡®ä¿Protocolç¼–ç è¢«åŒ…å«ï¼ˆç½‘ç»œç‰¹å¾çš„ä¸€éƒ¨åˆ†ï¼‰
        if 'Protocol_encoded' in self.df.columns and 'Protocol_encoded' not in feature_cols:
            feature_cols.append('Protocol_encoded')

        print(f"ç½‘ç»œæµç‰¹å¾æ•°é‡: {len([f for f in feature_cols if f not in time_features + ['Protocol_encoded']])}")
        print(f"æ—¶é—´ç‰¹å¾æ•°é‡: {len([f for f in feature_cols if f in time_features])}")
        print(f"åè®®ç‰¹å¾æ•°é‡: {len([f for f in feature_cols if f == 'Protocol_encoded'])}")
        print(f"å€™é€‰ç‰¹å¾æ€»æ•°: {len(feature_cols)}")

        # æ£€æŸ¥æ˜¯å¦æ„å¤–åŒ…å«äº†Activity
        activity_features = [f for f in feature_cols if 'activity' in f.lower()]
        if activity_features:
            print(f"âš ï¸ è­¦å‘Šï¼šå‘ç°Activityç›¸å…³ç‰¹å¾ï¼Œå°†è¢«ç§»é™¤: {activity_features}")
            feature_cols = [f for f in feature_cols if f not in activity_features]

        self.candidate_features = feature_cols
        return self

    def evaluate_with_fold_internal_pipeline(self, n_features=46, cv_folds=10):
        """æŒ‰è®ºæ–‡æ–¹æ³•ï¼šæ¯æŠ˜å†…éƒ¨åšç‰¹å¾é€‰æ‹©+æ ‡å‡†åŒ–+åˆ†ç±»çš„å®Œæ•´pipeline"""
        print(f"\nğŸ¯ è®ºæ–‡æ–¹æ³•ï¼šæ¯æŠ˜å†…Pipeline(ç‰¹å¾é€‰æ‹©â†’æ ‡å‡†åŒ–â†’åˆ†ç±»)")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.candidate_features]
        y = self.df[target]

        print(f"å€™é€‰ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")

        # æŒ‰è®ºæ–‡é…ç½®æ¨¡å‹
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=1e-3,
                random_state=42
            ),
            'SVM': SVC(
                kernel='rbf',
                C=1.0,
                random_state=42
            )
        }

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

        self.cv_results = {}

        for name, clf in models.items():
            print(f"\nğŸš€ è¯„ä¼°æ¨¡å‹: {name}")

            # æ„å»ºæŠ˜å†…Pipelineï¼šç‰¹å¾é€‰æ‹© â†’ æ ‡å‡†åŒ– â†’ åˆ†ç±»å™¨
            if name == 'RandomForest':
                # RFä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†éœ€è¦ç‰¹å¾é€‰æ‹©
                pipeline = Pipeline([
                    ('feat_sel', SelectFromModel(
                        RandomForestClassifier(n_estimators=100, random_state=42),
                        max_features=n_features
                    )),
                    ('clf', clf)
                ])
            else:
                # MLPå’ŒSVMéœ€è¦ç‰¹å¾é€‰æ‹©+æ ‡å‡†åŒ–
                pipeline = Pipeline([
                    ('feat_sel', SelectFromModel(
                        RandomForestClassifier(n_estimators=100, random_state=42),
                        max_features=n_features
                    )),
                    ('scaler', StandardScaler()),
                    ('clf', clf)
                ])

            # äº¤å‰éªŒè¯è¯„ä¼°
            cv_results = cross_validate(
                pipeline, X, y,
                cv=skf,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False
            )

            # è®¡ç®—ç»Ÿè®¡é‡
            results = {}
            for metric in scoring:
                scores = cv_results[f'test_{metric}']
                results[metric] = {
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'scores': scores.tolist()
                }
                print(f"  {metric:<15}: {scores.mean():.4f} Â± {scores.std():.4f}")

            self.cv_results[name] = results

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        best_f1 = self.cv_results[best_model]['f1_macro']['mean']
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (F1: {best_f1:.4f})")

        return self

    def evaluate_multiple_models(self, cv_folds=10):
        """æŒ‰è®ºæ–‡æ–¹æ³•ï¼šç”¨å›ºå®šçš„46ä¸ªç‰¹å¾ï¼Œåœ¨10æŠ˜CVä¸­è¯„ä¼°å¤šæ¨¡å‹"""
        print(f"\nğŸ¯ ä½¿ç”¨å›ºå®šçš„{len(self.selected_features)}ä¸ªç‰¹å¾è¿›è¡Œ{cv_folds}æŠ˜äº¤å‰éªŒè¯")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")

        # æŒ‰è®ºæ–‡é…ç½®æ¨¡å‹è¶…å‚æ•°
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=0.01,
                random_state=42
            ),
            'SVM': SVC(
                kernel='rbf',
                C=1.0,
                random_state=42
            )
        }

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

        self.cv_results = {}

        for name, clf in models.items():
            print(f"\nğŸš€ è¯„ä¼°æ¨¡å‹: {name}")

            # å¯¹MLPå’ŒSVMä½¿ç”¨Pipelineè¿›è¡ŒæŠ˜å†…æ ‡å‡†åŒ–
            if name in ('MLP', 'SVM'):
                estimator = Pipeline([
                    ('scaler', StandardScaler()),
                    ('clf', clf)
                ])
            else:
                estimator = clf

            # ä½¿ç”¨cross_validateè¿›è¡Œè¯„ä¼°
            from sklearn.model_selection import cross_validate
            cv_results = cross_validate(
                estimator, X, y,
                cv=skf,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False
            )

            # è®¡ç®—ç»Ÿè®¡é‡
            results = {}
            for metric in scoring:
                scores = cv_results[f'test_{metric}']
                results[metric] = {
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'scores': scores.tolist()
                }
                print(f"  {metric:<15}: {scores.mean():.4f} Â± {scores.std():.4f}")

            self.cv_results[name] = results

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        best_f1 = self.cv_results[best_model]['f1_macro']['mean']
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (F1: {best_f1:.4f})")

        return self

    def detailed_model_analysis(self, cv_folds=10):
        """è¯¦ç»†åˆ†ææœ€ä½³æ¨¡å‹ï¼šèšåˆ10æŠ˜CVçš„é¢„æµ‹ç»“æœ"""
        print(f"\nğŸ” è¯¦ç»†æ¨¡å‹åˆ†æ (èšåˆ{cv_folds}æŠ˜ç»“æœ)")

        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        print(f"ğŸ† åˆ†ææœ€ä½³æ¨¡å‹: {best_model}")

        # é‡å»ºæœ€ä½³æ¨¡å‹
        if best_model == 'RandomForest':
            clf = RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            )
            estimator = clf
        elif best_model == 'MLP':
            clf = MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=1e-3,
                random_state=42
            )
            estimator = Pipeline([('scaler', StandardScaler()), ('clf', clf)])
        else:  # SVM
            clf = SVC(kernel='rbf', C=1.0, random_state=42)
            estimator = Pipeline([('scaler', StandardScaler()), ('clf', clf)])

        # 10æŠ˜äº¤å‰éªŒè¯ï¼Œæ”¶é›†æ‰€æœ‰é¢„æµ‹
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        y_true_all, y_pred_all = [], []

        print("ğŸ”„ æ‰§è¡Œ10æŠ˜äº¤å‰éªŒè¯...")
        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # è®­ç»ƒå’Œé¢„æµ‹
            estimator.fit(X_train, y_train)
            y_pred = estimator.predict(X_test)

            y_true_all.extend(y_test.values)
            y_pred_all.extend(y_pred)

            print(f"  æŠ˜ {fold:2d} å®Œæˆ")

        # èšåˆç»“æœ
        y_true = np.array(y_true_all)
        y_pred = np.array(y_pred_all)

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='macro')
        recall = recall_score(y_true, y_pred, average='macro')
        f1 = f1_score(y_true, y_pred, average='macro')

        print(f"\nğŸ“Š èšåˆç»“æœ ({best_model}):")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"  å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"  F1åˆ†æ•° (F1-Score): {f1:.4f}")

        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred))

        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print(confusion_matrix(y_true, y_pred))

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.models_performance = {
            'best_model': best_model,
            'aggregated_metrics': {
                'accuracy': accuracy,
                'precision_macro': precision,
                'recall_macro': recall,
                'f1_macro': f1
            },
            'classification_report': classification_report(y_true, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
        }

        return self

    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {self.output_path}")

        # ä¿å­˜å¢å¼ºåçš„æ•°æ®
        enhanced_data_path = os.path.join(self.output_path, 'enhanced_apt_data.csv')
        self.df.to_csv(enhanced_data_path, index=False)
        print(f"âœ… å¢å¼ºæ•°æ®ä¿å­˜è‡³: {enhanced_data_path}")

        # ä¿å­˜é€‰æ‹©çš„ç‰¹å¾æ•°æ®
        if self.selected_features:
            target_col = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
            selected_cols = self.selected_features + [target_col, 'Stage']
            selected_data_path = os.path.join(self.output_path, 'selected_features_data.csv')
            self.df[selected_cols].to_csv(selected_data_path, index=False)
            print(f"âœ… é€‰æ‹©ç‰¹å¾æ•°æ®ä¿å­˜è‡³: {selected_data_path}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            importance_path = os.path.join(self.output_path, 'feature_importance.csv')
            self.feature_importance.to_csv(importance_path, index=False)
            print(f"âœ… ç‰¹å¾é‡è¦æ€§ä¿å­˜è‡³: {importance_path}")

        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        if self.cv_results:
            cv_path = os.path.join(self.output_path, 'cross_validation_results.json')
            with open(cv_path, 'w') as f:
                json.dump(self.cv_results, f, indent=4)
            print(f"âœ… äº¤å‰éªŒè¯ç»“æœä¿å­˜è‡³: {cv_path}")

        # ä¿å­˜è¯¦ç»†æ¨¡å‹æ€§èƒ½
        if self.models_performance:
            models_path = os.path.join(self.output_path, 'models_performance.json')
            with open(models_path, 'w') as f:
                json.dump(self.models_performance, f, indent=4)
            print(f"âœ… æ¨¡å‹æ€§èƒ½ç»“æœä¿å­˜è‡³: {models_path}")

        return self

    def run_complete_pipeline(self, n_features=46, cv_folds=10):
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹")
        print("="*80)

        start_time = time.time()

        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹ï¼ˆæŒ‰è®ºæ–‡æ–¹æ³•ä¿®æ­£ï¼‰
            (self
             .load_data()
             .clean_data()
             .create_statistical_features()
             .encode_and_normalize()
             .prepare_paper_aligned_features()
             .evaluate_with_fold_internal_pipeline(n_features=n_features, cv_folds=cv_folds)
             .save_results())

            total_time = time.time() - start_time

            print(f"\nğŸ‰ é¢„å¤„ç†å’Œè¯„ä¼°å®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ¯ é€‰æ‹©ç‰¹å¾æ•°é‡: {len(self.selected_features)}")

            # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
            if self.cv_results:
                print(f"\nğŸ“ˆ è®ºæ–‡æ–¹æ³•æ¨¡å‹æ€§èƒ½æ‘˜è¦ ({cv_folds}æŠ˜äº¤å‰éªŒè¯):")
                for model_name, metrics in self.cv_results.items():
                    f1_mean = metrics['f1_macro']['mean']
                    f1_std = metrics['f1_macro']['std']
                    print(f"  {model_name}: F1={f1_mean:.4f}Â±{f1_std:.3f}")

            # æ‰“å°ç‰¹å¾é€‰æ‹©æ‘˜è¦
            if hasattr(self, 'candidate_features'):
                print(f"\nğŸ¯ è®ºæ–‡å¯¹é½ç‰¹å¾æ‘˜è¦:")
                print(f"  å€™é€‰ç‰¹å¾æ•°é‡: {len(self.candidate_features)}")

                # æ£€æŸ¥æ—¶é—´ç‰¹å¾
                time_features = ['hour', 'minute', 'day_of_week']
                time_in_candidates = [f for f in self.candidate_features if f in time_features]
                print(f"  åŒ…å«æ—¶é—´ç‰¹å¾: {time_in_candidates}")

                # æ£€æŸ¥æ˜¯å¦æ’é™¤äº†Activity
                activity_excluded = not any('activity' in f.lower() for f in self.candidate_features)
                print(f"  å·²æ’é™¤Activityæ ‡ç­¾: {'âœ…' if activity_excluded else 'âŒ'}")

                # æ˜¾ç¤ºç‰¹å¾ç±»å‹åˆ†å¸ƒ
                network_features = len([f for f in self.candidate_features if f not in time_features + ['Protocol_encoded']])
                print(f"  ç½‘ç»œæµç‰¹å¾: {network_features}")
                print(f"  æ—¶é—´ç‰¹å¾: {len(time_in_candidates)}")
                print(f"  åè®®ç‰¹å¾: {1 if 'Protocol_encoded' in self.candidate_features else 0}")

            # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
            if self.cv_results:
                best_f1 = max(metrics['f1_macro']['mean'] for metrics in self.cv_results.values())
                print(f"\nğŸ¯ ä¸è®ºæ–‡å¯¹æ¯”:")
                print(f"  æˆ‘ä»¬çš„æœ€ä½³F1: {best_f1:.4f}")
                print(f"  è®ºæ–‡æŠ¥å‘ŠF1: ~0.9800")
                print(f"  å·®è·: {0.98 - best_f1:.4f}")
                if best_f1 >= 0.97:
                    print(f"  âœ… æ¥è¿‘è®ºæ–‡æ°´å¹³ï¼")
                elif best_f1 >= 0.95:
                    print(f"  ğŸ”¶ è‰¯å¥½æ°´å¹³ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    print(f"  âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")

        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        return self


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨")
    print("ç‰¹ç‚¹: ä¸°å¯Œç»Ÿè®¡é‡ + æ—¶é—´ç‰¹å¾ + éšæœºæ£®æ—ç‰¹å¾é€‰æ‹© + å¤šæ¨¡å‹è¯„ä¼°")
    print("="*80)

    # è®¾ç½®è·¯å¾„
    input_path = r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
    output_path = "enhanced_apt_output"

    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = EnhancedAPTPreprocessor(input_path, output_path)
    processor.run_complete_pipeline(n_features=46, cv_folds=10)


if __name__ == "__main__":
    main()
