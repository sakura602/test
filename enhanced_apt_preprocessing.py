"""
å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨
- ä¸°å¯Œçš„ç»Ÿè®¡é‡ç‰¹å¾
- è¯¦ç»†çš„æ—¶é—´å±æ€§
- éšæœºæ£®æ—ç‰¹å¾é€‰æ‹©
- å¤šæ¨¡å‹äº¤å‰éªŒè¯ (RF, MLP, SVM)
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import socket
import struct
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import warnings
import os
import time
import json
from sklearn.feature_selection import SelectFromModel
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import classification_report, confusion_matrix

warnings.filterwarnings('ignore')

class EnhancedAPTPreprocessor:
    def __init__(self, input_path, output_path=None):
        self.input_path =input_path or r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
        self.df = None
        if output_path:
            self.output_path = output_path
            os.makedirs(output_path, exist_ok=True)
        else:
            self.output_path = os.path.dirname(input_path)
        
        self.df = None
        self.label_encoders = {}
        self.selected_features = []
        self.feature_importance = None
        self.cv_results = {}
        self.models_performance = {}
        
    def load_data(self):
        """ä»…åŠ è½½åŸå§‹ DSRL-APT-2023 æ•°æ®é›†ï¼Œä¸åšä»»ä½•åˆ—åˆ é™¤"""
        print("ğŸ”„ åŠ è½½åŸå§‹æ•°æ®é›†...")
        start_time = time.time()

        if not os.path.exists(self.input_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {self.input_path}")

        # ç›´æ¥è¯»å– CSV
        self.df = pd.read_csv(self.input_path)
        elapsed = time.time() - start_time
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {self.df.shape}")
        print(f"ğŸ“‹ æ•°æ®åˆ—: {self.df.columns.tolist()}\n")
        return self
    
    def clean_data(self):
        """æ•°æ®æ¸…æ´—"""
        print("\nğŸ§¹ æ•°æ®æ¸…æ´—...")

        original_shape = self.df.shape

        if "Flow ID" in self.df.columns:
            self.df.drop(columns=["Flow ID"], inplace=True)
            print("ğŸ—‘ å·²åˆ é™¤åˆ—: 'Flow ID'")
        else:
            print("â„¹ï¸ æœªå‘ç° 'Flow ID' åˆ—ï¼Œè·³è¿‡åˆ é™¤")

        # 2. å¤„ç†IPåœ°å€åˆ— - è½¬æ¢ä¸º32ä½æ•´æ•°
        ip_cols = ['Src IP', 'Dst IP']
        for ip_col in ip_cols:
            if ip_col in self.df.columns:
                print(f"è½¬æ¢IPåœ°å€åˆ— {ip_col} ä¸ºæ•´æ•°")
                self.df[f'{ip_col}_int'] = self.df[ip_col].apply(self._ip_to_int)
                # åˆ é™¤åŸå§‹IPåˆ—
                self.df.drop(columns=[ip_col], inplace=True)

        # 3. å¤„ç†æ—¶é—´æˆ³åˆ—
        if 'Timestamp' in self.df.columns:
            print("å¤„ç†æ—¶é—´æˆ³åˆ—ï¼Œæå–æ—¶é—´ç‰¹å¾")
            self._extract_time_features()

        # 4. åˆ é™¤é‡å¤è¡Œ
        dup_count = self.df.duplicated().sum()
        if dup_count > 0:
            print(f"åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
            self.df.drop_duplicates(inplace=True)

        # 5. å¤„ç†ç¼ºå¤±å€¼
        null_count = self.df.isnull().sum().sum()
        if null_count > 0:
            print(f"å¤„ç† {null_count} ä¸ªç¼ºå¤±å€¼")
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)
            for col in self.df.select_dtypes(exclude=['number']).columns:
                if not self.df[col].mode().empty:
                    self.df[col].fillna(self.df[col].mode()[0], inplace=True)

        # 6. å¤„ç†æ— ç©·å€¼
        inf_count = np.isinf(self.df.select_dtypes(include=['number'])).sum().sum()
        if inf_count > 0:
            print(f"å¤„ç† {inf_count} ä¸ªæ— ç©·å€¼")
            self.df.replace([np.inf, -np.inf], np.nan, inplace=True)
            for col in self.df.select_dtypes(include=['number']).columns:
                self.df[col].fillna(self.df[col].median(), inplace=True)

        print(f"âœ… æ¸…æ´—å®Œæˆ: {original_shape} â†’ {self.df.shape}")
        return self

    def _ip_to_int(self, ip):
        """å°†IPåœ°å€è½¬æ¢ä¸º32ä½æ•´æ•°"""
        try:
            return struct.unpack("!I", socket.inet_aton(str(ip)))[0]
        except:
            return 0  # å¯¹äºæ— æ•ˆIPè¿”å›0

    def _extract_time_features(self):
        """ä»æ—¶é—´æˆ³æå–æ—¶é—´ç‰¹å¾"""
        try:
            # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼
            self.df['datetime'] = pd.to_datetime(self.df['Timestamp'], errors='coerce')

            # æå–æ—¶é—´ç‰¹å¾
            self.df['hour'] = self.df['datetime'].dt.hour
            self.df['minute'] = self.df['datetime'].dt.minute
            self.df['second'] = self.df['datetime'].dt.second
            self.df['day_of_week'] = self.df['datetime'].dt.dayofweek  # 0=Monday

            print("âœ… æå–æ—¶é—´ç‰¹å¾: hour, minute, second, day_of_week")
        except:
            return 0



    def create_statistical_features(self):
        """åˆ›å»ºæµé‡ç»Ÿè®¡é‡ç‰¹å¾ï¼ˆåŸºäºæ¯æ¡æµï¼‰"""
        print("\nğŸ“Š åˆ›å»ºç»Ÿè®¡é‡ç‰¹å¾...")
        orig_cols = self.df.shape[1]

        # 1. æµé‡é€Ÿç‡ç»Ÿè®¡ï¼ˆBytes/sec & Packets/secï¼‰
        if 'Flow Duration' in self.df.columns:
            # é¿å…é™¤é›¶
            duration = self.df['Flow Duration'].replace(0, 1)

            # å‰å‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Fwd Packet' in self.df.columns:
                self.df['fwd_bytes_per_sec'] = (
                        self.df['Total Length of Fwd Packet'] / duration
                )
                self.df['fwd_bytes_per_sec_log'] = np.log1p(
                    self.df['fwd_bytes_per_sec']
                )

            # åå‘å­—èŠ‚é€Ÿç‡ & å¯¹æ•°å˜æ¢
            if 'Total Length of Bwd Packet' in self.df.columns:
                self.df['bwd_bytes_per_sec'] = (
                        self.df['Total Length of Bwd Packet'] / duration
                )
                self.df['bwd_bytes_per_sec_log'] = np.log1p(
                    self.df['bwd_bytes_per_sec']
                )

            # å‰å‘åŒ…é€Ÿç‡
            if 'Total Fwd Packet' in self.df.columns:
                self.df['fwd_packets_per_sec'] = (
                        self.df['Total Fwd Packet'] / duration
                )

            # åå‘åŒ…é€Ÿç‡
            if 'Total Bwd Packet' in self.df.columns:
                self.df['bwd_packets_per_sec'] = (
                        self.df['Total Bwd Packet'] / duration
                )

        # 2. åŒ…é•¿åº¦æ¯”ä¾‹
        if (
                'Fwd Packet Length Mean' in self.df.columns
                and 'Bwd Packet Length Mean' in self.df.columns
        ):
            bwd_mean = self.df['Bwd Packet Length Mean'].replace(0, 1)
            self.df['fwd_bwd_length_ratio'] = (
                    self.df['Fwd Packet Length Mean'] / bwd_mean
            )

        added = self.df.shape[1] - orig_cols
        print(f"âœ… ç»Ÿè®¡é‡ç‰¹å¾åˆ›å»ºå®Œæˆï¼Œæ–°å¢ {added} ä¸ªç‰¹å¾")
        return self

    def encode_and_normalize(self):
        """ç¼–ç å’Œå½’ä¸€åŒ–"""
        print("\nğŸ”¢ ç¼–ç å’Œå½’ä¸€åŒ–...")

        # 1. å¯¹æ‰€æœ‰åˆ†ç±»ç‰¹å¾è¿›è¡ŒLabel Encoding
        categorical_cols = []

        # æ£€æµ‹åˆ†ç±»åˆ—
        for col in self.df.columns:
            if self.df[col].dtype == 'object' or col in ['Stage', 'Activity', 'Protocol']:
                if col in self.df.columns:
                    categorical_cols.append(col)

        print(f"æ£€æµ‹åˆ°åˆ†ç±»åˆ—: {categorical_cols}")

        for col in categorical_cols:
            if col in self.df.columns:
                le = LabelEncoder()
                # å¤„ç†ç¼ºå¤±å€¼
                self.df[col] = self.df[col].astype(str).fillna('Unknown')
                self.df[f'{col}_encoded'] = le.fit_transform(self.df[col])
                self.label_encoders[col] = le
                print(f"âœ… Labelç¼–ç  {col} åˆ—: {len(le.classes_)} ä¸ªç±»åˆ«")

                # æ‰“å°ç¼–ç æ˜ å°„ï¼ˆå‰5ä¸ªï¼‰
                mapping = dict(zip(le.classes_[:5], le.transform(le.classes_[:5])))
                print(f"   æ˜ å°„ç¤ºä¾‹: {mapping}{'...' if len(le.classes_) > 5 else ''}")

        # 2. å¯¹æ‰€æœ‰æ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()

        # æ’é™¤ç¼–ç åçš„æ ‡ç­¾åˆ—ï¼ˆä¿æŒåŸå§‹æ•´æ•°å€¼ï¼‰
        exclude_cols = [col for col in numeric_cols if col.endswith('_encoded')]
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        if feature_cols:
            print(f"å¯¹ {len(feature_cols)} ä¸ªæ•°å€¼ç‰¹å¾è¿›è¡ŒMin-Maxå½’ä¸€åŒ–...")
            scaler = MinMaxScaler()  # ä½¿ç”¨Min-Maxå½’ä¸€åŒ–
            self.df[feature_cols] = scaler.fit_transform(self.df[feature_cols])
            self.scaler = scaler  # ä¿å­˜scalerä»¥å¤‡åç”¨
            print("âœ… Min-Maxå½’ä¸€åŒ–å®Œæˆ: X_new = (X - min) / (max - min)")

        print(f"âœ… ç¼–ç å’Œå½’ä¸€åŒ–å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {self.df.shape}")
        return self

    def prepare_paper_aligned_features(self):
        # 1) è‡ªåŠ¨è¯†åˆ«â€œå”¯ä¸€â€ç›®æ ‡åˆ—
        for col in ('Label_encoded', 'Label', 'Stage_encoded', 'Stage'):
            if col in self.df.columns:
                target = col
                break
        else:
            raise ValueError("æ‰¾ä¸åˆ°ç›®æ ‡åˆ—ï¼šLabel/Label_encoded/Stage/Stage_encoded")

        # 2) å–æ‰€æœ‰æ•°å€¼å‹åˆ—
        numeric_cols = self.df.select_dtypes(include=[int, float]).columns.tolist()

        # 3) æ„é€ æ’é™¤æ¨¡å¼ï¼šæ ‡ç­¾ã€Activity + åç»­è¡ç”Ÿ
        exclude = {target, 'Activity', 'Activity_encoded'}
        # æ’é™¤æ‰€æœ‰åç§°ä¸­å«ä»¥ä¸‹å…³é”®å­—çš„åˆ—
        bad_kw = ('_encoded', '_log', 'per_sec', 'bulk', 'ratio')
        for c in numeric_cols:
            low = c.lower()
            if any(kw in low for kw in bad_kw):
                exclude.add(c)

        # 4) æœ€ç»ˆçš„å€™é€‰æ±  = numeric_cols - exclude
        feature_cols = [c for c in numeric_cols if c not in exclude]

        # 5) è¡¥å……æ—¶é—´æ‹†åˆ†ï¼ˆè‹¥å­˜åœ¨çš„è¯ï¼‰
        for tf in ('hour', 'minute', 'second', 'day_of_week'):
            if tf in self.df.columns and tf not in feature_cols:
                feature_cols.append(tf)

        # æ£€æŸ¥è®ºæ–‡ä¸­çš„å…³é”®ç‰¹å¾ï¼ˆåŒ…å«æ–°å¢çš„3ä¸ªé‡è¦ç‰¹å¾ï¼‰
        paper_critical_features = [
            'Protocol_encoded',  # åè®®ç‰¹å¾ï¼ˆTCP/UDPï¼‰
            'Flow Duration',     # æµæŒç»­æ—¶é—´
            'Fwd IAT Total',     # æ­£å‘IATæ€»æ—¶é—´
            'Fwd IAT Mean',      # æ­£å‘IATå¹³å‡æ—¶é—´
            'Bwd IAT Std',       # åå‘IATæ ‡å‡†å·®
            'Bwd IAT Max',       # åå‘IATæœ€å¤§å€¼
            'Bwd IAT Min',       # åå‘IATæœ€å°å€¼
            'FIN Flag Count',    # FINæ ‡å¿—ä½è®¡æ•°
            'SYN Flag Count',    # SYNæ ‡å¿—ä½è®¡æ•°
            'RST Flag Count',    # RSTæ ‡å¿—ä½è®¡æ•°
            'PSH Flag Count',    # PSHæ ‡å¿—ä½è®¡æ•°
            'ACK Flag Count',    # ACKæ ‡å¿—ä½è®¡æ•°
            'URG Flag Count',    # URGæ ‡å¿—ä½è®¡æ•°
            # æ–°å¢çš„3ä¸ªé‡è¦ç‰¹å¾
            'Total Length of Fwd Packet',  # å‰å‘åŒ…æ€»é•¿åº¦
            'Fwd Packet Length Min',       # å‰å‘åŒ…æœ€å°é•¿åº¦
            'Flow IAT Min'                 # æµé—´éš”æœ€å°å€¼
        ]

        # æ£€æŸ¥å…³é”®ç‰¹å¾çš„å­˜åœ¨æƒ…å†µ
        critical_found = []
        critical_missing = []

        for feat in paper_critical_features:
            if feat in self.df.columns:
                if feat not in feature_cols:
                    feature_cols.append(feat)  # ç¡®ä¿å…³é”®ç‰¹å¾è¢«åŒ…å«
                critical_found.append(feat)
            else:
                critical_missing.append(feat)

        # æ‰“å°ç¡®è®¤
        print(f"âœ… ç›®æ ‡åˆ—ï¼ˆå·²æ’é™¤ï¼‰: {target}")
        print(f"âœ… æ’é™¤äº† {len(exclude)} åˆ—ï¼Œå€™é€‰æ± å…±æœ‰ {len(feature_cols)} åˆ—")
        print("å€™é€‰ç‰¹å¾ç¤ºä¾‹:", feature_cols[:10], "...")

        # è¯¦ç»†åˆ†æè®ºæ–‡å…³é”®ç‰¹å¾
        print(f"\nğŸ“Š è®ºæ–‡å…³é”®ç‰¹å¾æ£€æŸ¥:")
        print(f"  è®ºæ–‡å…³é”®ç‰¹å¾æ‰¾åˆ°: {len(critical_found)}/13")

        if critical_found:
            print(f"\nâœ… æ‰¾åˆ°çš„è®ºæ–‡å…³é”®ç‰¹å¾:")
            for i, feat in enumerate(critical_found, 1):
                print(f"  {i:2d}. {feat}")

        if critical_missing:
            print(f"\nâŒ ç¼ºå¤±çš„è®ºæ–‡å…³é”®ç‰¹å¾:")
            for i, feat in enumerate(critical_missing, 1):
                print(f"  {i:2d}. {feat}")

        # æ£€æŸ¥ç‰¹å¾ç±»åˆ«åˆ†å¸ƒ
        protocol_features = [f for f in feature_cols if 'protocol' in f.lower()]
        duration_features = [f for f in feature_cols if 'duration' in f.lower()]
        iat_features = [f for f in feature_cols if 'iat' in f.lower()]
        flag_features = [f for f in feature_cols if 'flag' in f.lower()]
        time_features = ['hour', 'minute', 'day_of_week']
        time_in_candidates = [f for f in feature_cols if f in time_features]

        print(f"\nğŸ” ç‰¹å¾ç±»åˆ«åˆ†å¸ƒ:")
        print(f"  åè®®ç‰¹å¾: {len(protocol_features)} ({protocol_features})")
        print(f"  æµæŒç»­æ—¶é—´: {len(duration_features)} ({duration_features})")
        print(f"  IATæ—¶é—´é—´éš”: {len(iat_features)}")
        print(f"  TCPæ ‡å¿—ä½: {len(flag_features)}")
        print(f"  æ—¶é—´ç‰¹å¾: {len(time_in_candidates)} ({time_in_candidates})")

        self.candidate_features = feature_cols
        return self

    def evaluate_with_fold_internal_pipeline(self, n_features=46, cv_folds=10):
        """æŒ‰è®ºæ–‡æ–¹æ³•ï¼šæ¯æŠ˜å†…éƒ¨åšç‰¹å¾é€‰æ‹©+æ ‡å‡†åŒ–+åˆ†ç±»çš„å®Œæ•´pipeline"""
        print(f"\nğŸ¯ è®ºæ–‡æ–¹æ³•ï¼šæ¯æŠ˜å†…Pipeline(ç‰¹å¾é€‰æ‹©â†’æ ‡å‡†åŒ–â†’åˆ†ç±»)")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.candidate_features]
        y = self.df[target]

        print(f"å€™é€‰ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")

        # æŒ‰è®ºæ–‡é…ç½®æ¨¡å‹ï¼ˆä¼˜åŒ–è¶…å‚æ•°ï¼‰
        models = {
            'XGBoost': xgb.XGBClassifier(
                n_estimators=300,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                eval_metric='mlogloss'
            ),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(150, 100, 50),  # æ›´æ·±çš„ç½‘ç»œç»“æ„
                max_iter=2000,     # å¢åŠ è¿­ä»£æ¬¡æ•°
                early_stopping=True,
                validation_fraction=0.1,
                learning_rate_init=1e-3,
                alpha=0.001,       # L2æ­£åˆ™åŒ–
                random_state=42
            ),
            'SVM': SVC(
                kernel='rbf',
                C=10.0,           # ä¼˜åŒ–Cå‚æ•°
                gamma='scale',    # è‡ªåŠ¨è°ƒæ•´gamma
                random_state=42
            )
        }

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

        self.cv_results = {}

        for name, clf in models.items():
            print(f"\nğŸš€ è¯„ä¼°æ¨¡å‹: {name}")

            # æ„å»ºæŠ˜å†…Pipelineï¼šç‰¹å¾é€‰æ‹© â†’ æ ‡å‡†åŒ– â†’ åˆ†ç±»å™¨
            if name == 'XGBoost':
                # XGBoostä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†éœ€è¦ç‰¹å¾é€‰æ‹©
                pipeline = Pipeline([
                    ('feat_sel', SelectFromModel(
                        xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),
                        threshold=-np.inf,
                        max_features=n_features,
                        importance_getter='feature_importances_'
                    )),
                    ('clf', clf)
                ])
            else:
                # MLPå’ŒSVMéœ€è¦ç‰¹å¾é€‰æ‹©+æ ‡å‡†åŒ–
                pipeline = Pipeline([
                    ('feat_sel', SelectFromModel(
                        xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss'),
                        threshold=-np.inf,
                        max_features=n_features,
                        importance_getter='feature_importances_'
                    )),
                    ('scaler', StandardScaler()),
                    ('clf', clf)
                ])

            # äº¤å‰éªŒè¯è¯„ä¼°
            cv_results = cross_validate(
                pipeline, X, y,
                cv=skf,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False,
                return_estimator = True
            )
            for fold_idx, est in enumerate(cv_results['estimator'], 1):
                # feat_sel æ˜¯ SelectFromModel è¿™ä¸€æ­¥
                mask = est.named_steps['feat_sel'].get_support()
                selected_feats = X.columns[mask].tolist()
                print(f"Fold {fold_idx} é€‰å‡ºçš„ {len(selected_feats)} ä¸ªç‰¹å¾ï¼š")
                print(selected_feats)

            # è®¡ç®—ç»Ÿè®¡é‡
            results = {}
            for metric in scoring:
                scores = cv_results[f'test_{metric}']
                results[metric] = {
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'scores': scores.tolist()
                }
                print(f"  {metric:<15}: {scores.mean():.4f} Â± {scores.std():.4f}")

            self.cv_results[name] = results

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        best_f1 = self.cv_results[best_model]['f1_macro']['mean']
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (F1: {best_f1:.4f})")

        return self

    def evaluate_multiple_models(self, cv_folds=10):
        """æŒ‰è®ºæ–‡æ–¹æ³•ï¼šç”¨å›ºå®šçš„46ä¸ªç‰¹å¾ï¼Œåœ¨10æŠ˜CVä¸­è¯„ä¼°å¤šæ¨¡å‹"""
        print(f"\nğŸ¯ ä½¿ç”¨å›ºå®šçš„{len(self.selected_features)}ä¸ªç‰¹å¾è¿›è¡Œ{cv_folds}æŠ˜äº¤å‰éªŒè¯")

        # å‡†å¤‡æ•°æ®
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")

        # æŒ‰è®ºæ–‡é…ç½®æ¨¡å‹è¶…å‚æ•°
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            ),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=0.01,
                random_state=42
            ),
            'SVM': SVC(
                kernel='rbf',
                C=1.0,
                random_state=42
            )
        }

        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']

        self.cv_results = {}

        for name, clf in models.items():
            print(f"\nğŸš€ è¯„ä¼°æ¨¡å‹: {name}")

            # å¯¹MLPå’ŒSVMä½¿ç”¨Pipelineè¿›è¡ŒæŠ˜å†…æ ‡å‡†åŒ–
            if name in ('MLP', 'SVM'):
                estimator = Pipeline([
                    ('scaler', StandardScaler()),
                    ('clf', clf)
                ])
            else:
                estimator = clf

            # ä½¿ç”¨cross_validateè¿›è¡Œè¯„ä¼°
            from sklearn.model_selection import cross_validate
            cv_results = cross_validate(
                estimator, X, y,
                cv=skf,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False
            )

            # è®¡ç®—ç»Ÿè®¡é‡
            results = {}
            for metric in scoring:
                scores = cv_results[f'test_{metric}']
                results[metric] = {
                    'mean': scores.mean(),
                    'std': scores.std(),
                    'scores': scores.tolist()
                }
                print(f"  {metric:<15}: {scores.mean():.4f} Â± {scores.std():.4f}")

            self.cv_results[name] = results

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        best_f1 = self.cv_results[best_model]['f1_macro']['mean']
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (F1: {best_f1:.4f})")

        return self

    def detailed_model_analysis(self, cv_folds=10):
        """è¯¦ç»†åˆ†ææœ€ä½³æ¨¡å‹ï¼šèšåˆ10æŠ˜CVçš„é¢„æµ‹ç»“æœ"""
        print(f"\nğŸ” è¯¦ç»†æ¨¡å‹åˆ†æ (èšåˆ{cv_folds}æŠ˜ç»“æœ)")

        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        X = self.df[self.selected_features]
        y = self.df[target]

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.cv_results.keys(),
                        key=lambda x: self.cv_results[x]['f1_macro']['mean'])
        print(f"ğŸ† åˆ†ææœ€ä½³æ¨¡å‹: {best_model}")

        # é‡å»ºæœ€ä½³æ¨¡å‹
        if best_model == 'RandomForest':
            clf = RandomForestClassifier(
                n_estimators=300,
                class_weight='balanced',
                random_state=42,
                n_jobs=-1
            )
            estimator = clf
        elif best_model == 'MLP':
            clf = MLPClassifier(
                hidden_layer_sizes=(100,),
                max_iter=1000,
                early_stopping=True,
                learning_rate_init=1e-3,
                random_state=42
            )
            estimator = Pipeline([('scaler', StandardScaler()), ('clf', clf)])
        else:  # SVM
            clf = SVC(kernel='rbf', C=1.0, random_state=42)
            estimator = Pipeline([('scaler', StandardScaler()), ('clf', clf)])

        # 10æŠ˜äº¤å‰éªŒè¯ï¼Œæ”¶é›†æ‰€æœ‰é¢„æµ‹
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        y_true_all, y_pred_all = [], []

        print("ğŸ”„ æ‰§è¡Œ10æŠ˜äº¤å‰éªŒè¯...")
        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # è®­ç»ƒå’Œé¢„æµ‹
            estimator.fit(X_train, y_train)
            y_pred = estimator.predict(X_test)

            y_true_all.extend(y_test.values)
            y_pred_all.extend(y_pred)

            print(f"  æŠ˜ {fold:2d} å®Œæˆ")

        # èšåˆç»“æœ
        y_true = np.array(y_true_all)
        y_pred = np.array(y_pred_all)

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='macro')
        recall = recall_score(y_true, y_pred, average='macro')
        f1 = f1_score(y_true, y_pred, average='macro')

        print(f"\nğŸ“Š èšåˆç»“æœ ({best_model}):")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"  å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"  F1åˆ†æ•° (F1-Score): {f1:.4f}")

        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred))

        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print(confusion_matrix(y_true, y_pred))

        # æ·»åŠ æ¯ä¸ªæ”»å‡»é˜¶æ®µçš„è¯¦ç»†åˆ†æ
        self._analyze_per_stage_performance(y_true, y_pred)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.models_performance = {
            'best_model': best_model,
            'aggregated_metrics': {
                'accuracy': accuracy,
                'precision_macro': precision,
                'recall_macro': recall,
                'f1_macro': f1
            },
            'classification_report': classification_report(y_true, y_pred, output_dict=True),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
        }

        return self

    def _analyze_per_stage_performance(self, y_true, y_pred):
        """åˆ†ææ¯ä¸ªAPTæ”»å‡»é˜¶æ®µçš„æ£€æµ‹æ€§èƒ½"""
        print(f"\nğŸ¯ æ¯ä¸ªAPTæ”»å‡»é˜¶æ®µçš„æ£€æµ‹æ€§èƒ½åˆ†æ:")
        print("="*60)

        # å®šä¹‰é˜¶æ®µæ˜ å°„
        stage_names = {
            0: 'Benign (æ­£å¸¸æµé‡)',
            4: 'Data Exfiltration (æ•°æ®æ¸—é€)',
            2: 'Establish Foothold (å»ºç«‹ç«‹è¶³ç‚¹)',
            3: 'Lateral Movement (æ¨ªå‘ç§»åŠ¨)',
            1: 'Reconnaissance (ä¾¦å¯Ÿ)'
        }

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
        from sklearn.metrics import precision_recall_fscore_support, confusion_matrix

        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, zero_division=0
        )

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)

        print(f"{'é˜¶æ®µ':<25} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•°':<10} {'æ ·æœ¬æ•°':<8} {'æ£€æµ‹ç‡':<10}")
        print("-" * 80)

        total_correct = 0
        total_samples = 0

        for i, (stage_id, stage_name) in enumerate(stage_names.items()):
            if i < len(precision):
                # è®¡ç®—æ£€æµ‹ç‡ (è¯¥é˜¶æ®µè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹)
                detection_rate = cm[i, i] / support[i] if support[i] > 0 else 0

                print(f"{stage_name:<25} {precision[i]:<10.4f} {recall[i]:<10.4f} "
                      f"{f1[i]:<10.4f} {support[i]:<8d} {detection_rate:<10.4f}")

                total_correct += cm[i, i]
                total_samples += support[i]

        print("-" * 80)
        overall_accuracy = total_correct / total_samples if total_samples > 0 else 0
        print(f"{'æ€»ä½“å‡†ç¡®ç‡':<25} {'':<10} {'':<10} {'':<10} {total_samples:<8d} {overall_accuracy:<10.4f}")

        # åˆ†ææ”»å‡»é˜¶æ®µé—´çš„æ··æ·†æƒ…å†µ
        print(f"\nğŸ” æ”»å‡»é˜¶æ®µé—´æ··æ·†åˆ†æ:")
        print("="*50)

        for i, (true_stage_id, true_stage_name) in enumerate(stage_names.items()):
            if i < len(cm):
                print(f"\nçœŸå®é˜¶æ®µ: {true_stage_name}")
                for j, (pred_stage_id, pred_stage_name) in enumerate(stage_names.items()):
                    if j < len(cm[i]) and cm[i, j] > 0:
                        confusion_rate = cm[i, j] / support[i] if support[i] > 0 else 0
                        if i != j:  # åªæ˜¾ç¤ºé”™è¯¯åˆ†ç±»
                            print(f"  â†’ è¯¯åˆ†ç±»ä¸º {pred_stage_name}: {cm[i, j]} æ ·æœ¬ ({confusion_rate:.3f})")

        # è®¡ç®—å®å¹³å‡æŒ‡æ ‡
        macro_precision = np.mean(precision)
        macro_recall = np.mean(recall)
        macro_f1 = np.mean(f1)

        print(f"\nğŸ“Š å®å¹³å‡æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å®å¹³å‡ç²¾ç¡®ç‡: {macro_precision:.4f}")
        print(f"  å®å¹³å‡å¬å›ç‡: {macro_recall:.4f}")
        print(f"  å®å¹³å‡F1åˆ†æ•°: {macro_f1:.4f}")

        # è¯†åˆ«è¡¨ç°æœ€å¥½å’Œæœ€å·®çš„é˜¶æ®µ
        best_stage_idx = np.argmax(f1)
        worst_stage_idx = np.argmin(f1)

        print(f"\nğŸ† æ€§èƒ½åˆ†æ:")
        print(f"  æœ€ä½³æ£€æµ‹é˜¶æ®µ: {list(stage_names.values())[best_stage_idx]} (F1: {f1[best_stage_idx]:.4f})")
        print(f"  æœ€å·®æ£€æµ‹é˜¶æ®µ: {list(stage_names.values())[worst_stage_idx]} (F1: {f1[worst_stage_idx]:.4f})")

        # ä¿å­˜æ¯é˜¶æ®µæ€§èƒ½
        self.per_stage_performance = {
            'stage_names': stage_names,
            'precision': precision.tolist(),
            'recall': recall.tolist(),
            'f1_score': f1.tolist(),
            'support': support.tolist(),
            'confusion_matrix': cm.tolist(),
            'macro_avg': {
                'precision': macro_precision,
                'recall': macro_recall,
                'f1_score': macro_f1
            }
        }

    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {self.output_path}")

        # ä¿å­˜å¢å¼ºåçš„æ•°æ®
        enhanced_data_path = os.path.join(self.output_path, 'enhanced_apt_data.csv')
        self.df.to_csv(enhanced_data_path, index=False)
        print(f"âœ… å¢å¼ºæ•°æ®ä¿å­˜è‡³: {enhanced_data_path}")

        # ä¿å­˜é€‰æ‹©çš„ç‰¹å¾æ•°æ®
        if self.selected_features:
            target_col = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
            selected_cols = self.selected_features + [target_col, 'Stage']
            selected_data_path = os.path.join(self.output_path, 'selected_features_data.csv')
            self.df[selected_cols].to_csv(selected_data_path, index=False)
            print(f"âœ… é€‰æ‹©ç‰¹å¾æ•°æ®ä¿å­˜è‡³: {selected_data_path}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            importance_path = os.path.join(self.output_path, 'feature_importance.csv')
            self.feature_importance.to_csv(importance_path, index=False)
            print(f"âœ… ç‰¹å¾é‡è¦æ€§ä¿å­˜è‡³: {importance_path}")

        # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
        if self.cv_results:
            cv_path = os.path.join(self.output_path, 'cross_validation_results.json')
            with open(cv_path, 'w') as f:
                json.dump(self.cv_results, f, indent=4)
            print(f"âœ… äº¤å‰éªŒè¯ç»“æœä¿å­˜è‡³: {cv_path}")

        # ä¿å­˜è¯¦ç»†æ¨¡å‹æ€§èƒ½
        if self.models_performance:
            models_path = os.path.join(self.output_path, 'models_performance.json')
            with open(models_path, 'w') as f:
                json.dump(self.models_performance, f, indent=4)
            print(f"âœ… æ¨¡å‹æ€§èƒ½ç»“æœä¿å­˜è‡³: {models_path}")

        # ä¿å­˜æ¯é˜¶æ®µæ€§èƒ½åˆ†æ
        if hasattr(self, 'per_stage_performance'):
            stage_path = os.path.join(self.output_path, 'per_stage_performance.json')
            with open(stage_path, 'w') as f:
                json.dump(self.per_stage_performance, f, indent=4)
            print(f"âœ… æ¯é˜¶æ®µæ€§èƒ½åˆ†æä¿å­˜è‡³: {stage_path}")

        return self

    def run_complete_pipeline(self, n_features=46, cv_folds=10):
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹")
        print("="*80)

        start_time = time.time()

        try:
            # æ‰§è¡Œå®Œæ•´æµç¨‹ï¼ˆæŒ‰è®ºæ–‡æ–¹æ³•ä¿®æ­£ï¼‰
            (self
             .load_data()
             .clean_data()
             .create_statistical_features()
             .encode_and_normalize()
             .prepare_paper_aligned_features()
             .evaluate_with_fold_internal_pipeline(n_features=n_features, cv_folds=cv_folds)
             .save_results())

            total_time = time.time() - start_time

            print(f"\nğŸ‰ é¢„å¤„ç†å’Œè¯„ä¼°å®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ¯ é€‰æ‹©ç‰¹å¾æ•°é‡: {len(self.selected_features)}")

            # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
            if self.cv_results:
                print(f"\nğŸ“ˆ è®ºæ–‡æ–¹æ³•æ¨¡å‹æ€§èƒ½æ‘˜è¦ ({cv_folds}æŠ˜äº¤å‰éªŒè¯):")
                for model_name, metrics in self.cv_results.items():
                    f1_mean = metrics['f1_macro']['mean']
                    f1_std = metrics['f1_macro']['std']
                    print(f"  {model_name}: F1={f1_mean:.4f}Â±{f1_std:.3f}")

            # æ‰“å°ç‰¹å¾é€‰æ‹©æ‘˜è¦
            if hasattr(self, 'candidate_features'):
                print(f"\nğŸ¯ è®ºæ–‡å¯¹é½ç‰¹å¾æ‘˜è¦:")
                print(f"  å€™é€‰ç‰¹å¾æ•°é‡: {len(self.candidate_features)}")

                # æ£€æŸ¥æ—¶é—´ç‰¹å¾
                time_features = ['hour', 'minute', 'day_of_week']
                time_in_candidates = [f for f in self.candidate_features if f in time_features]
                print(f"  åŒ…å«æ—¶é—´ç‰¹å¾: {time_in_candidates}")

                # æ£€æŸ¥æ˜¯å¦æ’é™¤äº†Activity
                activity_excluded = not any('activity' in f.lower() for f in self.candidate_features)
                print(f"  å·²æ’é™¤Activityæ ‡ç­¾: {'âœ…' if activity_excluded else 'âŒ'}")

                # æ˜¾ç¤ºç‰¹å¾ç±»å‹åˆ†å¸ƒ
                network_features = len([f for f in self.candidate_features if f not in time_features + ['Protocol_encoded']])
                print(f"  ç½‘ç»œæµç‰¹å¾: {network_features}")
                print(f"  æ—¶é—´ç‰¹å¾: {len(time_in_candidates)}")
                print(f"  åè®®ç‰¹å¾: {1 if 'Protocol_encoded' in self.candidate_features else 0}")

            # ä¸è®ºæ–‡ç»“æœå¯¹æ¯”
            if self.cv_results:
                best_f1 = max(metrics['f1_macro']['mean'] for metrics in self.cv_results.values())
                print(f"\nğŸ¯ ä¸è®ºæ–‡å¯¹æ¯”:")
                print(f"  æˆ‘ä»¬çš„æœ€ä½³F1: {best_f1:.4f}")
                print(f"  è®ºæ–‡æŠ¥å‘ŠF1: ~0.9800")
                print(f"  å·®è·: {0.98 - best_f1:.4f}")
                if best_f1 >= 0.97:
                    print(f"  âœ… æ¥è¿‘è®ºæ–‡æ°´å¹³ï¼")
                elif best_f1 >= 0.95:
                    print(f"  ğŸ”¶ è‰¯å¥½æ°´å¹³ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–")
                else:
                    print(f"  âš ï¸ éœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜")

        except Exception as e:
            print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

        return self

    def build_attack_sequences(self, num_apt_sequences=1000, min_normal_insert=1, max_normal_insert=5):
        """æ„å»ºæ”»å‡»åºåˆ—ï¼Œå‚è€ƒdapt_preprocessing.pyçš„æ–¹æ³•"""
        print(f"\nğŸ”— æ„å»ºæ”»å‡»åºåˆ— (ç”Ÿæˆ{num_apt_sequences}ä¸ªAPTåºåˆ—)")

        # ç¡®å®šç›®æ ‡å˜é‡
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'

        # æ„å»ºattack2idæ˜ å°„
        self._build_attack2id_mapping()

        # æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®
        self._partition_data_by_stage(target)

        # æ„å»ºAPTåºåˆ—æ ‡ç­¾
        self._build_apt_sequence_labels(num_apt_sequences, min_normal_insert, max_normal_insert)

        # æ„å»ºæ­£å¸¸åºåˆ—æ ‡ç­¾
        self._build_normal_sequence_labels()

        # ä¸ºåºåˆ—é€‰æ‹©å®é™…æ•°æ®æ ·æœ¬
        self._select_samples_for_sequences()

        # è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—
        self._convert_labels_to_ids()

        # åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾
        self._assign_final_sequence_labels()

        # ä¿å­˜åºåˆ—æ•°æ®
        self._save_sequence_results()

        print(f"âœ… æ”»å‡»åºåˆ—æ„å»ºå®Œæˆ")
        print(f"  APTåºåˆ—æ•°é‡: {len(self.apt_sequences_data)}")
        print(f"  æ­£å¸¸åºåˆ—æ•°é‡: {len(self.normal_sequences_data)}")
        print(f"  Attack2IDæ˜ å°„: {self.attack2id}")

        return self

    def _build_attack2id_mapping(self):
        """æ„å»ºattack2idæ˜ å°„"""
        print("æ„å»ºattack2idæ˜ å°„...")

        # åŸºäºStage_encodedçš„å€¼æ„å»ºæ˜ å°„
        target = 'Stage_encoded' if 'Stage_encoded' in self.df.columns else 'Label'
        unique_stages = sorted(self.df[target].unique())

        # åˆ›å»ºå†…éƒ¨æ ‡ç­¾æ˜ å°„
        self.stage_to_internal = {
            0: 'SN',  # æ­£å¸¸æµé‡
            1: 'S1',  # æ•°æ®æ¸—é€
            2: 'S2',  # å»ºç«‹ç«‹è¶³ç‚¹
            3: 'S3',  # æ¨ªå‘ç§»åŠ¨
            4: 'S4'   # ä¾¦å¯Ÿ
        }

        # æ„å»ºattack2idæ˜ å°„
        self.attack2id = {
            'SN': 0,  # æ­£å¸¸æµé‡
            'S1': 1,  # æ•°æ®æ¸—é€
            'S2': 2,  # å»ºç«‹ç«‹è¶³ç‚¹
            'S3': 3,  # æ¨ªå‘ç§»åŠ¨
            'S4': 4   # ä¾¦å¯Ÿ
        }

        print(f"  Attack2IDæ˜ å°„: {self.attack2id}")

    def _partition_data_by_stage(self, target):
        """æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®"""
        print("æŒ‰æ”»å‡»é˜¶æ®µåˆ†ç»„æ•°æ®...")

        self.stage_dataframes = {}

        for stage_encoded, internal_label in self.stage_to_internal.items():
            stage_data = self.df[self.df[target] == stage_encoded]
            if not stage_data.empty:
                # åªä¿ç•™é€‰æ‹©çš„ç‰¹å¾
                if hasattr(self, 'candidate_features'):
                    stage_data = stage_data[self.candidate_features]
                self.stage_dataframes[internal_label] = stage_data
                print(f"  {internal_label}: {len(stage_data)} æ ·æœ¬")
            else:
                print(f"  {internal_label}: 0 æ ·æœ¬ (è·³è¿‡)")

    def _build_apt_sequence_labels(self, num_apt_sequences, min_normal_insert, max_normal_insert):
        """æ„å»ºAPTåºåˆ—æ ‡ç­¾"""
        print("æ„å»ºAPTåºåˆ—æ ‡ç­¾...")

        import random
        random.seed(42)

        # å¯ç”¨çš„æ”»å‡»é˜¶æ®µï¼ˆæŒ‰é¡ºåºï¼‰
        available_attack_stages = ['S1', 'S2', 'S3', 'S4']
        normal_stage = 'SN'

        # åºåˆ—ç±»å‹æ•°é‡ï¼ˆ1-4ä¸ªæ”»å‡»é˜¶æ®µï¼‰
        num_sequence_types = len(available_attack_stages)
        num_per_type = num_apt_sequences // num_sequence_types
        remainder = num_apt_sequences % num_sequence_types

        self.apt_sequences_labels = []

        for i in range(num_sequence_types):
            # å®šä¹‰å½“å‰åºåˆ—ç±»å‹çš„åŸºç¡€æ”»å‡»é˜¶æ®µ
            current_base_stages = available_attack_stages[:i+1]
            print(f"  ç”Ÿæˆç±»å‹{i+1} (æœ€å¤§é˜¶æ®µ: {current_base_stages[-1]}): {current_base_stages}")

            num_for_this_type = num_per_type + (1 if i < remainder else 0)

            for _ in range(num_for_this_type):
                # åŸºç¡€åºåˆ—
                current_seq = list(current_base_stages)

                # æ’å…¥éšæœºæ•°é‡çš„æ­£å¸¸æµé‡
                num_sn_to_insert = random.randint(min_normal_insert, max_normal_insert)
                for _ in range(num_sn_to_insert):
                    insert_pos = random.randint(0, len(current_seq))
                    current_seq.insert(insert_pos, normal_stage)

                self.apt_sequences_labels.append(current_seq)

        print(f"  æ„å»ºäº†{len(self.apt_sequences_labels)}ä¸ªAPTåºåˆ—æ ‡ç­¾")

    def _build_normal_sequence_labels(self):
        """æ„å»ºæ­£å¸¸åºåˆ—æ ‡ç­¾"""
        print("æ„å»ºæ­£å¸¸åºåˆ—æ ‡ç­¾...")

        self.normal_sequences_labels = []

        # ä¸ºæ¯ä¸ªAPTåºåˆ—ç”Ÿæˆå¯¹åº”é•¿åº¦çš„æ­£å¸¸åºåˆ—
        for apt_seq in self.apt_sequences_labels:
            normal_len = len(apt_seq)
            self.normal_sequences_labels.append(['SN'] * normal_len)

        print(f"  æ„å»ºäº†{len(self.normal_sequences_labels)}ä¸ªæ­£å¸¸åºåˆ—æ ‡ç­¾")

    def _select_samples_for_sequences(self):
        """ä¸ºåºåˆ—é€‰æ‹©å®é™…æ•°æ®æ ·æœ¬"""
        print("ä¸ºåºåˆ—é€‰æ‹©å®é™…æ•°æ®æ ·æœ¬...")

        import random
        random.seed(42)

        self.apt_sequences_data = []
        self.normal_sequences_data = []

        # ä¸ºAPTåºåˆ—é€‰æ‹©æ ·æœ¬
        for seq_labels in self.apt_sequences_labels:
            sequence_data = []
            for stage_label in seq_labels:
                if stage_label in self.stage_dataframes:
                    stage_df = self.stage_dataframes[stage_label]
                    if not stage_df.empty:
                        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
                        sample = stage_df.sample(1).iloc[0].to_dict()
                        sequence_data.append(sample)
                    else:
                        print(f"    è­¦å‘Š: {stage_label} é˜¶æ®µæ•°æ®ä¸ºç©º")
                else:
                    print(f"    è­¦å‘Š: æ‰¾ä¸åˆ° {stage_label} é˜¶æ®µæ•°æ®")

            if sequence_data:  # åªæ·»åŠ éç©ºåºåˆ—
                self.apt_sequences_data.append(sequence_data)

        # ä¸ºæ­£å¸¸åºåˆ—é€‰æ‹©æ ·æœ¬
        for seq_labels in self.normal_sequences_labels:
            sequence_data = []
            for stage_label in seq_labels:  # éƒ½æ˜¯'SN'
                if stage_label in self.stage_dataframes:
                    stage_df = self.stage_dataframes[stage_label]
                    if not stage_df.empty:
                        sample = stage_df.sample(1).iloc[0].to_dict()
                        sequence_data.append(sample)

            if sequence_data:
                self.normal_sequences_data.append(sequence_data)

        print(f"  APTåºåˆ—æ•°æ®: {len(self.apt_sequences_data)}")
        print(f"  æ­£å¸¸åºåˆ—æ•°æ®: {len(self.normal_sequences_data)}")

    def _convert_labels_to_ids(self):
        """è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—"""
        print("è½¬æ¢æ ‡ç­¾åºåˆ—ä¸ºIDåºåˆ—...")

        try:
            self.apt_sequences_ids = [[self.attack2id[label] for label in seq] for seq in self.apt_sequences_labels]
            self.normal_sequences_ids = [[self.attack2id[label] for label in seq] for seq in self.normal_sequences_labels]
            print(f"  APTåºåˆ—ID: {len(self.apt_sequences_ids)}")
            print(f"  æ­£å¸¸åºåˆ—ID: {len(self.normal_sequences_ids)}")
        except KeyError as e:
            print(f"é”™è¯¯: æ ‡ç­¾ {e} ä¸åœ¨attack2idæ˜ å°„ä¸­")
            raise

    def _assign_final_sequence_labels(self):
        """åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾"""
        print("åˆ†é…æœ€ç»ˆåºåˆ—æ ‡ç­¾...")

        # APTåºåˆ—æ ‡ç­¾åŸºäºæœ€é«˜æ”»å‡»é˜¶æ®µ
        stage_to_final_label = {'S1': 1, 'S2': 2, 'S3': 3, 'S4': 4, 'SN': 0}

        self.apt_labels = []
        for seq_labels in self.apt_sequences_labels:
            max_stage_num = 0
            for label in seq_labels:
                stage_num = stage_to_final_label.get(label, 0)
                max_stage_num = max(max_stage_num, stage_num)
            self.apt_labels.append(max_stage_num if max_stage_num > 0 else 1)

        # æ­£å¸¸åºåˆ—æ ‡ç­¾éƒ½æ˜¯0
        self.normal_labels = [0] * len(self.normal_sequences_ids)

        print(f"  APTæ ‡ç­¾: {len(self.apt_labels)}")
        print(f"  æ­£å¸¸æ ‡ç­¾: {len(self.normal_labels)}")

    def _save_sequence_results(self):
        """ä¿å­˜åºåˆ—ç»“æœ"""
        print("ä¿å­˜åºåˆ—ç»“æœ...")

        # ä¿å­˜attack2idæ˜ å°„
        attack2id_path = os.path.join(self.output_path, 'attack2id.json')
        with open(attack2id_path, 'w') as f:
            json.dump(self.attack2id, f, indent=4)
        print(f"  Attack2IDæ˜ å°„ä¿å­˜è‡³: {attack2id_path}")

        # ä¿å­˜APTåºåˆ—æ•°æ®
        apt_data_path = os.path.join(self.output_path, 'apt_sequences_data.json')
        with open(apt_data_path, 'w') as f:
            # è½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹
            serializable_apt_data = [
                [{k: (v.item() if hasattr(v, 'item') else v) for k, v in step.items()} for step in seq]
                for seq in self.apt_sequences_data
            ]
            json.dump(serializable_apt_data, f, indent=2)
        print(f"  APTåºåˆ—æ•°æ®ä¿å­˜è‡³: {apt_data_path}")

        # ä¿å­˜æ­£å¸¸åºåˆ—æ•°æ®
        normal_data_path = os.path.join(self.output_path, 'normal_sequences_data.json')
        with open(normal_data_path, 'w') as f:
            serializable_normal_data = [
                [{k: (v.item() if hasattr(v, 'item') else v) for k, v in step.items()} for step in seq]
                for seq in self.normal_sequences_data
            ]
            json.dump(serializable_normal_data, f, indent=2)
        print(f"  æ­£å¸¸åºåˆ—æ•°æ®ä¿å­˜è‡³: {normal_data_path}")

        # ä¿å­˜åºåˆ—æ ‡ç­¾
        apt_labels_path = os.path.join(self.output_path, 'apt_labels.npy')
        np.save(apt_labels_path, np.array(self.apt_labels))
        print(f"  APTæ ‡ç­¾ä¿å­˜è‡³: {apt_labels_path}")

        normal_labels_path = os.path.join(self.output_path, 'normal_labels.npy')
        np.save(normal_labels_path, np.array(self.normal_labels))
        print(f"  æ­£å¸¸æ ‡ç­¾ä¿å­˜è‡³: {normal_labels_path}")

        # ä¿å­˜åºåˆ—ID
        apt_ids_path = os.path.join(self.output_path, 'apt_sequences_ids.npy')
        np.save(apt_ids_path, np.array(self.apt_sequences_ids, dtype=object), allow_pickle=True)
        print(f"  APTåºåˆ—IDä¿å­˜è‡³: {apt_ids_path}")

        normal_ids_path = os.path.join(self.output_path, 'normal_sequences_ids.npy')
        np.save(normal_ids_path, np.array(self.normal_sequences_ids, dtype=object), allow_pickle=True)
        print(f"  æ­£å¸¸åºåˆ—IDä¿å­˜è‡³: {normal_ids_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¢å¼ºç‰ˆAPTæ•°æ®é¢„å¤„ç†å™¨")
    print("ç‰¹ç‚¹: XGBoostç‰¹å¾é€‰æ‹© + è®ºæ–‡å…³é”®ç‰¹å¾ + æ”»å‡»åºåˆ—æ„å»º")
    print("="*80)

    # è®¾ç½®è·¯å¾„
    input_path = r"D:\PycharmProjects\DSRL-APT-2023\DSRL-APT-2023.csv"
    output_path = "enhanced_apt_output"

    # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
    processor = EnhancedAPTPreprocessor(input_path, output_path)

    # è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†å’Œè¯„ä¼°æµç¨‹ï¼ˆè°ƒæ•´ç‰¹å¾æ•°é‡ä¸ºå¯ç”¨ç‰¹å¾æ•°ï¼‰
    processor.run_complete_pipeline(n_features=20, cv_folds=10)

    # æ„å»ºæ”»å‡»åºåˆ—
    processor.build_attack_sequences(num_apt_sequences=10000, min_normal_insert=1, max_normal_insert=5)


if __name__ == "__main__":
    main()
